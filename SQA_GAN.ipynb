{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQA_GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10218422403635597915\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Main file for ISING GAN\n",
    "# Author Pengyuan (Bill) Zhai, Aug 2020\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf # SQA_GAN uses the tensorflow.nn implementation instead of tf v1\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _MklMatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "with tf.device('/CPU:0'):\n",
    "    # Create some tensors\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clas+ 0has 18000\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "# Keep all Legacy variables as it is\n",
    "task_name = \"ISING_32_32\"\n",
    "npSeed = 188\n",
    "np.random.seed(npSeed)\n",
    "x_height, x_width = [32, 32]\n",
    "num_channels = 40 # The number of Trotter slices\n",
    "num_classes = 1 # Once \"class\" since I am using WGAN (1 output node for the critic score)\n",
    "latent_size = 100\n",
    "#labeled_rate = 1.00 # Legacy from BSSGAN 1.0, 0.5, 0.1 this limits the knowledge base of the learning\n",
    "#unlabeled_supp_rate = 0 # Legacy from BSSGAN: percentage of unlabeled data to be supplemented to the learning\n",
    "#c_ul = 0 # Legacy code from BSSGAN: class portion of unlabeled data\n",
    "# task_path = 'numpyData/BINARY_CR_16_1_split_128_128'\n",
    "\n",
    "task_path = 'numpyData/'+task_name\n",
    "# Legacy Code: keep as it is: all data (labeled and unlabeled) by class [[all data goes here]]\n",
    "# Legacy Code: I'm not using real class labels here but I will keep them here for later just in case\n",
    "train_data_by_class, test_data_by_class, train_label_by_class, test_label_by_class = [], [], [], []\n",
    "\n",
    "# Note: for a certain class, the temperatures has shape [batch_size]: [temp0, temp1,...,]\n",
    "train_temp_by_class = [] # Extra information (temperature) for CGAN\n",
    "test_temp_by_class = [] # Extra information (temperature) for CGAN\n",
    "\n",
    "# train_mask_by_class = [] # Commented out, Legacy code\n",
    "#unlabeled_indices_by_class = []\n",
    "#labeled_indices_by_class = [] # Legacy: Marked for labeled data selection to batch\n",
    "\n",
    "#unlabeled_data = np.zeros((0, x_width, x_height, num_channels)) # Legacy: In order to randomly select from\n",
    "# all unlabeled data without considering classes\n",
    "\n",
    "# Legacy code: export baseline data\n",
    "#labeled_data_baseline = np.zeros((0, x_width, x_height, num_channels)) # In order to export 0.25 labeled \n",
    "# Load real data:\n",
    "for i in range(num_classes):\n",
    "    train_data_by_class.append(np.load(task_path+'/class_'+str(i)+'/trainX.npy'))\n",
    "    #train_label_by_class.append(np.load(task_path+'/class_'+str(i)+'/trainy.npy'))\n",
    "    train_temp_by_class.append(np.load(task_path+'/class_'+str(i)+'/train_temp.npy'))\n",
    "    \n",
    "    test_data_by_class.append(np.load(task_path+'/class_'+str(i)+'/testX.npy'))\n",
    "    #test_label_by_class.append(np.load(task_path+'/class_'+str(i)+'/testy.npy'))\n",
    "    test_temp_by_class.append(np.load(task_path+'/class_'+str(i)+'/test_temp.npy'))\n",
    "    print(\"clas+ \"+str(i)+'has '+str(len(train_data_by_class[i])))\n",
    "\n",
    "    \n",
    "# Legacy code: semi-supervised. Might be useful in the future\n",
    "    # Select data as unlabeled:\n",
    "# for i in range(num_classes):\n",
    "#     numInClass = len(train_label_by_class[i])\n",
    "#     #mask =  np.concatenate((np.ones(int(numInClass * labeled_rate), dtype='int'), np.zeros(numInClass - int(numInClass * labeled_rate), dtype='int')), axis=0)\n",
    "#     num_unlabeled = int(numInClass-numInClass*labeled_rate)\n",
    "#     train_data_class = train_data_by_class[i]\n",
    "#     train_label_class = train_data_by_class[i]\n",
    "#     indices = [i for i in range(numInClass)]\n",
    "#     np.random.shuffle(indices)\n",
    "#     unlabeled_is, labeled_is = np.split(indices,[num_unlabeled]) # Unlabeled and labeled indices\n",
    "    \n",
    "#     # Concatenate image data array\n",
    "#     unlabeled_data = np.concatenate((unlabeled_data, train_data_by_class[i][unlabeled_is]))\n",
    "#     labeled_data_baseline = np.concatenate((labeled_data_baseline, train_data_by_class[i][labeled_is]))\n",
    "    \n",
    "#     #unlabeled_indices_by_class.append(unlabeled_is)\n",
    "#     labeled_indices_by_class.append(labeled_is)\n",
    "\n",
    "#np.save(unlabeled_data, 'numpyData/0.25_labeled_data')\n",
    "    \n",
    "numTrain = sum([len(c) for c in train_data_by_class])\n",
    "numTest = sum([len(c) for c in test_data_by_class])\n",
    "log_path = './ISING_GAN_log_' + task_name + '.csv' # Don't worry about log_path, it is named after the task\n",
    "#log_path_baseline = './baseline_log.csv'\n",
    "model_path ='./savedModels/GAN'\n",
    "#baseline_path = './savedModels/baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 32, 32, 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_by_class[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # normalize data\n",
    "#     x /= 255.0\n",
    "    x = x\n",
    "    return x.reshape((-1, x_height, x_width, num_channels)) #x is 4 dimensional-- (num_images, height, width, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save masked/labeled image to numpy array\n",
    "# def save_masked(toFolder):\n",
    "#     for i in range(num_classes):\n",
    "#         if not os.path.exists('numpyData/'+toFolder+'/class_'+str(i)):\n",
    "#             os.makedirs('numpyData/'+toFolder+'/class_'+str(i))\n",
    "#         np.save('numpyData/'+toFolder+'/class_'+str(i)+'/trainX', train_data_by_class[i][train_mask_by_class[i].astype(bool)])\n",
    "#         np.save('numpyData/'+toFolder+'/class_'+str(i)+'/trainy', train_label_by_class[i][train_mask_by_class[i].astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_masked(\"labeled_for_cnn_seed_\"+str(npSeed)+'_rate_'+str(labeled_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shuffle data array, labels array, and labeledMask array, each image's properties should remain consistant (labeled/unlabeled)\n",
    "#during the whole experiment. Instead of assigning labeled mask per next batch, it should be globally defined\n",
    "#prior to running the experiment --BZ, August, 2019\n",
    "def shuffle_data(data, labels, labeledMask):#all arrays here are row vectors? labels and data are columnwise\n",
    "    #np.random.seed(123)#for debugging purpose\n",
    "    indices = np.arange(labels.shape[0]) #index sequence whose length = len(labels)\n",
    "    np.random.shuffle(indices) #In place\n",
    "    shuffled_indices = indices #Useless assignment for clarity- shuffle is in place\n",
    "    if labeledMask is None:#for cases used by get_test_batch() function--BZ, August, 2019\n",
    "        return data[shuffled_indices], labels[shuffled_indices]\n",
    "    else:\n",
    "        return data[shuffled_indices], labels[shuffled_indices], labeledMask[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new batch functions:--BZ, August, 2019\n",
    "#because labeled mask should stay with the same images after each shuffle (which happens at the start of each new epoch),\n",
    "#get_batch() and get_labeled_mask() should be merged into one function\n",
    "# def get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize):\n",
    "#     #first shuffle the indices:\n",
    "#     XTrainRandom, yTrainRandom, labeledMaskRandom = shuffle_data(XTrain, yTrain, labeledMask);\n",
    "#     #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "#     counter = 0;\n",
    "#     while True:\n",
    "#         if counter >= len(yTrain):\n",
    "#             break;\n",
    "#         returnXTrain = XTrainRandom[counter:counter + batchSize];\n",
    "#         returnYTrain = yTrainRandom[counter:counter + batchSize];\n",
    "#         returnLabeledMask = labeledMaskRandom[counter:counter + batchSize];\n",
    "#         counter = counter + batchSize;\n",
    "#         yield returnXTrain, returnYTrain, returnLabeledMask\n",
    "# def get_test_batch(XTest, yTest, batchSize): #essentially the same function as above, restated here for explicity\n",
    "#     #first shuffle the indices:\n",
    "#     XTestRandom, yTestRandom = shuffle_data(XTest, yTest, None);\n",
    "#     #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "#     counter = 0;\n",
    "#     while True:\n",
    "#         if counter >= len(yTest):\n",
    "#             break;\n",
    "#         returnXTest = XTestRandom[counter:counter + batchSize];\n",
    "#         returnYTest = yTestRandom[counter:counter + batchSize];\n",
    "#         counter = counter + batchSize;\n",
    "#         yield returnXTest, returnYTest\n",
    "    \n",
    "# def get_balance_train_batch(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize):\n",
    "#     numEachClass = int(np.floor(batchSize / num_classes))\n",
    "#     returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "#     returnLabel = np.zeros((0, num_classes))\n",
    "#     returnLabeledMask = np.zeros((0))\n",
    "#     for i in range(num_classes):\n",
    "#         train_data = train_data_by_class[i]\n",
    "#         train_label = train_label_by_class[i]\n",
    "#         train_mask = train_mask_by_class[i]\n",
    "#         indices = [v for v in range(len(train_label))]\n",
    "#         np.random.shuffle(indices)\n",
    "#         selectIndices = indices[0:numEachClass]\n",
    "#         returnData = np.concatenate((returnData, train_data[selectIndices]))\n",
    "#         returnLabel = np.concatenate((returnLabel, train_label[selectIndices]))\n",
    "#         returnLabeledMask = np.concatenate((returnLabeledMask, train_mask[selectIndices]))\n",
    "#     return returnData, returnLabel, returnLabeledMask\n",
    "\n",
    "# def get_balance_train_batch_2(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize):\n",
    "#     numEachClass = int(batchSize / num_classes)\n",
    "#     numEachClass_unlabeled_labeled = [numEachClass - int(numEachClass*labeled_rate), int(numEachClass*labeled_rate)]\n",
    "# #     print(numEachClass)\n",
    "# #     print(numEachClass_unlabeled_labeled)\n",
    "#     returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "#     returnLabel = np.zeros((0, num_classes))\n",
    "#     returnLabeledMask = np.zeros((0))\n",
    "#     for i in range(num_classes):\n",
    "#         for j in [0,1]: #for unlabeled and labeled\n",
    "#             indices = []\n",
    "#             for k in range(len(train_mask_by_class[i])):\n",
    "#                 if train_mask_by_class[i][k] == j:\n",
    "#                     indices.append(k)\n",
    "# #             print('label status_'+str(j))\n",
    "# #             print(indices)\n",
    "#             np.random.shuffle(indices)\n",
    "#             selectIndices = indices[0:numEachClass_unlabeled_labeled[j]]\n",
    "# #             print('selected')\n",
    "# #             print(selectIndices)\n",
    "#             train_data = train_data_by_class[i]\n",
    "#             train_label = train_label_by_class[i]\n",
    "#             train_mask = train_mask_by_class[i]\n",
    "            \n",
    "#             returnData = np.concatenate((returnData, train_data[selectIndices]))\n",
    "#             returnLabel = np.concatenate((returnLabel, train_label[selectIndices]))\n",
    "#             returnLabeledMask = np.concatenate((returnLabeledMask, train_mask[selectIndices]))\n",
    "#     return returnData, returnLabel, returnLabeledMask\n",
    "\n",
    "# Legacy:\n",
    "# Input: c_ul is the relative portion ratio related to each class in a batch\n",
    "# def get_balance_train_batch_3(train_data_by_class, train_temp_by_class, batchSize, c_ul):\n",
    "#     numEachPortion = int(batchSize/(num_classes+1+c_ul)) # One portion of fake,c_ul portion of unlabeled\n",
    "#     returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "#     returnLabel = np.zeros((0, num_classes))\n",
    "#     returnLabeledMask = np.zeros((0))\n",
    "#     # First load labeled Data (mask is 1)\n",
    "#     for i in range(num_classes):\n",
    "#         labeled_is = np.random.choice(labeled_indices_by_class[i], numEachPortion)\n",
    "#         returnData = np.concatenate((returnData, train_data_by_class[i][labeled_is]))\n",
    "#         returnLabel = np.concatenate((returnLabel, train_label_by_class[i][labeled_is]))\n",
    "#         returnLabeledMask = np.concatenate((returnLabeledMask, np.ones(numEachPortion))) # 1 for labeled\n",
    "#     # Then load unlabeled Data (mask is 0)\n",
    "#     if len(unlabeled_data) is not 0:\n",
    "#         num_unlabeled = numEachPortion * c_ul\n",
    "#         unlabeled_is = np.random.choice([i for i in range(len(unlabeled_data))], num_unlabeled)\n",
    "#         returnData = np.concatenate((returnData, unlabeled_data[unlabeled_is]))\n",
    "#         returnLabel = np.concatenate((returnLabel, np.zeros((num_unlabeled, num_classes))))\n",
    "#         returnLabeledMask = np.concatenate((returnLabeledMask, np.zeros(num_unlabeled))) # 0 for unlabeled\n",
    "#     return returnData, returnLabel, returnLabeledMask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_batch(train_data_by_class, train_temp_by_class, batch_size):\n",
    "    return get_test_batch(train_data_by_class, train_temp_by_class, batch_size)\n",
    "        \n",
    "\n",
    "def get_test_batch(test_data_by_class, test_temp_by_class, batchSize):\n",
    "    XTest = np.zeros((0, x_width, x_height, num_channels))\n",
    "    yTest = np.zeros((0, 1)) # For temperatures\n",
    "    for i in range(num_classes):\n",
    "        XTest = np.concatenate((XTest, test_data_by_class[i]))\n",
    "        yTest = np.concatenate((yTest, test_temp_by_class[i]))\n",
    "    #first shuffle the indices:\n",
    "    XTestRandom, yTestRandom = shuffle_data(XTest, yTest, None);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTest):\n",
    "            break;\n",
    "        returnXTest = XTestRandom[counter:counter + batchSize];\n",
    "        returnYTest = yTestRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTest, returnYTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[0.22846143]]\n",
      "1.0\n",
      "[[1.90269514]]\n",
      "1.0\n",
      "[[2.98949948]]\n",
      "1.0\n",
      "[[0.58427922]]\n",
      "1.0\n",
      "[[0.16110806]]\n",
      "1.0\n",
      "[[1.42222112]]\n",
      "1.0\n",
      "[[0.9018451]]\n",
      "1.0\n",
      "[[1.14170709]]\n",
      "1.0\n",
      "[[2.36066804]]\n",
      "1.0\n",
      "[[2.53227662]]\n",
      "1.0\n",
      "[[2.66173309]]\n",
      "1.0\n",
      "[[1.06295315]]\n",
      "1.0\n",
      "[[2.48712436]]\n",
      "1.0\n",
      "[[0.99004951]]\n",
      "1.0\n",
      "[[2.03905196]]\n",
      "1.0\n",
      "[[0.58862944]]\n",
      "1.0\n",
      "[[2.11315566]]\n",
      "1.0\n",
      "[[1.55467774]]\n",
      "1.0\n",
      "[[0.28201411]]\n",
      "1.0\n",
      "[[1.82514126]]\n",
      "1.0\n",
      "[[0.98419922]]\n",
      "1.0\n",
      "[[2.07550378]]\n",
      "1.0\n",
      "[[1.55737787]]\n",
      "1.0\n",
      "[[2.27651383]]\n",
      "1.0\n",
      "[[1.4118706]]\n",
      "1.0\n",
      "[[2.30426522]]\n",
      "1.0\n",
      "[[2.80769039]]\n",
      "1.0\n",
      "[[2.49672484]]\n",
      "1.0\n",
      "[[0.85609281]]\n",
      "1.0\n",
      "[[0.43742188]]\n",
      "1.0\n",
      "[[2.37566879]]\n",
      "1.0\n",
      "[[2.28926447]]\n",
      "1.0\n",
      "[[2.63578179]]\n",
      "1.0\n",
      "[[2.33756688]]\n",
      "1.0\n",
      "[[2.84654233]]\n",
      "1.0\n",
      "[[2.62363118]]\n",
      "1.0\n",
      "[[2.35751788]]\n",
      "1.0\n",
      "[[2.35676784]]\n",
      "1.0\n",
      "[[1.46512326]]\n",
      "1.0\n",
      "[[0.25741288]]\n",
      "1.0\n",
      "[[2.04070204]]\n",
      "1.0\n",
      "[[1.85409271]]\n",
      "1.0\n",
      "[[0.57887895]]\n",
      "1.0\n",
      "[[1.17845893]]\n",
      "1.0\n",
      "[[1.16510826]]\n",
      "1.0\n",
      "[[0.13230662]]\n",
      "1.0\n",
      "[[2.92064603]]\n",
      "1.0\n",
      "[[1.72163609]]\n",
      "1.0\n",
      "[[2.60353018]]\n",
      "1.0\n",
      "[[1.46977349]]\n",
      "1.0\n",
      "[[1.81374069]]\n",
      "1.0\n",
      "[[2.16430822]]\n",
      "1.0\n",
      "[[2.39456973]]\n",
      "1.0\n",
      "[[0.15555779]]\n",
      "1.0\n",
      "[[0.52622632]]\n",
      "1.0\n",
      "[[1.50202511]]\n",
      "1.0\n",
      "[[0.93199661]]\n",
      "1.0\n",
      "[[1.52647633]]\n",
      "1.0\n",
      "[[2.70508526]]\n",
      "1.0\n",
      "[[0.51857594]]\n",
      "1.0\n",
      "[[0.53837693]]\n",
      "1.0\n",
      "[[0.31141558]]\n",
      "1.0\n",
      "[[2.02735137]]\n",
      "1.0\n",
      "[[1.36101806]]\n",
      "1.0\n",
      "[[1.32951648]]\n",
      "1.0\n",
      "[[1.85814291]]\n",
      "1.0\n",
      "[[1.93449673]]\n",
      "1.0\n",
      "[[1.79483975]]\n",
      "1.0\n",
      "[[2.2079604]]\n",
      "1.0\n",
      "[[1.26651333]]\n",
      "1.0\n",
      "[[1.84704236]]\n",
      "1.0\n",
      "[[2.97629882]]\n",
      "1.0\n",
      "[[0.33496676]]\n",
      "1.0\n",
      "[[2.0959048]]\n",
      "1.0\n",
      "[[0.89539478]]\n",
      "1.0\n",
      "[[0.03750188]]\n",
      "1.0\n",
      "[[0.86284315]]\n",
      "1.0\n",
      "[[0.1638082]]\n",
      "1.0\n",
      "[[2.76643832]]\n",
      "1.0\n",
      "[[1.4058703]]\n",
      "1.0\n",
      "[[1.76873844]]\n",
      "1.0\n",
      "[[2.54742737]]\n",
      "1.0\n",
      "[[0.33916697]]\n",
      "1.0\n",
      "[[0.83359169]]\n",
      "1.0\n",
      "[[2.3260163]]\n",
      "1.0\n",
      "[[0.98359919]]\n",
      "1.0\n",
      "[[2.77978899]]\n",
      "1.0\n",
      "[[2.39246963]]\n",
      "1.0\n",
      "[[1.21656083]]\n",
      "1.0\n",
      "[[0.35896796]]\n",
      "1.0\n",
      "[[1.45372269]]\n",
      "1.0\n",
      "[[2.03170159]]\n",
      "1.0\n",
      "[[1.01060054]]\n",
      "1.0\n",
      "[[0.42767139]]\n",
      "1.0\n",
      "[[1.93764689]]\n",
      "1.0\n",
      "[[0.5739287]]\n",
      "1.0\n",
      "[[0.12555629]]\n",
      "1.0\n",
      "[[0.18345918]]\n",
      "1.0\n",
      "[[2.93984699]]\n",
      "1.0\n",
      "[[1.59037952]]\n",
      "1.0\n",
      "[[0.96229812]]\n",
      "1.0\n",
      "[[1.54102706]]\n",
      "1.0\n",
      "[[0.64473224]]\n",
      "1.0\n",
      "[[0.68718437]]\n",
      "1.0\n",
      "[[2.66863343]]\n",
      "1.0\n",
      "[[1.74488725]]\n",
      "1.0\n",
      "[[1.70153508]]\n",
      "1.0\n",
      "[[2.60068004]]\n",
      "1.0\n",
      "[[0.38596931]]\n",
      "1.0\n",
      "[[0.5379269]]\n",
      "1.0\n",
      "[[2.08705436]]\n",
      "1.0\n",
      "[[0.35611781]]\n",
      "1.0\n",
      "[[1.30821542]]\n",
      "1.0\n",
      "[[2.7959898]]\n",
      "1.0\n",
      "[[2.33696685]]\n",
      "1.0\n",
      "[[2.79823991]]\n",
      "1.0\n",
      "[[1.86069304]]\n",
      "1.0\n",
      "[[1.23126157]]\n",
      "1.0\n",
      "[[0.03720187]]\n",
      "1.0\n",
      "[[0.12945648]]\n",
      "1.0\n",
      "[[2.05360268]]\n",
      "1.0\n",
      "[[0.04620232]]\n",
      "1.0\n",
      "[[2.29961498]]\n",
      "1.0\n",
      "[[1.37556878]]\n",
      "1.0\n",
      "[[0.40397021]]\n",
      "1.0\n",
      "[[0.02310116]]\n",
      "1.0\n",
      "[[0.1957598]]\n",
      "1.0\n",
      "[[1.27476374]]\n",
      "1.0\n",
      "[[2.35181759]]\n",
      "1.0\n",
      "[[2.12140607]]\n",
      "1.0\n",
      "[[1.38126907]]\n",
      "1.0\n",
      "[[0.20926047]]\n",
      "1.0\n",
      "[[1.5278264]]\n",
      "1.0\n",
      "[[0.37096856]]\n",
      "1.0\n",
      "[[1.49017451]]\n",
      "1.0\n",
      "[[0.19996001]]\n",
      "1.0\n",
      "[[0.75828792]]\n",
      "1.0\n",
      "[[1.59922997]]\n",
      "1.0\n",
      "[[2.69353468]]\n",
      "1.0\n",
      "[[0.61038053]]\n",
      "1.0\n",
      "[[1.16030802]]\n",
      "1.0\n",
      "[[1.52842643]]\n",
      "1.0\n",
      "[[0.15240763]]\n",
      "1.0\n",
      "[[0.07995401]]\n",
      "1.0\n",
      "[[2.37866894]]\n",
      "1.0\n",
      "[[1.89354468]]\n",
      "1.0\n",
      "[[1.73603681]]\n",
      "1.0\n",
      "[[0.99739988]]\n",
      "1.0\n",
      "[[2.48637432]]\n",
      "1.0\n",
      "[[0.59597981]]\n",
      "1.0\n",
      "[[1.1519076]]\n",
      "1.0\n",
      "[[0.73113656]]\n",
      "1.0\n",
      "[[1.32771639]]\n",
      "1.0\n",
      "[[1.97949898]]\n",
      "1.0\n",
      "[[2.17885895]]\n",
      "1.0\n",
      "[[1.72433622]]\n",
      "1.0\n",
      "[[1.75718786]]\n",
      "1.0\n",
      "[[1.03295165]]\n",
      "1.0\n",
      "[[1.77083855]]\n",
      "1.0\n",
      "[[2.98334917]]\n",
      "1.0\n",
      "[[0.25306266]]\n",
      "1.0\n",
      "[[0.40817042]]\n",
      "1.0\n",
      "[[0.5698785]]\n",
      "1.0\n",
      "[[0.27301366]]\n",
      "1.0\n",
      "[[0.30286515]]\n",
      "1.0\n",
      "[[2.21636082]]\n",
      "1.0\n",
      "[[2.97284864]]\n",
      "1.0\n",
      "[[2.51742587]]\n",
      "1.0\n",
      "[[0.67683385]]\n",
      "1.0\n",
      "[[1.24326217]]\n",
      "1.0\n",
      "[[2.68093405]]\n",
      "1.0\n",
      "[[2.71858593]]\n",
      "1.0\n",
      "[[1.13435672]]\n",
      "1.0\n",
      "[[1.60538027]]\n",
      "1.0\n",
      "[[2.51727587]]\n",
      "1.0\n",
      "[[0.18270914]]\n",
      "1.0\n",
      "[[0.61503076]]\n",
      "1.0\n",
      "[[2.17780889]]\n",
      "1.0\n",
      "[[0.25696286]]\n",
      "1.0\n",
      "[[2.53617681]]\n",
      "1.0\n",
      "[[2.09485475]]\n",
      "1.0\n",
      "[[0.18195911]]\n",
      "1.0\n",
      "[[1.09100456]]\n",
      "1.0\n",
      "[[1.96254813]]\n",
      "1.0\n",
      "[[1.92264614]]\n",
      "1.0\n",
      "[[2.21096055]]\n",
      "1.0\n",
      "[[0.19845993]]\n",
      "1.0\n",
      "[[0.43262164]]\n",
      "1.0\n",
      "[[2.70328517]]\n",
      "1.0\n",
      "[[0.80929047]]\n",
      "1.0\n",
      "[[2.4460223]]\n",
      "1.0\n",
      "[[0.42512126]]\n",
      "1.0\n",
      "[[2.34266714]]\n",
      "1.0\n",
      "[[0.18540928]]\n",
      "1.0\n",
      "[[1.07345368]]\n",
      "1.0\n",
      "[[1.08710436]]\n",
      "1.0\n",
      "[[2.75068754]]\n",
      "1.0\n",
      "[[1.15700786]]\n",
      "1.0\n",
      "[[0.98569929]]\n",
      "1.0\n",
      "[[1.30896545]]\n",
      "1.0\n",
      "[[1.47517376]]\n",
      "1.0\n",
      "[[1.57432872]]\n",
      "1.0\n",
      "[[0.15195761]]\n",
      "1.0\n",
      "[[1.00295015]]\n",
      "1.0\n",
      "[[2.73163658]]\n",
      "1.0\n",
      "[[2.35076754]]\n",
      "1.0\n",
      "[[1.82469124]]\n",
      "1.0\n",
      "[[1.70213511]]\n",
      "1.0\n",
      "[[1.85754288]]\n",
      "1.0\n",
      "[[2.83334167]]\n",
      "1.0\n",
      "[[0.44852243]]\n",
      "1.0\n",
      "[[0.4618731]]\n",
      "1.0\n",
      "[[2.99264963]]\n",
      "1.0\n",
      "[[2.50047503]]\n",
      "1.0\n",
      "[[1.19645983]]\n",
      "1.0\n",
      "[[2.87684384]]\n",
      "1.0\n",
      "[[2.55957798]]\n",
      "1.0\n",
      "[[1.21821092]]\n",
      "1.0\n",
      "[[2.10895545]]\n",
      "1.0\n",
      "[[0.61518077]]\n",
      "1.0\n",
      "[[2.68423421]]\n",
      "1.0\n",
      "[[0.46892345]]\n",
      "1.0\n",
      "[[1.27461374]]\n",
      "1.0\n",
      "[[0.06225312]]\n",
      "1.0\n",
      "[[1.49077454]]\n",
      "1.0\n",
      "[[2.52582629]]\n",
      "1.0\n",
      "[[1.20816041]]\n",
      "1.0\n",
      "[[2.4940247]]\n",
      "1.0\n",
      "[[0.00120007]]\n",
      "1.0\n",
      "[[2.19986]]\n",
      "1.0\n",
      "[[0.23761189]]\n",
      "1.0\n",
      "[[1.37511876]]\n",
      "1.0\n",
      "[[0.47687385]]\n",
      "1.0\n",
      "[[0.58307916]]\n",
      "1.0\n",
      "[[0.39931997]]\n",
      "1.0\n",
      "[[0.0418521]]\n",
      "1.0\n",
      "[[1.94319716]]\n",
      "1.0\n",
      "[[0.72168609]]\n",
      "1.0\n",
      "[[0.98089905]]\n",
      "1.0\n",
      "[[0.19305966]]\n",
      "1.0\n",
      "[[1.32051603]]\n",
      "1.0\n",
      "[[0.81724087]]\n",
      "1.0\n",
      "[[2.08945448]]\n",
      "1.0\n",
      "[[1.539977]]\n",
      "1.0\n",
      "[[0.69093455]]\n",
      "1.0\n",
      "[[2.48817441]]\n",
      "1.0\n",
      "[[2.4239712]]\n",
      "1.0\n",
      "[[1.08260414]]\n",
      "1.0\n",
      "[[0.86359319]]\n",
      "1.0\n",
      "[[1.23966199]]\n",
      "1.0\n",
      "[[1.06070304]]\n",
      "1.0\n",
      "[[1.65803291]]\n",
      "1.0\n",
      "[[0.58877945]]\n",
      "1.0\n",
      "[[0.84019202]]\n",
      "1.0\n",
      "[[0.84034202]]\n",
      "1.0\n",
      "[[2.46342317]]\n",
      "1.0\n",
      "[[1.65203261]]\n",
      "1.0\n",
      "[[2.86049303]]\n",
      "1.0\n",
      "[[2.93174659]]\n",
      "1.0\n",
      "[[0.66903346]]\n",
      "1.0\n",
      "[[0.38506926]]\n",
      "1.0\n",
      "[[2.7059853]]\n",
      "1.0\n",
      "[[1.92414621]]\n",
      "1.0\n",
      "[[0.88864444]]\n",
      "1.0\n",
      "[[2.75668784]]\n",
      "1.0\n",
      "[[0.29101456]]\n",
      "1.0\n",
      "[[0.35701786]]\n",
      "1.0\n",
      "[[1.85559278]]\n",
      "1.0\n",
      "[[2.78023901]]\n",
      "1.0\n",
      "[[2.32481624]]\n",
      "1.0\n",
      "[[0.55622782]]\n",
      "1.0\n",
      "[[1.68608431]]\n",
      "1.0\n",
      "[[0.51527577]]\n",
      "1.0\n",
      "[[0.3339167]]\n",
      "1.0\n",
      "[[1.28151408]]\n",
      "1.0\n",
      "[[1.10840543]]\n",
      "1.0\n",
      "[[1.81269064]]\n",
      "1.0\n",
      "[[0.78573929]]\n",
      "1.0\n",
      "[[2.24171209]]\n",
      "1.0\n",
      "[[1.39341968]]\n",
      "1.0\n",
      "[[2.0219511]]\n",
      "1.0\n",
      "[[1.34526727]]\n",
      "1.0\n",
      "[[2.52957648]]\n",
      "1.0\n",
      "[[1.1678084]]\n",
      "1.0\n",
      "[[0.34651733]]\n",
      "1.0\n",
      "[[0.25411271]]\n",
      "1.0\n",
      "[[1.27146358]]\n",
      "1.0\n",
      "[[2.17765889]]\n",
      "1.0\n",
      "[[1.03835192]]\n",
      "1.0\n",
      "[[2.46852343]]\n",
      "1.0\n",
      "[[0.66603331]]\n",
      "1.0\n",
      "[[1.76318816]]\n",
      "1.0\n",
      "[[1.62008101]]\n",
      "1.0\n",
      "[[2.09965499]]\n",
      "1.0\n",
      "[[0.71643583]]\n",
      "1.0\n",
      "[[1.3839692]]\n",
      "1.0\n",
      "[[1.02515126]]\n",
      "1.0\n",
      "[[0.37081855]]\n",
      "1.0\n",
      "[[1.04225212]]\n",
      "1.0\n",
      "[[1.18745938]]\n",
      "1.0\n",
      "[[1.60628032]]\n",
      "1.0\n",
      "[[1.58962949]]\n",
      "1.0\n",
      "[[0.0378019]]\n",
      "1.0\n",
      "[[1.27566379]]\n",
      "1.0\n",
      "[[0.6099305]]\n",
      "1.0\n",
      "[[1.52707636]]\n",
      "1.0\n",
      "[[1.91349568]]\n",
      "1.0\n",
      "[[1.62143108]]\n",
      "1.0\n",
      "[[2.41932097]]\n",
      "1.0\n",
      "[[2.06740337]]\n",
      "1.0\n",
      "[[0.14340718]]\n",
      "1.0\n",
      "[[1.30686535]]\n",
      "1.0\n",
      "[[2.55102755]]\n",
      "1.0\n",
      "[[1.81224062]]\n",
      "1.0\n",
      "[[2.19610981]]\n",
      "1.0\n",
      "[[1.06925347]]\n",
      "1.0\n",
      "[[0.0498025]]\n",
      "1.0\n",
      "[[2.30006501]]\n",
      "1.0\n",
      "[[0.34291715]]\n",
      "1.0\n",
      "[[1.39941998]]\n",
      "1.0\n",
      "[[0.27316367]]\n",
      "1.0\n",
      "[[0.50612531]]\n",
      "1.0\n",
      "[[0.42797141]]\n",
      "1.0\n",
      "[[0.03555179]]\n",
      "1.0\n",
      "[[1.94034702]]\n",
      "1.0\n",
      "[[1.91619581]]\n",
      "1.0\n",
      "[[2.33366669]]\n",
      "1.0\n",
      "[[2.11900595]]\n",
      "1.0\n",
      "[[1.48297415]]\n",
      "1.0\n",
      "[[0.50147508]]\n",
      "1.0\n",
      "[[1.64078204]]\n",
      "1.0\n",
      "[[2.94584729]]\n",
      "1.0\n",
      "[[1.1079554]]\n",
      "1.0\n",
      "[[0.82414121]]\n",
      "1.0\n",
      "[[0.30211511]]\n",
      "1.0\n",
      "[[2.71348568]]\n",
      "1.0\n",
      "[[0.20746038]]\n",
      "1.0\n",
      "[[1.49272464]]\n",
      "1.0\n",
      "[[2.60173009]]\n",
      "1.0\n",
      "[[2.76088805]]\n",
      "1.0\n",
      "[[2.47632382]]\n",
      "1.0\n",
      "[[2.55087755]]\n",
      "1.0\n",
      "[[1.97619881]]\n",
      "1.0\n",
      "[[2.70478524]]\n",
      "1.0\n",
      "[[2.85734287]]\n",
      "1.0\n",
      "[[2.46672334]]\n",
      "1.0\n",
      "[[1.55842793]]\n",
      "1.0\n",
      "[[1.54852743]]\n",
      "1.0\n",
      "[[2.71888595]]\n",
      "1.0\n",
      "[[2.46117306]]\n",
      "1.0\n",
      "[[1.36296815]]\n",
      "1.0\n",
      "[[2.56227812]]\n",
      "1.0\n",
      "[[0.03510176]]\n",
      "1.0\n",
      "[[0.19455974]]\n",
      "1.0\n",
      "[[2.22851143]]\n",
      "1.0\n",
      "[[1.88664434]]\n",
      "1.0\n",
      "[[2.49627482]]\n",
      "1.0\n",
      "[[0.84049203]]\n",
      "1.0\n",
      "[[1.89879494]]\n",
      "1.0\n",
      "[[0.27811391]]\n",
      "1.0\n",
      "[[1.0578529]]\n",
      "1.0\n",
      "[[2.77948898]]\n",
      "1.0\n",
      "[[1.49632482]]\n",
      "1.0\n",
      "[[0.39541978]]\n",
      "1.0\n",
      "[[0.76278815]]\n",
      "1.0\n",
      "[[2.99759988]]\n",
      "1.0\n",
      "[[0.92914646]]\n",
      "1.0\n",
      "[[2.60638032]]\n",
      "1.0\n",
      "[[1.83804191]]\n",
      "1.0\n",
      "[[0.51917597]]\n",
      "1.0\n",
      "[[2.37026852]]\n",
      "1.0\n",
      "[[0.50237513]]\n",
      "1.0\n",
      "[[1.90119506]]\n",
      "1.0\n",
      "[[0.75963799]]\n",
      "1.0\n",
      "[[1.22076104]]\n",
      "1.0\n",
      "[[0.28651433]]\n",
      "1.0\n",
      "[[0.00030002]]\n",
      "1.0\n",
      "[[0.75003751]]\n",
      "1.0\n",
      "[[2.939997]]\n",
      "1.0\n",
      "[[2.50857543]]\n",
      "1.0\n",
      "[[2.48412421]]\n",
      "1.0\n",
      "[[2.88284414]]\n",
      "1.0\n",
      "[[2.64883244]]\n",
      "1.0\n",
      "[[1.23021152]]\n",
      "1.0\n",
      "[[1.1139557]]\n",
      "1.0\n",
      "[[0.88039403]]\n",
      "1.0\n",
      "[[2.46807341]]\n",
      "1.0\n",
      "[[1.06490325]]\n",
      "1.0\n",
      "[[2.71468574]]\n",
      "1.0\n",
      "[[0.37726887]]\n",
      "1.0\n",
      "[[2.21906096]]\n",
      "1.0\n",
      "[[0.70563529]]\n",
      "1.0\n",
      "[[1.7979899]]\n",
      "1.0\n",
      "[[2.06335317]]\n",
      "1.0\n",
      "[[0.6558328]]\n",
      "1.0\n",
      "[[0.21751088]]\n",
      "1.0\n",
      "[[1.47232362]]\n",
      "1.0\n",
      "[[1.72688635]]\n",
      "1.0\n",
      "[[2.42517126]]\n",
      "1.0\n",
      "[[1.00115006]]\n",
      "1.0\n",
      "[[2.55927797]]\n",
      "1.0\n",
      "[[2.21936097]]\n",
      "1.0\n",
      "[[1.89459473]]\n",
      "1.0\n",
      "[[0.20056004]]\n",
      "1.0\n",
      "[[1.8818441]]\n",
      "1.0\n",
      "[[0.75153758]]\n",
      "1.0\n",
      "[[0.28276415]]\n",
      "1.0\n",
      "[[0.6339317]]\n",
      "1.0\n",
      "[[2.64928247]]\n",
      "1.0\n",
      "[[2.69443472]]\n",
      "1.0\n",
      "[[0.0678034]]\n",
      "1.0\n",
      "[[2.78908946]]\n",
      "1.0\n",
      "[[1.49242463]]\n",
      "1.0\n",
      "[[2.48337417]]\n",
      "1.0\n",
      "[[2.73973699]]\n",
      "1.0\n",
      "[[2.75638782]]\n",
      "1.0\n",
      "[[0.24316217]]\n",
      "1.0\n",
      "[[1.31706586]]\n",
      "1.0\n",
      "[[2.21036052]]\n",
      "1.0\n",
      "[[1.02425122]]\n",
      "1.0\n",
      "[[2.50902545]]\n",
      "1.0\n",
      "[[2.20301015]]\n",
      "1.0\n",
      "[[1.73243663]]\n",
      "1.0\n",
      "[[0.61638083]]\n",
      "1.0\n",
      "[[0.96694835]]\n",
      "1.0\n",
      "[[2.33486675]]\n",
      "1.0\n",
      "[[2.25221261]]\n",
      "1.0\n",
      "[[2.21981099]]\n",
      "1.0\n",
      "[[0.54887745]]\n",
      "1.0\n",
      "[[0.94114706]]\n",
      "1.0\n",
      "[[2.72068604]]\n",
      "1.0\n",
      "[[2.20406021]]\n",
      "1.0\n",
      "[[1.04240213]]\n",
      "1.0\n",
      "[[0.97669884]]\n",
      "1.0\n",
      "[[2.90459523]]\n",
      "1.0\n",
      "[[0.96169809]]\n",
      "1.0\n",
      "[[0.80674034]]\n",
      "1.0\n",
      "[[0.00360019]]\n",
      "1.0\n",
      "[[0.21136058]]\n",
      "1.0\n",
      "[[2.57712886]]\n",
      "1.0\n",
      "[[1.05635282]]\n",
      "1.0\n",
      "[[2.51892595]]\n",
      "1.0\n",
      "[[1.6878844]]\n",
      "1.0\n",
      "[[0.16800841]]\n",
      "1.0\n",
      "[[2.27231362]]\n",
      "1.0\n",
      "[[2.15095755]]\n",
      "1.0\n",
      "[[0.7638382]]\n",
      "1.0\n",
      "[[1.15070754]]\n",
      "1.0\n",
      "[[2.95139757]]\n",
      "1.0\n",
      "[[2.61643082]]\n",
      "1.0\n",
      "[[0.16080805]]\n",
      "1.0\n",
      "[[2.68888445]]\n",
      "1.0\n",
      "[[2.59257963]]\n",
      "1.0\n",
      "[[1.6119806]]\n",
      "1.0\n",
      "[[1.47472374]]\n",
      "1.0\n",
      "[[1.82739137]]\n",
      "1.0\n",
      "[[1.35531777]]\n",
      "1.0\n",
      "[[0.74118707]]\n",
      "1.0\n",
      "[[2.61373069]]\n",
      "1.0\n",
      "[[2.24441222]]\n",
      "1.0\n",
      "[[2.75263763]]\n",
      "1.0\n",
      "[[0.23161159]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[2.73718686]]\n",
      "1.0\n",
      "[[2.11420571]]\n",
      "1.0\n",
      "[[0.04800241]]\n",
      "1.0\n",
      "[[0.88159409]]\n",
      "1.0\n",
      "[[0.13830692]]\n",
      "1.0\n",
      "[[2.05735287]]\n",
      "1.0\n",
      "[[2.97089855]]\n",
      "1.0\n",
      "[[0.87469374]]\n",
      "1.0\n",
      "[[0.30256514]]\n",
      "1.0\n",
      "[[2.20541027]]\n",
      "1.0\n",
      "[[0.24196211]]\n",
      "1.0\n",
      "[[0.7357868]]\n",
      "1.0\n",
      "[[1.59472974]]\n",
      "1.0\n",
      "[[0.14220712]]\n",
      "1.0\n",
      "[[0.25441273]]\n",
      "1.0\n",
      "[[0.78198911]]\n",
      "1.0\n",
      "[[1.5819791]]\n",
      "1.0\n",
      "[[1.15340768]]\n",
      "1.0\n",
      "[[2.94359718]]\n",
      "1.0\n",
      "[[2.84774239]]\n",
      "1.0\n",
      "[[2.08735437]]\n",
      "1.0\n",
      "[[2.4319216]]\n",
      "1.0\n",
      "[[1.83309166]]\n",
      "1.0\n",
      "[[2.62318116]]\n",
      "1.0\n",
      "[[2.54022701]]\n",
      "1.0\n",
      "[[0.14895746]]\n",
      "1.0\n",
      "[[0.41507076]]\n",
      "1.0\n",
      "[[1.47892395]]\n",
      "1.0\n",
      "[[0.02295116]]\n",
      "1.0\n",
      "[[1.32276614]]\n",
      "1.0\n",
      "[[1.33071654]]\n",
      "1.0\n",
      "[[1.26201311]]\n",
      "1.0\n",
      "[[0.56402821]]\n",
      "1.0\n",
      "[[2.50572529]]\n",
      "1.0\n",
      "[[1.84674234]]\n",
      "1.0\n",
      "[[2.03575179]]\n",
      "1.0\n",
      "[[2.25251263]]\n",
      "1.0\n",
      "[[0.21466074]]\n",
      "1.0\n",
      "[[1.59998]]\n",
      "1.0\n",
      "[[1.34571729]]\n",
      "1.0\n",
      "[[1.3518676]]\n",
      "1.0\n",
      "[[2.29226462]]\n",
      "1.0\n",
      "[[0.94099706]]\n",
      "1.0\n",
      "[[0.05370269]]\n",
      "1.0\n",
      "[[0.56642833]]\n",
      "1.0\n",
      "[[2.36486825]]\n",
      "1.0\n",
      "[[2.79418971]]\n",
      "1.0\n",
      "[[0.58262914]]\n",
      "1.0\n",
      "[[0.95224762]]\n",
      "1.0\n",
      "[[1.81914096]]\n",
      "1.0\n",
      "[[2.95769789]]\n",
      "1.0\n",
      "[[1.25556278]]\n",
      "1.0\n",
      "[[0.77703886]]\n",
      "1.0\n",
      "[[2.31086555]]\n",
      "1.0\n",
      "[[2.79613981]]\n",
      "1.0\n",
      "[[2.07865394]]\n",
      "1.0\n",
      "[[0.70143508]]\n",
      "1.0\n",
      "[[0.51242563]]\n",
      "1.0\n",
      "[[2.80139007]]\n",
      "1.0\n",
      "[[2.27441372]]\n",
      "1.0\n",
      "[[2.31071554]]\n",
      "1.0\n",
      "[[0.17565879]]\n",
      "1.0\n",
      "[[0.7659383]]\n",
      "1.0\n",
      "[[1.47862394]]\n",
      "1.0\n",
      "[[2.17840892]]\n",
      "1.0\n",
      "[[2.90234512]]\n",
      "1.0\n",
      "[[0.92599631]]\n",
      "1.0\n",
      "[[1.50967549]]\n",
      "1.0\n",
      "[[0.87409371]]\n",
      "1.0\n",
      "[[1.40932047]]\n",
      "1.0\n",
      "[[1.94334717]]\n",
      "1.0\n",
      "[[2.78368919]]\n",
      "1.0\n",
      "[[0.34036703]]\n",
      "1.0\n",
      "[[2.67223361]]\n",
      "1.0\n",
      "[[1.50277514]]\n",
      "1.0\n",
      "[[2.37101855]]\n",
      "1.0\n",
      "[[1.14680735]]\n",
      "1.0\n",
      "[[1.55557778]]\n",
      "1.0\n",
      "[[0.77973899]]\n",
      "1.0\n",
      "[[0.31471574]]\n",
      "1.0\n",
      "[[2.0380019]]\n",
      "1.0\n",
      "[[0.71688585]]\n",
      "1.0\n",
      "[[2.7278864]]\n",
      "1.0\n",
      "[[0.87274364]]\n",
      "1.0\n",
      "[[1.27851393]]\n",
      "1.0\n",
      "[[0.68313416]]\n",
      "1.0\n",
      "[[1.69373469]]\n",
      "1.0\n",
      "[[0.95854793]]\n",
      "1.0\n",
      "[[0.80374019]]\n",
      "1.0\n",
      "[[0.97024852]]\n",
      "1.0\n",
      "[[0.87169359]]\n",
      "1.0\n",
      "[[1.94574729]]\n",
      "1.0\n",
      "[[2.0080004]]\n",
      "1.0\n",
      "[[2.68303415]]\n",
      "1.0\n",
      "[[0.29716487]]\n",
      "1.0\n",
      "[[1.99929997]]\n",
      "1.0\n",
      "[[2.75053753]]\n",
      "1.0\n",
      "[[0.07275365]]\n",
      "1.0\n",
      "[[0.77868894]]\n",
      "1.0\n",
      "[[2.64298215]]\n",
      "1.0\n",
      "[[0.77883895]]\n",
      "1.0\n",
      "[[0.20806041]]\n",
      "1.0\n",
      "[[1.4898745]]\n",
      "1.0\n",
      "[[1.77203861]]\n",
      "1.0\n",
      "[[0.33331667]]\n",
      "1.0\n",
      "[[0.70263514]]\n",
      "1.0\n",
      "[[1.57072854]]\n",
      "1.0\n",
      "[[0.34636733]]\n",
      "1.0\n",
      "[[0.85339268]]\n",
      "1.0\n",
      "[[1.73288665]]\n",
      "1.0\n",
      "[[0.80659034]]\n",
      "1.0\n",
      "[[2.43882194]]\n",
      "1.0\n",
      "[[2.78953948]]\n",
      "1.0\n",
      "[[1.43512176]]\n",
      "1.0\n",
      "[[2.79778989]]\n",
      "1.0\n",
      "[[0.15930797]]\n",
      "1.0\n",
      "[[1.56442823]]\n",
      "1.0\n",
      "[[0.17025852]]\n",
      "1.0\n",
      "[[0.43142158]]\n",
      "1.0\n",
      "[[1.74038702]]\n",
      "1.0\n",
      "[[2.60878044]]\n",
      "1.0\n",
      "[[1.70318516]]\n",
      "1.0\n",
      "[[0.3838692]]\n",
      "1.0\n",
      "[[0.0517526]]\n",
      "1.0\n",
      "[[2.07295365]]\n",
      "1.0\n",
      "[[1.18055903]]\n",
      "1.0\n",
      "[[2.38301915]]\n",
      "1.0\n",
      "[[2.95889795]]\n",
      "1.0\n",
      "[[1.84719236]]\n",
      "1.0\n",
      "[[1.70483525]]\n",
      "1.0\n",
      "[[1.51552578]]\n",
      "1.0\n",
      "[[0.4117706]]\n",
      "1.0\n",
      "[[1.59877994]]\n",
      "1.0\n",
      "[[2.56422821]]\n",
      "1.0\n",
      "[[1.25241263]]\n",
      "1.0\n",
      "[[0.76728837]]\n",
      "1.0\n",
      "[[1.52302616]]\n",
      "1.0\n",
      "[[0.01650083]]\n",
      "1.0\n",
      "[[1.54462724]]\n",
      "1.0\n",
      "[[0.11910596]]\n",
      "1.0\n",
      "[[0.30196511]]\n",
      "1.0\n",
      "[[1.52407621]]\n",
      "1.0\n",
      "[[1.65848293]]\n",
      "1.0\n",
      "[[1.69328467]]\n",
      "1.0\n",
      "[[0.53642683]]\n",
      "1.0\n",
      "[[0.77298866]]\n",
      "1.0\n",
      "[[2.31131557]]\n",
      "1.0\n",
      "[[2.53212661]]\n",
      "1.0\n",
      "[[2.60683034]]\n",
      "1.0\n",
      "[[2.83484174]]\n",
      "1.0\n",
      "[[2.67133357]]\n",
      "1.0\n",
      "[[2.35361768]]\n",
      "1.0\n",
      "[[2.50257513]]\n",
      "1.0\n",
      "[[2.74423721]]\n",
      "1.0\n",
      "[[2.83019151]]\n",
      "1.0\n",
      "[[1.27641383]]\n",
      "1.0\n",
      "[[1.71533577]]\n",
      "1.0\n",
      "[[0.7059353]]\n",
      "1.0\n",
      "[[0.44372219]]\n",
      "1.0\n",
      "[[2.88569429]]\n",
      "1.0\n",
      "[[1.06970349]]\n",
      "1.0\n",
      "[[0.79698986]]\n",
      "1.0\n",
      "[[2.67613381]]\n",
      "1.0\n",
      "[[1.88859443]]\n",
      "1.0\n",
      "[[0.73338668]]\n",
      "1.0\n",
      "[[0.37666884]]\n",
      "1.0\n",
      "[[2.81549078]]\n",
      "1.0\n",
      "[[1.37211861]]\n",
      "1.0\n",
      "[[0.51497576]]\n",
      "1.0\n",
      "[[2.16355818]]\n",
      "1.0\n",
      "[[2.31296565]]\n",
      "1.0\n",
      "[[2.39096955]]\n",
      "1.0\n",
      "[[1.51672584]]\n",
      "1.0\n",
      "[[2.41122056]]\n",
      "1.0\n",
      "[[2.42097105]]\n",
      "1.0\n",
      "[[1.56757838]]\n",
      "1.0\n",
      "[[0.71808591]]\n",
      "1.0\n",
      "[[1.2378619]]\n",
      "1.0\n",
      "[[2.23571179]]\n",
      "1.0\n",
      "[[0.479874]]\n",
      "1.0\n",
      "[[0.45227262]]\n",
      "1.0\n",
      "[[1.99479974]]\n",
      "1.0\n",
      "[[0.88324417]]\n",
      "1.0\n",
      "[[0.90799541]]\n",
      "1.0\n",
      "[[0.07650383]]\n",
      "1.0\n",
      "[[1.65953298]]\n",
      "1.0\n",
      "[[0.23251163]]\n",
      "1.0\n",
      "[[2.08030402]]\n",
      "1.0\n",
      "[[1.91949598]]\n",
      "1.0\n",
      "[[2.15515776]]\n",
      "1.0\n",
      "[[1.92339617]]\n",
      "1.0\n",
      "[[2.68933447]]\n",
      "1.0\n",
      "[[1.12685635]]\n",
      "1.0\n",
      "[[2.60473024]]\n",
      "1.0\n",
      "[[1.83144158]]\n",
      "1.0\n",
      "[[1.12715636]]\n",
      "1.0\n",
      "[[0.01020052]]\n",
      "1.0\n",
      "[[1.21206061]]\n",
      "1.0\n",
      "[[2.28911446]]\n",
      "1.0\n",
      "[[2.52222611]]\n",
      "1.0\n",
      "[[0.10725537]]\n",
      "1.0\n",
      "[[0.18420922]]\n",
      "1.0\n",
      "[[0.07965399]]\n",
      "1.0\n",
      "[[0.69363469]]\n",
      "1.0\n",
      "[[0.0939047]]\n",
      "1.0\n",
      "[[2.28626432]]\n",
      "1.0\n",
      "[[2.73838692]]\n",
      "1.0\n",
      "[[1.12100606]]\n",
      "1.0\n",
      "[[2.26751338]]\n",
      "1.0\n",
      "[[0.45047253]]\n",
      "1.0\n",
      "[[0.58442923]]\n",
      "1.0\n",
      "[[0.16890845]]\n",
      "1.0\n",
      "[[1.1018051]]\n",
      "1.0\n",
      "[[0.67473374]]\n",
      "1.0\n",
      "[[2.68453423]]\n",
      "1.0\n",
      "[[2.49942497]]\n",
      "1.0\n",
      "[[2.19175959]]\n",
      "1.0\n",
      "[[0.56942848]]\n",
      "1.0\n",
      "[[1.54837742]]\n",
      "1.0\n",
      "[[1.02665134]]\n",
      "1.0\n",
      "[[0.2257613]]\n",
      "1.0\n",
      "[[0.47327367]]\n",
      "1.0\n",
      "[[0.70233512]]\n",
      "1.0\n",
      "[[1.67978399]]\n",
      "1.0\n",
      "[[2.6059303]]\n",
      "1.0\n",
      "[[1.22346118]]\n",
      "1.0\n",
      "[[2.22521126]]\n",
      "1.0\n",
      "[[2.20076004]]\n",
      "1.0\n",
      "[[1.08905446]]\n",
      "1.0\n",
      "[[0.46847343]]\n",
      "1.0\n",
      "[[0.0817541]]\n",
      "1.0\n",
      "[[2.82149108]]\n",
      "1.0\n",
      "[[0.47867394]]\n",
      "1.0\n",
      "[[2.93429672]]\n",
      "1.0\n",
      "[[0.03930197]]\n",
      "1.0\n",
      "[[2.29031452]]\n",
      "1.0\n",
      "[[0.08055404]]\n",
      "1.0\n",
      "[[1.88934447]]\n",
      "1.0\n",
      "[[1.0319016]]\n",
      "1.0\n",
      "[[0.86269314]]\n",
      "1.0\n",
      "[[1.91334567]]\n",
      "1.0\n",
      "[[2.39471974]]\n",
      "1.0\n",
      "[[1.49572479]]\n",
      "1.0\n",
      "[[1.1259563]]\n",
      "1.0\n",
      "[[1.34931747]]\n",
      "1.0\n",
      "[[2.90414521]]\n",
      "1.0\n",
      "[[0.95869794]]\n",
      "1.0\n",
      "[[2.23886195]]\n",
      "1.0\n",
      "[[2.95634782]]\n",
      "1.0\n",
      "[[0.31516577]]\n",
      "1.0\n",
      "[[0.96754838]]\n",
      "1.0\n",
      "[[0.40937048]]\n",
      "1.0\n",
      "[[1.72898645]]\n",
      "1.0\n",
      "[[0.46502326]]\n",
      "1.0\n",
      "[[2.11870594]]\n",
      "1.0\n",
      "[[0.95344768]]\n",
      "1.0\n",
      "[[2.8879444]]\n",
      "1.0\n",
      "[[1.12745638]]\n",
      "1.0\n",
      "[[2.78533927]]\n",
      "1.0\n",
      "[[2.95979799]]\n",
      "1.0\n",
      "[[2.15245763]]\n",
      "1.0\n",
      "[[0.20266014]]\n",
      "1.0\n",
      "[[0.60288015]]\n",
      "1.0\n",
      "[[0.11940598]]\n",
      "1.0\n",
      "[[2.53032652]]\n",
      "1.0\n",
      "[[0.73173659]]\n",
      "1.0\n",
      "[[0.02040103]]\n",
      "1.0\n",
      "[[0.06420322]]\n",
      "1.0\n",
      "[[0.18465924]]\n",
      "1.0\n",
      "[[0.01350068]]\n",
      "1.0\n",
      "[[2.70868544]]\n",
      "1.0\n",
      "[[1.36851843]]\n",
      "1.0\n",
      "[[2.71138557]]\n",
      "1.0\n",
      "[[1.4879244]]\n",
      "1.0\n",
      "[[0.97069854]]\n",
      "1.0\n",
      "[[0.75438773]]\n",
      "1.0\n",
      "[[0.81919097]]\n",
      "1.0\n",
      "[[0.15000751]]\n",
      "1.0\n",
      "[[0.87319367]]\n",
      "1.0\n",
      "[[1.6238312]]\n",
      "1.0\n",
      "[[1.82904146]]\n",
      "1.0\n",
      "[[0.84649233]]\n",
      "1.0\n",
      "[[2.36966849]]\n",
      "1.0\n",
      "[[2.49102455]]\n",
      "1.0\n",
      "[[1.12520627]]\n",
      "1.0\n",
      "[[2.84294215]]\n",
      "1.0\n",
      "[[1.3938697]]\n",
      "1.0\n",
      "[[2.50227512]]\n",
      "1.0\n",
      "[[1.28676434]]\n",
      "1.0\n",
      "[[0.46622332]]\n",
      "1.0\n",
      "[[2.73433672]]\n",
      "1.0\n",
      "[[2.98559928]]\n",
      "1.0\n",
      "[[0.67968399]]\n",
      "1.0\n",
      "[[2.77123856]]\n",
      "1.0\n",
      "[[2.64103205]]\n",
      "1.0\n",
      "[[0.10050503]]\n",
      "1.0\n",
      "[[0.93094655]]\n",
      "1.0\n",
      "[[1.34211711]]\n",
      "1.0\n",
      "[[1.61813091]]\n",
      "1.0\n",
      "[[2.57367869]]\n",
      "1.0\n",
      "[[1.87134357]]\n",
      "1.0\n",
      "[[1.98444923]]\n",
      "1.0\n",
      "[[1.34166709]]\n",
      "1.0\n",
      "[[1.43737187]]\n",
      "1.0\n",
      "[[2.63308166]]\n",
      "1.0\n",
      "[[0.4678734]]\n",
      "1.0\n",
      "[[2.92424621]]\n",
      "1.0\n",
      "[[0.51872594]]\n",
      "1.0\n",
      "[[1.27266364]]\n",
      "1.0\n",
      "[[1.69778489]]\n",
      "1.0\n",
      "[[2.11375569]]\n",
      "1.0\n",
      "[[0.65223262]]\n",
      "1.0\n",
      "[[1.34661734]]\n",
      "1.0\n",
      "[[1.01120057]]\n",
      "1.0\n",
      "[[2.40282014]]\n",
      "1.0\n",
      "[[0.43802191]]\n",
      "1.0\n",
      "[[1.79573979]]\n",
      "1.0\n",
      "[[0.58487925]]\n",
      "1.0\n",
      "[[1.24971249]]\n",
      "1.0\n",
      "[[0.74838743]]\n",
      "1.0\n",
      "[[1.1958598]]\n",
      "1.0\n",
      "[[2.03350168]]\n",
      "1.0\n",
      "[[2.40612031]]\n",
      "1.0\n",
      "[[0.77943898]]\n",
      "1.0\n",
      "[[0.80479025]]\n",
      "1.0\n",
      "[[1.8819941]]\n",
      "1.0\n",
      "[[2.03755188]]\n",
      "1.0\n",
      "[[0.50342518]]\n",
      "1.0\n",
      "[[2.01550078]]\n",
      "1.0\n",
      "[[1.78568929]]\n",
      "1.0\n",
      "[[1.00010001]]\n",
      "1.0\n",
      "[[0.34771739]]\n",
      "1.0\n",
      "[[2.33471674]]\n",
      "1.0\n",
      "[[0.70863544]]\n",
      "1.0\n",
      "[[1.90539527]]\n",
      "1.0\n",
      "[[2.34521726]]\n",
      "1.0\n",
      "[[0.93499676]]\n",
      "1.0\n",
      "[[2.4659733]]\n",
      "1.0\n",
      "[[0.31921597]]\n",
      "1.0\n",
      "[[1.82079104]]\n",
      "1.0\n",
      "[[1.76903846]]\n",
      "1.0\n",
      "[[0.87514376]]\n",
      "1.0\n",
      "[[2.620031]]\n",
      "1.0\n",
      "[[1.58572929]]\n",
      "1.0\n",
      "[[2.92109606]]\n",
      "1.0\n",
      "[[2.31641582]]\n",
      "1.0\n",
      "[[1.23426172]]\n",
      "1.0\n",
      "[[1.67933397]]\n",
      "1.0\n",
      "[[0.53522677]]\n",
      "1.0\n",
      "[[1.48462424]]\n",
      "1.0\n",
      "[[0.87124357]]\n",
      "1.0\n",
      "[[0.75213761]]\n",
      "1.0\n",
      "[[1.47052353]]\n",
      "1.0\n",
      "[[2.14915746]]\n",
      "1.0\n",
      "[[1.53967699]]\n",
      "1.0\n",
      "[[0.98059904]]\n",
      "1.0\n",
      "[[2.62963148]]\n",
      "1.0\n",
      "[[1.5239262]]\n",
      "1.0\n",
      "[[0.60903046]]\n",
      "1.0\n",
      "[[0.21841093]]\n",
      "1.0\n",
      "[[2.54037702]]\n",
      "1.0\n",
      "[[0.14655734]]\n",
      "1.0\n",
      "[[0.45677285]]\n",
      "1.0\n",
      "[[1.08320417]]\n",
      "1.0\n",
      "[[2.68438422]]\n",
      "1.0\n",
      "[[1.98714936]]\n",
      "1.0\n",
      "[[0.14850743]]\n",
      "1.0\n",
      "[[2.62648133]]\n",
      "1.0\n",
      "[[0.8257913]]\n",
      "1.0\n",
      "[[2.95184759]]\n",
      "1.0\n",
      "[[1.12415621]]\n",
      "1.0\n",
      "[[1.94544728]]\n",
      "1.0\n",
      "[[2.05315266]]\n",
      "1.0\n",
      "[[0.10965549]]\n",
      "1.0\n",
      "[[2.12845643]]\n",
      "1.0\n",
      "[[0.53972699]]\n",
      "1.0\n",
      "[[1.84044203]]\n",
      "1.0\n",
      "[[2.97254863]]\n",
      "1.0\n",
      "[[2.09530477]]\n",
      "1.0\n",
      "[[2.39486975]]\n",
      "1.0\n",
      "[[2.46522326]]\n",
      "1.0\n",
      "[[2.15755788]]\n",
      "1.0\n",
      "[[2.61973099]]\n",
      "1.0\n",
      "[[1.98174909]]\n",
      "1.0\n",
      "[[0.53657684]]\n",
      "1.0\n",
      "[[1.17725887]]\n",
      "1.0\n",
      "[[2.01805091]]\n",
      "1.0\n",
      "[[0.16305816]]\n",
      "1.0\n",
      "[[2.20646033]]\n",
      "1.0\n",
      "[[1.66928347]]\n",
      "1.0\n",
      "[[2.21246063]]\n",
      "1.0\n",
      "[[0.55817792]]\n",
      "1.0\n",
      "[[0.44897246]]\n",
      "1.0\n",
      "[[0.94864744]]\n",
      "1.0\n",
      "[[0.07725387]]\n",
      "1.0\n",
      "[[1.08740438]]\n",
      "1.0\n",
      "[[1.37136857]]\n",
      "1.0\n",
      "[[2.4859243]]\n",
      "1.0\n",
      "[[0.59957999]]\n",
      "1.0\n",
      "[[2.14030702]]\n",
      "1.0\n",
      "[[1.61543078]]\n",
      "1.0\n",
      "[[1.2158108]]\n",
      "1.0\n",
      "[[1.90344518]]\n",
      "1.0\n",
      "[[2.50122506]]\n",
      "1.0\n",
      "[[0.67908396]]\n",
      "1.0\n",
      "[[0.59222962]]\n",
      "1.0\n",
      "[[1.97544878]]\n",
      "1.0\n",
      "[[1.78868944]]\n",
      "1.0\n",
      "[[2.58777939]]\n",
      "1.0\n",
      "[[2.19445973]]\n",
      "1.0\n",
      "[[2.00260013]]\n",
      "1.0\n",
      "[[2.50407521]]\n",
      "1.0\n",
      "[[0.07410371]]\n",
      "1.0\n",
      "[[0.3378169]]\n",
      "1.0\n",
      "[[2.83229162]]\n",
      "1.0\n",
      "[[1.01210061]]\n",
      "1.0\n",
      "[[0.98014901]]\n",
      "1.0\n",
      "[[2.9279964]]\n",
      "1.0\n",
      "[[2.6558828]]\n",
      "1.0\n",
      "[[1.04930247]]\n",
      "1.0\n",
      "[[2.49372469]]\n",
      "1.0\n",
      "[[0.50297516]]\n",
      "1.0\n",
      "[[0.17340868]]\n",
      "1.0\n",
      "[[1.3079154]]\n",
      "1.0\n",
      "[[0.62508126]]\n",
      "1.0\n",
      "[[1.46962349]]\n",
      "1.0\n",
      "[[2.87609381]]\n",
      "1.0\n",
      "[[0.43937198]]\n",
      "1.0\n",
      "[[0.9099455]]\n",
      "1.0\n",
      "[[1.51927597]]\n",
      "1.0\n",
      "[[1.83759188]]\n",
      "1.0\n",
      "[[2.60953048]]\n",
      "1.0\n",
      "[[2.92034602]]\n",
      "1.0\n",
      "[[1.20201011]]\n",
      "1.0\n",
      "[[1.48087405]]\n",
      "1.0\n",
      "[[0.93619682]]\n",
      "1.0\n",
      "[[2.67448373]]\n",
      "1.0\n",
      "[[2.82959148]]\n",
      "1.0\n",
      "[[2.17900895]]\n",
      "1.0\n",
      "[[1.80624032]]\n",
      "1.0\n",
      "[[1.35291765]]\n",
      "1.0\n",
      "[[2.1220061]]\n",
      "1.0\n",
      "[[0.78153908]]\n",
      "1.0\n",
      "[[0.46997351]]\n",
      "1.0\n",
      "[[0.45407271]]\n",
      "1.0\n",
      "[[0.93319667]]\n",
      "1.0\n",
      "[[2.01040052]]\n",
      "1.0\n",
      "[[0.039902]]\n",
      "1.0\n",
      "[[1.64153208]]\n",
      "1.0\n",
      "[[0.6678334]]\n",
      "1.0\n",
      "[[0.02715137]]\n",
      "1.0\n",
      "[[1.80264014]]\n",
      "1.0\n",
      "[[0.55352768]]\n",
      "1.0\n",
      "[[2.5418771]]\n",
      "1.0\n",
      "[[0.07425372]]\n",
      "1.0\n",
      "[[2.48487425]]\n",
      "1.0\n",
      "[[2.90474524]]\n",
      "1.0\n",
      "[[2.97734887]]\n",
      "1.0\n",
      "[[0.19110956]]\n",
      "1.0\n",
      "[[2.98754938]]\n",
      "1.0\n",
      "[[1.10015501]]\n",
      "1.0\n",
      "[[1.66328317]]\n",
      "1.0\n",
      "[[0.05310266]]\n",
      "1.0\n",
      "[[2.00275014]]\n",
      "1.0\n",
      "[[2.63623181]]\n",
      "1.0\n",
      "[[1.05815291]]\n",
      "1.0\n",
      "[[0.70293515]]\n",
      "1.0\n",
      "[[1.96734837]]\n",
      "1.0\n",
      "[[1.76603831]]\n",
      "1.0\n",
      "[[2.52012601]]\n",
      "1.0\n",
      "[[1.69898495]]\n",
      "1.0\n",
      "[[1.81464074]]\n",
      "1.0\n",
      "[[1.08305416]]\n",
      "1.0\n",
      "[[1.8078904]]\n",
      "1.0\n",
      "[[1.76858843]]\n",
      "1.0\n",
      "[[1.43707186]]\n",
      "1.0\n",
      "[[1.14080705]]\n",
      "1.0\n",
      "[[2.86709336]]\n",
      "1.0\n",
      "[[2.40042002]]\n",
      "1.0\n",
      "[[0.20866044]]\n",
      "1.0\n",
      "[[1.27161359]]\n",
      "1.0\n",
      "[[0.58022902]]\n",
      "1.0\n",
      "[[1.02740138]]\n",
      "1.0\n",
      "[[0.92419622]]\n",
      "1.0\n",
      "[[2.99669984]]\n",
      "1.0\n",
      "[[0.93334667]]\n",
      "1.0\n",
      "[[1.51102556]]\n",
      "1.0\n",
      "[[2.53362668]]\n",
      "1.0\n",
      "[[1.21941098]]\n",
      "1.0\n",
      "[[0.40247013]]\n",
      "1.0\n",
      "[[0.6718836]]\n",
      "1.0\n",
      "[[2.0359018]]\n",
      "1.0\n",
      "[[2.02750138]]\n",
      "1.0\n",
      "[[1.35156758]]\n",
      "1.0\n",
      "[[1.87224362]]\n",
      "1.0\n",
      "[[1.59142958]]\n",
      "1.0\n",
      "[[0.04830242]]\n",
      "1.0\n",
      "[[2.74693735]]\n",
      "1.0\n",
      "[[0.84064204]]\n",
      "1.0\n",
      "[[1.80114006]]\n",
      "1.0\n",
      "[[0.14955749]]\n",
      "1.0\n",
      "[[1.93164659]]\n",
      "1.0\n",
      "[[1.50022502]]\n",
      "1.0\n",
      "[[2.69023451]]\n",
      "1.0\n",
      "[[1.23906196]]\n",
      "1.0\n",
      "[[0.30106506]]\n",
      "1.0\n",
      "[[1.59862994]]\n",
      "1.0\n",
      "[[1.73648683]]\n",
      "1.0\n",
      "[[2.58732937]]\n",
      "1.0\n",
      "[[2.78338917]]\n",
      "1.0\n",
      "[[0.23911196]]\n",
      "1.0\n",
      "[[0.31621582]]\n",
      "1.0\n",
      "[[1.06235312]]\n",
      "1.0\n",
      "[[1.48282415]]\n",
      "1.0\n",
      "[[0.25081255]]\n",
      "1.0\n",
      "[[2.54262713]]\n",
      "1.0\n",
      "[[0.39977]]\n",
      "1.0\n",
      "[[1.8219911]]\n",
      "1.0\n",
      "[[0.79518977]]\n",
      "1.0\n",
      "[[1.16135807]]\n",
      "1.0\n",
      "[[2.94614731]]\n",
      "1.0\n",
      "[[2.97464873]]\n",
      "1.0\n",
      "[[0.20731037]]\n",
      "1.0\n",
      "[[1.57897895]]\n",
      "1.0\n",
      "[[0.87349368]]\n",
      "1.0\n",
      "[[2.81864093]]\n",
      "1.0\n",
      "[[0.63048153]]\n",
      "1.0\n",
      "[[0.57677885]]\n",
      "1.0\n",
      "[[2.98364918]]\n",
      "1.0\n",
      "[[1.25016251]]\n",
      "1.0\n",
      "[[2.39276964]]\n",
      "1.0\n",
      "[[0.25201261]]\n",
      "1.0\n",
      "[[0.54632732]]\n",
      "1.0\n",
      "[[2.52237612]]\n",
      "1.0\n",
      "[[2.85569279]]\n",
      "1.0\n",
      "[[0.73398671]]\n",
      "1.0\n",
      "[[2.28176409]]\n",
      "1.0\n",
      "[[2.54307716]]\n",
      "1.0\n",
      "[[0.6138307]]\n",
      "1.0\n",
      "[[1.94049703]]\n",
      "1.0\n",
      "[[0.01560079]]\n",
      "1.0\n",
      "[[2.70838542]]\n",
      "1.0\n",
      "[[0.1918596]]\n",
      "1.0\n",
      "[[1.78913946]]\n",
      "1.0\n",
      "[[0.35221762]]\n",
      "1.0\n",
      "[[1.72718636]]\n",
      "1.0\n",
      "[[1.30926547]]\n",
      "1.0\n",
      "[[2.41662083]]\n",
      "1.0\n",
      "[[1.63148158]]\n",
      "1.0\n",
      "[[0.70473524]]\n",
      "1.0\n",
      "[[2.34851743]]\n",
      "1.0\n",
      "[[1.47202361]]\n",
      "1.0\n",
      "[[0.73233662]]\n",
      "1.0\n",
      "[[1.89129457]]\n",
      "1.0\n",
      "[[1.8359418]]\n",
      "1.0\n",
      "[[1.29621482]]\n",
      "1.0\n",
      "[[1.54732737]]\n",
      "1.0\n",
      "[[1.89669484]]\n",
      "1.0\n",
      "[[1.7039352]]\n",
      "1.0\n",
      "[[0.6159308]]\n",
      "1.0\n",
      "[[0.27406371]]\n",
      "1.0\n",
      "[[2.38931947]]\n",
      "1.0\n",
      "[[0.37846893]]\n",
      "1.0\n",
      "[[2.1239562]]\n",
      "1.0\n",
      "[[0.56897846]]\n",
      "1.0\n",
      "[[2.07910396]]\n",
      "1.0\n",
      "[[0.82474124]]\n",
      "1.0\n",
      "[[2.59362968]]\n",
      "1.0\n",
      "[[1.43032152]]\n",
      "1.0\n",
      "[[2.699985]]\n",
      "1.0\n",
      "[[1.96764839]]\n",
      "1.0\n",
      "[[1.05020252]]\n",
      "1.0\n",
      "[[2.89694485]]\n",
      "1.0\n",
      "[[2.51472574]]\n",
      "1.0\n",
      "[[1.67228362]]\n",
      "1.0\n",
      "[[0.23536178]]\n",
      "1.0\n",
      "[[0.30856544]]\n",
      "1.0\n",
      "[[2.12920646]]\n",
      "1.0\n",
      "[[0.36931847]]\n",
      "1.0\n",
      "[[2.38526927]]\n",
      "1.0\n",
      "[[2.99099955]]\n",
      "1.0\n",
      "[[1.22001101]]\n",
      "1.0\n",
      "[[2.65828292]]\n",
      "1.0\n",
      "[[2.93759688]]\n",
      "1.0\n",
      "[[1.50322517]]\n",
      "1.0\n",
      "[[0.68748438]]\n",
      "1.0\n",
      "[[0.16020802]]\n",
      "1.0\n",
      "[[0.99109956]]\n",
      "1.0\n",
      "[[0.77133857]]\n",
      "1.0\n",
      "[[2.0939547]]\n",
      "1.0\n",
      "[[2.860043]]\n",
      "1.0\n",
      "[[0.04140208]]\n",
      "1.0\n",
      "[[0.9898995]]\n",
      "1.0\n",
      "[[1.78373919]]\n",
      "1.0\n",
      "[[2.57052853]]\n",
      "1.0\n",
      "[[0.7158358]]\n",
      "1.0\n",
      "[[0.85204261]]\n",
      "1.0\n",
      "[[1.41727087]]\n",
      "1.0\n",
      "[[0.57047853]]\n",
      "1.0\n",
      "[[1.30011501]]\n",
      "1.0\n",
      "[[2.33711686]]\n",
      "1.0\n",
      "[[2.78218911]]\n",
      "1.0\n",
      "[[2.33006651]]\n",
      "1.0\n",
      "[[2.40852043]]\n",
      "1.0\n",
      "[[0.74073704]]\n",
      "1.0\n",
      "[[0.48887445]]\n",
      "1.0\n",
      "[[1.76708836]]\n",
      "1.0\n",
      "[[2.77138857]]\n",
      "1.0\n",
      "[[2.95844792]]\n",
      "1.0\n",
      "[[0.13035653]]\n",
      "1.0\n",
      "[[0.99859994]]\n",
      "1.0\n",
      "[[1.80219011]]\n",
      "1.0\n",
      "[[2.69218461]]\n",
      "1.0\n",
      "[[2.55057753]]\n",
      "1.0\n",
      "[[1.71098555]]\n",
      "1.0\n",
      "[[1.33086655]]\n",
      "1.0\n",
      "[[0.71358569]]\n",
      "1.0\n",
      "[[0.62658134]]\n",
      "1.0\n",
      "[[1.99284965]]\n",
      "1.0\n",
      "[[0.07245363]]\n",
      "1.0\n",
      "[[1.19885995]]\n",
      "1.0\n",
      "[[2.29091455]]\n",
      "1.0\n",
      "[[1.78463924]]\n",
      "1.0\n",
      "[[2.92904645]]\n",
      "1.0\n",
      "[[0.91654583]]\n",
      "1.0\n",
      "[[0.66348318]]\n",
      "1.0\n",
      "[[1.60118006]]\n",
      "1.0\n",
      "[[0.7018851]]\n",
      "1.0\n",
      "[[2.74738737]]\n",
      "1.0\n",
      "[[2.44872244]]\n",
      "1.0\n",
      "[[1.26516326]]\n",
      "1.0\n",
      "[[2.39141957]]\n",
      "1.0\n",
      "[[0.28261414]]\n",
      "1.0\n",
      "[[2.98829942]]\n",
      "1.0\n",
      "[[0.75858794]]\n",
      "1.0\n",
      "[[0.74958749]]\n",
      "1.0\n",
      "[[0.23881195]]\n",
      "1.0\n",
      "[[2.55132757]]\n",
      "1.0\n",
      "[[0.25216262]]\n",
      "1.0\n",
      "[[1.7019851]]\n",
      "1.0\n",
      "[[1.58152908]]\n",
      "1.0\n",
      "[[1.91919596]]\n",
      "1.0\n",
      "[[2.56092805]]\n",
      "1.0\n",
      "[[2.58342917]]\n",
      "1.0\n",
      "[[0.77208861]]\n",
      "1.0\n",
      "[[1.44067204]]\n",
      "1.0\n",
      "[[0.89374469]]\n",
      "1.0\n",
      "[[1.4499725]]\n",
      "1.0\n",
      "[[1.87464374]]\n",
      "1.0\n",
      "[[1.23936197]]\n",
      "1.0\n",
      "[[0.63813191]]\n",
      "1.0\n",
      "[[0.91339568]]\n",
      "1.0\n",
      "[[2.66068304]]\n",
      "1.0\n",
      "[[2.23361168]]\n",
      "1.0\n",
      "[[1.78298915]]\n",
      "1.0\n",
      "[[0.85714286]]\n",
      "1.0\n",
      "[[2.09350468]]\n",
      "1.0\n",
      "[[0.49037453]]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47747388]]\n",
      "1.0\n",
      "[[0.059853]]\n",
      "1.0\n",
      "[[2.82554128]]\n",
      "1.0\n",
      "[[2.39981999]]\n",
      "1.0\n",
      "[[1.72448623]]\n",
      "1.0\n",
      "[[2.97134857]]\n",
      "1.0\n",
      "[[0.54962749]]\n",
      "1.0\n",
      "[[2.79913996]]\n",
      "1.0\n",
      "[[0.17520877]]\n",
      "1.0\n",
      "[[1.04090205]]\n",
      "1.0\n",
      "[[2.71333567]]\n",
      "1.0\n",
      "[[1.18685935]]\n",
      "1.0\n",
      "[[2.45022251]]\n",
      "1.0\n",
      "[[2.85869294]]\n",
      "1.0\n",
      "[[2.6119306]]\n",
      "1.0\n",
      "[[0.34051703]]\n",
      "1.0\n",
      "[[2.56467824]]\n",
      "1.0\n",
      "[[2.16850843]]\n",
      "1.0\n",
      "[[2.04535227]]\n",
      "1.0\n",
      "[[2.94164708]]\n",
      "1.0\n",
      "[[1.72373619]]\n",
      "1.0\n",
      "[[2.19040952]]\n",
      "1.0\n",
      "[[0.24736238]]\n",
      "1.0\n",
      "[[2.47497375]]\n",
      "1.0\n",
      "[[2.02225112]]\n",
      "1.0\n",
      "[[0.40562029]]\n",
      "1.0\n",
      "[[2.1079054]]\n",
      "1.0\n",
      "[[2.88554428]]\n",
      "1.0\n",
      "[[2.45697285]]\n",
      "1.0\n",
      "[[2.21681084]]\n",
      "1.0\n",
      "[[0.19890995]]\n",
      "1.0\n",
      "[[1.97979899]]\n",
      "1.0\n",
      "[[2.5679784]]\n",
      "1.0\n",
      "[[0.0478524]]\n",
      "1.0\n",
      "[[0.30601531]]\n",
      "1.0\n",
      "[[2.30846543]]\n",
      "1.0\n",
      "[[0.45347268]]\n",
      "1.0\n",
      "[[0.17295866]]\n",
      "1.0\n",
      "[[2.86139307]]\n",
      "1.0\n",
      "[[0.81934097]]\n",
      "1.0\n",
      "[[1.43917196]]\n",
      "1.0\n",
      "[[2.67433372]]\n",
      "1.0\n",
      "[[2.81669084]]\n",
      "1.0\n",
      "[[2.91929597]]\n",
      "1.0\n",
      "[[1.4238712]]\n",
      "1.0\n",
      "[[2.46582329]]\n",
      "1.0\n",
      "[[1.05695285]]\n",
      "1.0\n",
      "[[0.6498325]]\n",
      "1.0\n",
      "[[1.88229412]]\n",
      "1.0\n",
      "[[0.3678184]]\n",
      "1.0\n",
      "[[2.06470324]]\n",
      "1.0\n",
      "[[1.64948248]]\n",
      "1.0\n",
      "[[0.45512276]]\n",
      "1.0\n",
      "[[1.91559578]]\n",
      "1.0\n",
      "[[2.56122806]]\n",
      "1.0\n",
      "[[0.01500076]]\n",
      "1.0\n",
      "[[1.00310016]]\n",
      "1.0\n",
      "[[2.10430522]]\n",
      "1.0\n",
      "[[2.26721336]]\n",
      "1.0\n",
      "[[2.52327617]]\n",
      "1.0\n",
      "[[2.24096205]]\n",
      "1.0\n",
      "[[0.27841393]]\n",
      "1.0\n",
      "[[2.54607731]]\n",
      "1.0\n",
      "[[0.73008651]]\n",
      "1.0\n",
      "[[1.43602181]]\n",
      "1.0\n",
      "[[2.50272514]]\n",
      "1.0\n",
      "[[2.05210261]]\n",
      "1.0\n",
      "[[1.88094405]]\n",
      "1.0\n",
      "[[2.47707386]]\n",
      "1.0\n",
      "[[2.93114656]]\n",
      "1.0\n",
      "[[1.5659283]]\n",
      "1.0\n",
      "[[0.62223112]]\n",
      "1.0\n",
      "[[2.72548628]]\n",
      "1.0\n",
      "[[1.18115906]]\n",
      "1.0\n",
      "[[2.79853993]]\n",
      "1.0\n",
      "[[2.54472724]]\n",
      "1.0\n",
      "[[1.66058303]]\n",
      "1.0\n",
      "[[1.41007051]]\n",
      "1.0\n",
      "[[1.52422622]]\n",
      "1.0\n",
      "[[2.35961798]]\n",
      "1.0\n",
      "[[1.99209961]]\n",
      "1.0\n",
      "[[2.67118356]]\n",
      "1.0\n",
      "[[0.60468024]]\n",
      "1.0\n",
      "[[1.20411021]]\n",
      "1.0\n",
      "[[1.83294165]]\n",
      "1.0\n",
      "[[2.57742887]]\n",
      "1.0\n",
      "[[0.44447223]]\n",
      "1.0\n",
      "[[2.8639432]]\n",
      "1.0\n",
      "[[1.63013151]]\n",
      "1.0\n",
      "[[2.47452373]]\n",
      "1.0\n",
      "[[2.80109006]]\n",
      "1.0\n",
      "[[1.96164809]]\n",
      "1.0\n",
      "[[0.46397321]]\n",
      "1.0\n",
      "[[1.91259563]]\n",
      "1.0\n",
      "[[1.24026202]]\n",
      "1.0\n",
      "[[1.55002751]]\n",
      "1.0\n",
      "[[0.83104156]]\n",
      "1.0\n",
      "[[0.11655584]]\n",
      "1.0\n",
      "[[1.54417721]]\n",
      "1.0\n",
      "[[0.43682185]]\n",
      "1.0\n",
      "[[2.2358618]]\n",
      "1.0\n",
      "[[0.7618881]]\n",
      "1.0\n",
      "[[0.82069104]]\n",
      "1.0\n",
      "[[0.27331367]]\n",
      "1.0\n",
      "[[0.49157459]]\n",
      "1.0\n",
      "[[0.49937498]]\n",
      "1.0\n",
      "[[0.51152558]]\n",
      "1.0\n",
      "[[0.00045003]]\n",
      "1.0\n",
      "[[2.93084654]]\n",
      "1.0\n",
      "[[1.28721437]]\n",
      "1.0\n",
      "[[2.20226012]]\n",
      "1.0\n",
      "[[0.37561879]]\n",
      "1.0\n",
      "[[1.81614081]]\n",
      "1.0\n",
      "[[2.13610681]]\n",
      "1.0\n",
      "[[1.22061104]]\n",
      "1.0\n",
      "[[0.74103706]]\n",
      "1.0\n",
      "[[2.30291515]]\n",
      "1.0\n",
      "[[0.34246713]]\n",
      "1.0\n",
      "[[1.77278864]]\n",
      "1.0\n",
      "[[0.00015002]]\n",
      "1.0\n",
      "[[0.4438722]]\n",
      "1.0\n",
      "[[1.14875744]]\n",
      "1.0\n",
      "[[2.04445223]]\n",
      "1.0\n",
      "[[1.32471624]]\n",
      "1.0\n",
      "[[0.4777739]]\n",
      "1.0\n",
      "[[2.39576979]]\n",
      "1.0\n",
      "[[0.36856844]]\n",
      "1.0\n",
      "[[1.91454573]]\n",
      "1.0\n",
      "[[1.35051753]]\n",
      "1.0\n",
      "[[1.62683135]]\n",
      "1.0\n",
      "[[1.6179809]]\n",
      "1.0\n",
      "[[1.45042253]]\n",
      "1.0\n",
      "[[0.53822692]]\n",
      "1.0\n",
      "[[0.84574229]]\n",
      "1.0\n",
      "[[1.5419271]]\n",
      "1.0\n",
      "[[0.20611031]]\n",
      "1.0\n",
      "[[0.3157658]]\n",
      "1.0\n",
      "[[1.65533277]]\n",
      "1.0\n",
      "[[1.3559178]]\n",
      "1.0\n",
      "[[2.06650333]]\n",
      "1.0\n",
      "[[1.5779289]]\n",
      "1.0\n",
      "[[1.75118756]]\n",
      "1.0\n",
      "[[1.67168359]]\n",
      "1.0\n",
      "[[2.74828742]]\n",
      "1.0\n",
      "[[2.39216961]]\n",
      "1.0\n",
      "[[1.29021452]]\n",
      "1.0\n",
      "[[2.43207161]]\n",
      "1.0\n",
      "[[2.63863193]]\n",
      "1.0\n",
      "[[2.08165409]]\n",
      "1.0\n",
      "[[2.74498725]]\n",
      "1.0\n",
      "[[0.26371319]]\n",
      "1.0\n",
      "[[0.32011601]]\n",
      "1.0\n",
      "[[1.5759788]]\n",
      "1.0\n",
      "[[2.54217711]]\n",
      "1.0\n",
      "[[2.41617081]]\n",
      "1.0\n",
      "[[1.75088755]]\n",
      "1.0\n",
      "[[1.26306316]]\n",
      "1.0\n",
      "[[1.39206961]]\n",
      "1.0\n",
      "[[0.43022152]]\n",
      "1.0\n",
      "[[1.44667234]]\n",
      "1.0\n",
      "[[1.39776989]]\n",
      "1.0\n",
      "[[2.90939547]]\n",
      "1.0\n",
      "[[1.59742988]]\n",
      "1.0\n",
      "[[1.29681485]]\n",
      "1.0\n",
      "[[2.35346768]]\n",
      "1.0\n",
      "[[0.78123907]]\n",
      "1.0\n",
      "[[0.8838442]]\n",
      "1.0\n",
      "[[0.00495026]]\n",
      "1.0\n",
      "[[2.47782389]]\n",
      "1.0\n",
      "[[0.58277915]]\n",
      "1.0\n",
      "[[2.35376769]]\n",
      "1.0\n",
      "[[0.66423322]]\n",
      "1.0\n",
      "[[2.56677834]]\n",
      "1.0\n",
      "[[1.85529277]]\n",
      "1.0\n",
      "[[1.25136257]]\n",
      "1.0\n",
      "[[0.15090755]]\n",
      "1.0\n",
      "[[0.56957849]]\n",
      "1.0\n",
      "[[0.87334367]]\n",
      "1.0\n",
      "[[2.08270414]]\n",
      "1.0\n",
      "[[2.98964948]]\n",
      "1.0\n",
      "[[1.45162259]]\n",
      "1.0\n",
      "[[0.27496376]]\n",
      "1.0\n",
      "[[0.48572429]]\n",
      "1.0\n",
      "[[2.45937297]]\n",
      "1.0\n",
      "[[2.33681684]]\n",
      "1.0\n",
      "[[1.4079704]]\n",
      "1.0\n",
      "[[1.94109706]]\n",
      "1.0\n",
      "[[0.28051403]]\n",
      "1.0\n",
      "[[0.03795191]]\n",
      "1.0\n",
      "[[1.03745188]]\n",
      "1.0\n",
      "[[2.12905646]]\n",
      "1.0\n",
      "[[2.81129057]]\n",
      "1.0\n",
      "[[1.78418921]]\n",
      "1.0\n",
      "[[2.18950948]]\n",
      "1.0\n",
      "[[2.62918146]]\n",
      "1.0\n",
      "[[2.24876244]]\n",
      "1.0\n",
      "[[1.15430772]]\n",
      "1.0\n",
      "[[0.89704486]]\n",
      "1.0\n",
      "[[2.24486225]]\n",
      "1.0\n",
      "[[0.70098506]]\n",
      "1.0\n",
      "[[0.6358818]]\n",
      "1.0\n",
      "[[2.80964048]]\n",
      "1.0\n",
      "[[1.59712986]]\n",
      "1.0\n",
      "[[0.92314616]]\n",
      "1.0\n",
      "[[0.19290965]]\n",
      "1.0\n",
      "[[0.30091505]]\n",
      "1.0\n",
      "[[0.34621732]]\n",
      "1.0\n",
      "[[2.40447023]]\n",
      "1.0\n",
      "[[1.84509226]]\n",
      "1.0\n",
      "[[0.1158058]]\n",
      "1.0\n",
      "[[1.48702436]]\n",
      "1.0\n",
      "[[0.79008951]]\n",
      "1.0\n",
      "[[2.7319366]]\n",
      "1.0\n",
      "[[0.68868444]]\n",
      "1.0\n",
      "[[0.52742638]]\n",
      "1.0\n",
      "[[2.01325067]]\n",
      "1.0\n",
      "[[1.54057703]]\n",
      "1.0\n",
      "[[1.82829142]]\n",
      "1.0\n",
      "[[1.96029802]]\n",
      "1.0\n",
      "[[0.3097655]]\n",
      "1.0\n",
      "[[0.4018701]]\n",
      "1.0\n",
      "[[2.27741387]]\n",
      "1.0\n",
      "[[2.09740487]]\n",
      "1.0\n",
      "[[0.35641783]]\n",
      "1.0\n",
      "[[0.06915347]]\n",
      "1.0\n",
      "[[2.05945298]]\n",
      "1.0\n",
      "[[0.67428372]]\n",
      "1.0\n",
      "[[1.20246013]]\n",
      "1.0\n",
      "[[0.67758389]]\n",
      "1.0\n",
      "[[2.00530027]]\n",
      "1.0\n",
      "[[0.12870644]]\n",
      "1.0\n",
      "[[2.6159808]]\n",
      "1.0\n",
      "[[1.89234462]]\n",
      "1.0\n",
      "[[1.56607831]]\n",
      "1.0\n",
      "[[0.63243163]]\n",
      "1.0\n",
      "[[1.75883795]]\n",
      "1.0\n",
      "[[0.77748888]]\n",
      "1.0\n",
      "[[1.9958498]]\n",
      "1.0\n",
      "[[1.21281065]]\n",
      "1.0\n",
      "[[1.8059403]]\n",
      "1.0\n",
      "[[0.35956799]]\n",
      "1.0\n",
      "[[0.28816442]]\n",
      "1.0\n",
      "[[2.00680034]]\n",
      "1.0\n",
      "[[2.99144957]]\n",
      "1.0\n",
      "[[1.719836]]\n",
      "1.0\n",
      "[[0.26521327]]\n",
      "1.0\n",
      "[[0.03345168]]\n",
      "1.0\n",
      "[[0.4738737]]\n",
      "1.0\n",
      "[[1.43872194]]\n",
      "1.0\n",
      "[[0.55367769]]\n",
      "1.0\n",
      "[[0.99619982]]\n",
      "1.0\n",
      "[[1.29051453]]\n",
      "1.0\n",
      "[[0.25861294]]\n",
      "1.0\n",
      "[[2.17540877]]\n",
      "1.0\n",
      "[[2.96099805]]\n",
      "1.0\n",
      "[[2.14075704]]\n",
      "1.0\n",
      "[[1.52332617]]\n",
      "1.0\n",
      "[[2.2499625]]\n",
      "1.0\n",
      "[[2.26016301]]\n",
      "1.0\n",
      "[[0.20461024]]\n",
      "1.0\n",
      "[[2.15035752]]\n",
      "1.0\n",
      "[[1.21146058]]\n",
      "1.0\n",
      "[[2.69698485]]\n",
      "1.0\n",
      "[[1.0718036]]\n",
      "1.0\n",
      "[[1.93089655]]\n",
      "1.0\n",
      "[[2.24831242]]\n",
      "1.0\n",
      "[[0.26821342]]\n",
      "1.0\n",
      "[[1.82694135]]\n",
      "1.0\n",
      "[[1.77173859]]\n",
      "1.0\n",
      "[[1.90044503]]\n",
      "1.0\n",
      "[[2.9980499]]\n",
      "1.0\n",
      "[[2.23901195]]\n",
      "1.0\n",
      "[[0.99469974]]\n",
      "1.0\n",
      "[[2.5658783]]\n",
      "1.0\n",
      "[[2.58882944]]\n",
      "1.0\n",
      "[[0.86704336]]\n",
      "1.0\n",
      "[[2.29931497]]\n",
      "1.0\n",
      "[[2.55012751]]\n",
      "1.0\n",
      "[[1.8858943]]\n",
      "1.0\n",
      "[[0.01830092]]\n",
      "1.0\n",
      "[[1.64513226]]\n",
      "1.0\n",
      "[[0.20881045]]\n",
      "1.0\n",
      "[[2.76448823]]\n",
      "1.0\n",
      "[[1.73048653]]\n",
      "1.0\n",
      "[[1.16675834]]\n",
      "1.0\n",
      "[[0.04920247]]\n",
      "1.0\n",
      "[[2.51217561]]\n",
      "1.0\n",
      "[[1.39266964]]\n",
      "1.0\n",
      "[[1.58377919]]\n",
      "1.0\n",
      "[[1.01255063]]\n",
      "1.0\n",
      "[[2.5879294]]\n",
      "1.0\n",
      "[[2.2540127]]\n",
      "1.0\n",
      "[[0.11340568]]\n",
      "1.0\n",
      "[[2.8859943]]\n",
      "1.0\n",
      "[[1.37661884]]\n",
      "1.0\n",
      "[[1.42717136]]\n",
      "1.0\n",
      "[[2.85374269]]\n",
      "1.0\n",
      "[[1.93509676]]\n",
      "1.0\n",
      "[[0.88249413]]\n",
      "1.0\n",
      "[[0.47417372]]\n",
      "1.0\n",
      "[[2.53917696]]\n",
      "1.0\n",
      "[[2.33621681]]\n",
      "1.0\n",
      "[[1.29876494]]\n",
      "1.0\n",
      "[[2.90669534]]\n",
      "1.0\n",
      "[[0.28636433]]\n",
      "1.0\n",
      "[[2.44437222]]\n",
      "1.0\n",
      "[[0.03240163]]\n",
      "1.0\n",
      "[[1.96569829]]\n",
      "1.0\n",
      "[[1.03115156]]\n",
      "1.0\n",
      "[[0.42872144]]\n",
      "1.0\n",
      "[[0.2497625]]\n",
      "1.0\n",
      "[[2.66923346]]\n",
      "1.0\n",
      "[[1.80459023]]\n",
      "1.0\n",
      "[[1.36431822]]\n",
      "1.0\n",
      "[[2.82044102]]\n",
      "1.0\n",
      "[[2.35541777]]\n",
      "1.0\n",
      "[[2.61478074]]\n",
      "1.0\n",
      "[[1.9238462]]\n",
      "1.0\n",
      "[[1.89204461]]\n",
      "1.0\n",
      "[[0.18615932]]\n",
      "1.0\n",
      "[[1.09040453]]\n",
      "1.0\n",
      "[[2.22161108]]\n",
      "1.0\n",
      "[[1.68938447]]\n",
      "1.0\n",
      "[[2.56827842]]\n",
      "1.0\n",
      "[[2.81654083]]\n",
      "1.0\n",
      "[[1.06145308]]\n",
      "1.0\n",
      "[[0.4357718]]\n",
      "1.0\n",
      "[[0.63963199]]\n",
      "1.0\n",
      "[[0.02655134]]\n",
      "1.0\n",
      "[[1.46302316]]\n",
      "1.0\n",
      "[[2.82299115]]\n",
      "1.0\n",
      "[[2.98109906]]\n",
      "1.0\n",
      "[[1.88889445]]\n",
      "1.0\n",
      "[[2.13220661]]\n",
      "1.0\n",
      "[[0.93049653]]\n",
      "1.0\n",
      "[[0.90679535]]\n",
      "1.0\n",
      "[[0.36841843]]\n",
      "1.0\n",
      "[[1.96929847]]\n",
      "1.0\n",
      "[[2.5360268]]\n",
      "1.0\n",
      "[[0.69333467]]\n",
      "1.0\n",
      "[[2.24456223]]\n",
      "1.0\n",
      "[[1.40167009]]\n",
      "1.0\n",
      "[[0.43127157]]\n",
      "1.0\n",
      "[[2.34836742]]\n",
      "1.0\n",
      "[[0.98464924]]\n",
      "1.0\n",
      "[[0.52052603]]\n",
      "1.0\n",
      "[[1.15415771]]\n",
      "1.0\n",
      "[[1.73663684]]\n",
      "1.0\n",
      "[[2.25611281]]\n",
      "1.0\n",
      "[[2.62753138]]\n",
      "1.0\n",
      "[[0.28681435]]\n",
      "1.0\n",
      "[[0.21511076]]\n",
      "1.0\n",
      "[[2.6879844]]\n",
      "1.0\n",
      "[[0.8418421]]\n",
      "1.0\n",
      "[[0.20911046]]\n",
      "1.0\n",
      "[[2.21471074]]\n",
      "1.0\n",
      "[[0.49097456]]\n",
      "1.0\n",
      "[[0.55202761]]\n",
      "1.0\n",
      "[[2.32061603]]\n",
      "1.0\n",
      "[[2.32946648]]\n",
      "1.0\n",
      "[[1.66688335]]\n",
      "1.0\n",
      "[[1.71698585]]\n",
      "1.0\n",
      "[[2.49297465]]\n",
      "1.0\n",
      "[[0.38146908]]\n",
      "1.0\n",
      "[[1.58422922]]\n",
      "1.0\n",
      "[[2.64043202]]\n",
      "1.0\n",
      "[[1.26321317]]\n",
      "1.0\n",
      "[[1.53682685]]\n",
      "1.0\n",
      "[[1.09835492]]\n",
      "1.0\n",
      "[[1.58857943]]\n",
      "1.0\n",
      "[[1.81479074]]\n",
      "1.0\n",
      "[[0.06270314]]\n",
      "1.0\n",
      "[[0.2718136]]\n",
      "1.0\n",
      "[[2.16580829]]\n",
      "1.0\n",
      "[[0.73563679]]\n",
      "1.0\n",
      "[[0.46067304]]\n",
      "1.0\n",
      "[[2.72608631]]\n",
      "1.0\n",
      "[[0.64113206]]\n",
      "1.0\n",
      "[[1.91034552]]\n",
      "1.0\n",
      "[[0.20641033]]\n",
      "1.0\n",
      "[[2.52207611]]\n",
      "1.0\n",
      "[[2.88704435]]\n",
      "1.0\n",
      "[[1.57807891]]\n",
      "1.0\n",
      "[[1.9838492]]\n",
      "1.0\n",
      "[[1.90164509]]\n",
      "1.0\n",
      "[[2.57157858]]\n",
      "1.0\n",
      "[[0.80974049]]\n",
      "1.0\n",
      "[[2.62183109]]\n",
      "1.0\n",
      "[[1.80084005]]\n",
      "1.0\n",
      "[[1.80744038]]\n",
      "1.0\n",
      "[[1.179809]]\n",
      "1.0\n",
      "[[1.5639782]]\n",
      "1.0\n",
      "[[0.62298116]]\n",
      "1.0\n",
      "[[1.26216311]]\n",
      "1.0\n",
      "[[1.44262214]]\n",
      "1.0\n",
      "[[2.86979349]]\n",
      "1.0\n",
      "[[1.03760189]]\n",
      "1.0\n",
      "[[2.56692835]]\n",
      "1.0\n",
      "[[2.40912046]]\n",
      "1.0\n",
      "[[0.41672084]]\n",
      "1.0\n",
      "[[2.81699085]]\n",
      "1.0\n",
      "[[1.21236062]]\n",
      "1.0\n",
      "[[1.43062154]]\n",
      "1.0\n",
      "[[0.34321717]]\n",
      "1.0\n",
      "[[0.50057504]]\n",
      "1.0\n",
      "[[1.80519026]]\n",
      "1.0\n",
      "[[0.79713986]]\n",
      "1.0\n",
      "[[0.43607181]]\n",
      "1.0\n",
      "[[2.22236112]]\n",
      "1.0\n",
      "[[1.55482775]]\n",
      "1.0\n",
      "[[2.74228712]]\n",
      "1.0\n",
      "[[1.18145908]]\n",
      "1.0\n",
      "[[1.52272614]]\n",
      "1.0\n",
      "[[0.31756589]]\n",
      "1.0\n",
      "[[1.07495375]]\n",
      "1.0\n",
      "[[0.74328717]]\n",
      "1.0\n",
      "[[2.81039052]]\n",
      "1.0\n",
      "[[1.33296665]]\n",
      "1.0\n",
      "[[2.4439222]]\n",
      "1.0\n",
      "[[1.83549178]]\n",
      "1.0\n",
      "[[0.83569179]]\n",
      "1.0\n",
      "[[2.86784339]]\n",
      "1.0\n",
      "[[1.37961899]]\n",
      "1.0\n",
      "[[1.50232512]]\n",
      "1.0\n",
      "[[2.37461873]]\n",
      "1.0\n",
      "[[1.94094705]]\n",
      "1.0\n",
      "[[2.67418371]]\n",
      "1.0\n",
      "[[2.1179559]]\n",
      "1.0\n",
      "[[2.40267014]]\n",
      "1.0\n",
      "[[2.78983949]]\n",
      "1.0\n",
      "[[2.85314266]]\n",
      "1.0\n",
      "[[1.57267864]]\n",
      "1.0\n",
      "[[0.91249563]]\n",
      "1.0\n",
      "[[1.73018651]]\n",
      "1.0\n",
      "[[1.54012701]]\n",
      "1.0\n",
      "[[2.06695335]]\n",
      "1.0\n",
      "[[0.44087205]]\n",
      "1.0\n",
      "[[2.27066354]]\n",
      "1.0\n",
      "[[0.6258313]]\n",
      "1.0\n",
      "[[1.07690385]]\n",
      "1.0\n",
      "[[2.22131107]]\n",
      "1.0\n",
      "[[0.22471124]]\n",
      "1.0\n",
      "[[2.83529177]]\n",
      "1.0\n",
      "[[1.69298465]]\n",
      "1.0\n",
      "[[1.04900246]]\n",
      "1.0\n",
      "[[0.73053653]]\n",
      "1.0\n",
      "[[1.32576629]]\n",
      "1.0\n",
      "[[1.53847693]]\n",
      "1.0\n",
      "[[0.02730137]]\n",
      "1.0\n",
      "[[0.5077754]]\n",
      "1.0\n",
      "[[2.00380019]]\n",
      "1.0\n",
      "[[1.80759038]]\n",
      "1.0\n",
      "[[2.0479024]]\n",
      "1.0\n",
      "[[0.42947148]]\n",
      "1.0\n",
      "[[0.62763139]]\n",
      "1.0\n",
      "[[1.5839292]]\n",
      "1.0\n",
      "[[1.2099605]]\n",
      "1.0\n",
      "[[1.83279164]]\n",
      "1.0\n",
      "[[0.29041453]]\n",
      "1.0\n",
      "[[0.71973599]]\n",
      "1.0\n",
      "[[1.53907696]]\n",
      "1.0\n",
      "[[1.44007201]]\n",
      "1.0\n",
      "[[0.10740538]]\n",
      "1.0\n",
      "[[1.53082655]]\n",
      "1.0\n",
      "[[1.1558078]]\n",
      "1.0\n",
      "[[0.6178809]]\n",
      "1.0\n",
      "[[0.68658434]]\n",
      "1.0\n",
      "[[1.99449973]]\n",
      "1.0\n",
      "[[1.18655933]]\n",
      "1.0\n",
      "[[2.05885295]]\n",
      "1.0\n",
      "[[2.06635332]]\n",
      "1.0\n",
      "[[1.93689685]]\n",
      "1.0\n",
      "[[0.20221012]]\n",
      "1.0\n",
      "[[1.41877094]]\n",
      "1.0\n",
      "[[1.03970199]]\n",
      "1.0\n",
      "[[1.6538327]]\n",
      "1.0\n",
      "[[1.38846943]]\n",
      "1.0\n",
      "[[1.47112356]]\n",
      "1.0\n",
      "[[1.15910796]]\n",
      "1.0\n",
      "[[2.10220511]]\n",
      "1.0\n",
      "[[2.36531827]]\n",
      "1.0\n",
      "[[0.78753938]]\n",
      "1.0\n",
      "[[0.24286215]]\n",
      "1.0\n",
      "[[2.46072304]]\n",
      "1.0\n",
      "[[0.08445423]]\n",
      "1.0\n",
      "[[1.30446523]]\n",
      "1.0\n",
      "[[0.9298965]]\n",
      "1.0\n",
      "[[0.70203511]]\n",
      "1.0\n",
      "[[0.90214511]]\n",
      "1.0\n",
      "[[1.27956398]]\n",
      "1.0\n",
      "[[2.66008301]]\n",
      "1.0\n",
      "[[1.50652533]]\n",
      "1.0\n",
      "[[0.11160559]]\n",
      "1.0\n",
      "[[1.13405671]]\n",
      "1.0\n",
      "[[2.6018801]]\n",
      "1.0\n",
      "[[0.30451523]]\n",
      "1.0\n",
      "[[0.57437873]]\n",
      "1.0\n",
      "[[1.19900996]]\n",
      "1.0\n",
      "[[1.17485875]]\n",
      "1.0\n",
      "[[2.1700085]]\n",
      "1.0\n",
      "[[2.459973]]\n",
      "1.0\n",
      "[[1.33611681]]\n",
      "1.0\n",
      "[[0.54617732]]\n",
      "1.0\n",
      "[[2.35121756]]\n",
      "1.0\n",
      "[[2.51127557]]\n",
      "1.0\n",
      "[[2.01745088]]\n",
      "1.0\n",
      "[[1.24266214]]\n",
      "1.0\n",
      "[[1.00025002]]\n",
      "1.0\n",
      "[[0.29266464]]\n",
      "1.0\n",
      "[[1.7078354]]\n",
      "1.0\n",
      "[[0.05715287]]\n",
      "1.0\n",
      "[[1.80534027]]\n",
      "1.0\n",
      "[[0.32641633]]\n",
      "1.0\n",
      "[[0.81769089]]\n",
      "1.0\n",
      "[[2.18680934]]\n",
      "1.0\n",
      "[[1.8878444]]\n",
      "1.0\n",
      "[[0.79398971]]\n",
      "1.0\n",
      "[[0.24541228]]\n",
      "1.0\n",
      "[[1.0278514]]\n",
      "1.0\n",
      "[[1.69763489]]\n",
      "1.0\n",
      "[[1.65668284]]\n",
      "1.0\n",
      "[[2.11570579]]\n",
      "1.0\n",
      "[[1.31106556]]\n",
      "1.0\n",
      "[[1.82709136]]\n",
      "1.0\n",
      "[[2.86409321]]\n",
      "1.0\n",
      "[[2.72338617]]\n",
      "1.0\n",
      "[[0.2778139]]\n",
      "1.0\n",
      "[[1.16165809]]\n",
      "1.0\n",
      "[[0.77103856]]\n",
      "1.0\n",
      "[[2.76958848]]\n",
      "1.0\n",
      "[[0.05100256]]\n",
      "1.0\n",
      "[[0.84229212]]\n",
      "1.0\n",
      "[[0.78768939]]\n",
      "1.0\n",
      "[[0.01005051]]\n",
      "1.0\n",
      "[[1.70603531]]\n",
      "1.0\n",
      "[[1.08125407]]\n",
      "1.0\n",
      "[[2.97374869]]\n",
      "1.0\n",
      "[[0.63153158]]\n",
      "1.0\n",
      "[[0.52532627]]\n",
      "1.0\n",
      "[[2.30966549]]\n",
      "1.0\n",
      "[[1.72238612]]\n",
      "1.0\n",
      "[[1.85424272]]\n",
      "1.0\n",
      "[[1.83954198]]\n",
      "1.0\n",
      "[[0.60513026]]\n",
      "1.0\n",
      "[[0.44147208]]\n",
      "1.0\n",
      "[[0.42077105]]\n",
      "1.0\n",
      "[[2.62948148]]\n",
      "1.0\n",
      "[[1.64048203]]\n",
      "1.0\n",
      "[[1.47457373]]\n",
      "1.0\n",
      "[[2.43057153]]\n",
      "1.0\n",
      "[[2.89424471]]\n",
      "1.0\n",
      "[[1.1679584]]\n",
      "1.0\n",
      "[[1.899995]]\n",
      "1.0\n",
      "[[1.31406571]]\n",
      "1.0\n",
      "[[1.52497625]]\n",
      "1.0\n",
      "[[2.24771239]]\n",
      "1.0\n",
      "[[2.6639832]]\n",
      "1.0\n",
      "[[1.6298315]]\n",
      "1.0\n",
      "[[0.94324717]]\n",
      "1.0\n",
      "[[2.05555278]]\n",
      "1.0\n",
      "[[0.59237963]]\n",
      "1.0\n",
      "[[2.90639532]]\n",
      "1.0\n",
      "[[2.10055503]]\n",
      "1.0\n",
      "[[0.20941048]]\n",
      "1.0\n",
      "[[1.72268614]]\n",
      "1.0\n",
      "[[1.14500726]]\n",
      "1.0\n",
      "[[2.01445073]]\n",
      "1.0\n",
      "[[2.7659883]]\n",
      "1.0\n",
      "[[1.99059953]]\n",
      "1.0\n",
      "[[1.41712086]]\n",
      "1.0\n",
      "[[0.79668984]]\n",
      "1.0\n",
      "[[2.16040802]]\n",
      "1.0\n",
      "[[0.16905846]]\n",
      "1.0\n",
      "[[1.35621782]]\n",
      "1.0\n",
      "[[0.30136508]]\n",
      "1.0\n",
      "[[1.03655183]]\n",
      "1.0\n",
      "[[0.29641483]]\n",
      "1.0\n",
      "[[1.3059653]]\n",
      "1.0\n",
      "[[0.51647583]]\n",
      "1.0\n",
      "[[0.55517777]]\n",
      "1.0\n",
      "[[2.56242812]]\n",
      "1.0\n",
      "[[1.59847993]]\n",
      "1.0\n",
      "[[1.00475024]]\n",
      "1.0\n",
      "[[2.89649483]]\n",
      "1.0\n",
      "[[0.43352168]]\n",
      "1.0\n",
      "[[1.89699485]]\n",
      "1.0\n",
      "[[2.99444972]]\n",
      "1.0\n",
      "[[1.62413121]]\n",
      "1.0\n",
      "[[2.71438572]]\n",
      "1.0\n",
      "[[0.76053803]]\n",
      "1.0\n",
      "[[1.82499125]]\n",
      "1.0\n",
      "[[0.31606581]]\n",
      "1.0\n",
      "[[1.09535477]]\n",
      "1.0\n",
      "[[2.25731287]]\n",
      "1.0\n",
      "[[2.25686285]]\n",
      "1.0\n",
      "[[0.79743988]]\n",
      "1.0\n",
      "[[2.56272814]]\n",
      "1.0\n",
      "[[1.08170409]]\n",
      "1.0\n",
      "[[1.10315516]]\n",
      "1.0\n",
      "[[1.61723087]]\n",
      "1.0\n",
      "[[0.18750938]]\n",
      "1.0\n",
      "[[2.47317366]]\n",
      "1.0\n",
      "[[1.37706886]]\n",
      "1.0\n",
      "[[1.41442073]]\n",
      "1.0\n",
      "[[2.34146708]]\n",
      "1.0\n",
      "[[0.36346818]]\n",
      "1.0\n",
      "[[2.58102905]]\n",
      "1.0\n",
      "[[0.44102206]]\n",
      "1.0\n",
      "[[1.45912296]]\n",
      "1.0\n",
      "[[1.03430172]]\n",
      "1.0\n",
      "[[2.08720436]]\n",
      "1.0\n",
      "[[1.21356068]]\n",
      "1.0\n",
      "[[0.18360919]]\n",
      "1.0\n",
      "[[1.11860594]]\n",
      "1.0\n",
      "[[0.98224912]]\n",
      "1.0\n",
      "[[1.40332017]]\n",
      "1.0\n",
      "[[2.70628532]]\n",
      "1.0\n",
      "[[1.05365269]]\n",
      "1.0\n",
      "[[0.37051853]]\n",
      "1.0\n",
      "[[2.3258663]]\n",
      "1.0\n",
      "[[0.1359068]]\n",
      "1.0\n",
      "[[0.35356769]]\n",
      "1.0\n",
      "[[1.63733187]]\n",
      "1.0\n",
      "[[2.48292415]]\n",
      "1.0\n",
      "[[2.28311416]]\n",
      "1.0\n",
      "[[0.49112456]]\n",
      "1.0\n",
      "[[1.54702736]]\n",
      "1.0\n",
      "[[0.74973749]]\n",
      "1.0\n",
      "[[2.19925997]]\n",
      "1.0\n",
      "[[2.14495725]]\n",
      "1.0\n",
      "[[1.87839392]]\n",
      "1.0\n",
      "[[2.20286015]]\n",
      "1.0\n",
      "[[1.29291465]]\n",
      "1.0\n",
      "[[0.80509026]]\n",
      "1.0\n",
      "[[1.20966049]]\n",
      "1.0\n",
      "[[2.17285865]]\n",
      "1.0\n",
      "[[2.76853843]]\n",
      "1.0\n",
      "[[2.60653033]]\n",
      "1.0\n",
      "[[2.21831092]]\n",
      "1.0\n",
      "[[2.3459673]]\n",
      "1.0\n",
      "[[0.27901396]]\n",
      "1.0\n",
      "[[1.16660834]]\n",
      "1.0\n",
      "[[2.30606531]]\n",
      "1.0\n",
      "[[1.95564779]]\n",
      "1.0\n",
      "[[0.47612381]]\n",
      "1.0\n",
      "[[0.29371469]]\n",
      "1.0\n",
      "[[2.71948598]]\n",
      "1.0\n",
      "[[0.00270014]]\n",
      "1.0\n",
      "[[2.49867494]]\n",
      "1.0\n",
      "[[0.00615032]]\n",
      "1.0\n",
      "[[0.07710386]]\n",
      "1.0\n",
      "[[2.93129657]]\n",
      "1.0\n",
      "[[1.60253013]]\n",
      "1.0\n",
      "[[0.86824342]]\n",
      "1.0\n",
      "[[0.58937948]]\n",
      "1.0\n",
      "[[0.34561729]]\n",
      "1.0\n",
      "[[1.2539127]]\n",
      "1.0\n",
      "[[2.73763688]]\n",
      "1.0\n",
      "[[1.34121707]]\n",
      "1.0\n",
      "[[0.97054853]]\n",
      "1.0\n",
      "[[0.58352918]]\n",
      "1.0\n",
      "[[1.20036002]]\n",
      "1.0\n",
      "[[2.52177609]]\n",
      "1.0\n",
      "[[1.39761989]]\n",
      "1.0\n",
      "[[1.5879794]]\n",
      "1.0\n",
      "[[2.24306216]]\n",
      "1.0\n",
      "[[2.6738837]]\n",
      "1.0\n",
      "[[0.15030752]]\n",
      "1.0\n",
      "[[2.20931047]]\n",
      "1.0\n",
      "[[2.7938897]]\n",
      "1.0\n",
      "[[1.45762289]]\n",
      "1.0\n",
      "[[1.50832542]]\n",
      "1.0\n",
      "[[2.00005001]]\n",
      "1.0\n",
      "[[1.4858243]]\n",
      "1.0\n",
      "[[2.81534077]]\n",
      "1.0\n",
      "[[0.9858493]]\n",
      "1.0\n",
      "[[2.63053153]]\n",
      "1.0\n",
      "[[1.17470874]]\n",
      "1.0\n",
      "[[2.5619781]]\n",
      "1.0\n",
      "[[2.25356268]]\n",
      "1.0\n",
      "[[2.41137057]]\n",
      "1.0\n",
      "[[2.0740037]]\n",
      "1.0\n",
      "[[2.44377219]]\n",
      "1.0\n",
      "[[0.91564579]]\n",
      "1.0\n",
      "[[2.14165709]]\n",
      "1.0\n",
      "[[1.61678084]]\n",
      "1.0\n",
      "[[2.71213561]]\n",
      "1.0\n",
      "[[0.9838992]]\n",
      "1.0\n",
      "[[2.30471524]]\n",
      "1.0\n",
      "[[1.97919896]]\n",
      "1.0\n",
      "[[0.66498326]]\n",
      "1.0\n",
      "[[0.57497876]]\n",
      "1.0\n",
      "[[0.77223862]]\n",
      "1.0\n",
      "[[1.64723237]]\n",
      "1.0\n",
      "[[1.45177259]]\n",
      "1.0\n",
      "[[2.04685235]]\n",
      "1.0\n",
      "[[1.86459323]]\n",
      "1.0\n",
      "[[1.49407471]]\n",
      "1.0\n",
      "[[1.41292065]]\n",
      "1.0\n",
      "[[0.90469524]]\n",
      "1.0\n",
      "[[0.76833842]]\n",
      "1.0\n",
      "[[1.9878994]]\n",
      "1.0\n",
      "[[2.39231962]]\n",
      "1.0\n",
      "[[2.03215161]]\n",
      "1.0\n",
      "[[0.93349668]]\n",
      "1.0\n",
      "[[1.02905146]]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.20501026]]\n",
      "1.0\n",
      "[[2.70733537]]\n",
      "1.0\n",
      "[[0.84889245]]\n",
      "1.0\n",
      "[[0.65523277]]\n",
      "1.0\n",
      "[[1.08620432]]\n",
      "1.0\n",
      "[[1.17755888]]\n",
      "1.0\n",
      "[[1.65083255]]\n",
      "1.0\n",
      "[[1.44922247]]\n",
      "1.0\n",
      "[[1.66943348]]\n",
      "1.0\n",
      "[[2.48232412]]\n",
      "1.0\n",
      "[[1.38351918]]\n",
      "1.0\n",
      "[[2.03230162]]\n",
      "1.0\n",
      "[[2.66818341]]\n",
      "1.0\n",
      "[[0.87709386]]\n",
      "1.0\n",
      "[[2.66713336]]\n",
      "1.0\n",
      "[[1.6679334]]\n",
      "1.0\n",
      "[[0.64233212]]\n",
      "1.0\n",
      "[[2.27666384]]\n",
      "1.0\n",
      "[[2.64823241]]\n",
      "1.0\n",
      "[[0.28171409]]\n",
      "1.0\n",
      "[[0.63483175]]\n",
      "1.0\n",
      "[[1.69103456]]\n",
      "1.0\n",
      "[[2.13565679]]\n",
      "1.0\n",
      "[[1.85979299]]\n",
      "1.0\n",
      "[[0.79068954]]\n",
      "1.0\n",
      "[[2.95469774]]\n",
      "1.0\n",
      "[[2.75923796]]\n",
      "1.0\n",
      "[[1.13240663]]\n",
      "1.0\n",
      "[[2.13820691]]\n",
      "1.0\n",
      "[[0.10020502]]\n",
      "1.0\n",
      "[[0.11850593]]\n",
      "1.0\n",
      "[[0.37306866]]\n",
      "1.0\n",
      "[[2.99384969]]\n",
      "1.0\n",
      "[[2.23406171]]\n",
      "1.0\n",
      "[[0.53402671]]\n",
      "1.0\n",
      "[[0.77538878]]\n",
      "1.0\n",
      "[[2.35421771]]\n",
      "1.0\n",
      "[[1.37976899]]\n",
      "1.0\n",
      "[[2.93879694]]\n",
      "1.0\n",
      "[[0.91624582]]\n",
      "1.0\n",
      "[[0.63543178]]\n",
      "1.0\n",
      "[[1.06700336]]\n",
      "1.0\n",
      "[[0.35116757]]\n",
      "1.0\n",
      "[[1.02170109]]\n",
      "1.0\n",
      "[[1.19825992]]\n",
      "1.0\n",
      "[[1.11470574]]\n",
      "1.0\n",
      "[[2.0078504]]\n",
      "1.0\n",
      "[[0.19050953]]\n",
      "1.0\n",
      "[[0.58712936]]\n",
      "1.0\n",
      "[[1.49827492]]\n",
      "1.0\n",
      "[[0.00660034]]\n",
      "1.0\n",
      "[[2.78098905]]\n",
      "1.0\n",
      "[[1.13225662]]\n",
      "1.0\n",
      "[[2.82884144]]\n",
      "1.0\n",
      "[[2.70943547]]\n",
      "1.0\n",
      "[[2.47962398]]\n",
      "1.0\n",
      "[[1.21326067]]\n",
      "1.0\n",
      "[[0.04875245]]\n",
      "1.0\n",
      "[[0.38011901]]\n",
      "1.0\n",
      "[[0.01920097]]\n",
      "1.0\n",
      "[[0.31336568]]\n",
      "1.0\n",
      "[[1.50082505]]\n",
      "1.0\n",
      "[[2.80064003]]\n",
      "1.0\n",
      "[[2.37071854]]\n",
      "1.0\n",
      "[[0.63423172]]\n",
      "1.0\n",
      "[[2.67808391]]\n",
      "1.0\n",
      "[[2.1940097]]\n",
      "1.0\n",
      "[[1.7819391]]\n",
      "1.0\n",
      "[[2.38616931]]\n",
      "1.0\n",
      "[[2.77693885]]\n",
      "1.0\n",
      "[[1.23516176]]\n",
      "1.0\n",
      "[[2.98694935]]\n",
      "1.0\n",
      "[[1.91964599]]\n",
      "1.0\n",
      "[[0.21691085]]\n",
      "1.0\n",
      "[[0.1578079]]\n",
      "1.0\n",
      "[[1.22091105]]\n",
      "1.0\n",
      "[[0.2859143]]\n",
      "1.0\n",
      "[[2.74963748]]\n",
      "1.0\n",
      "[[0.54332717]]\n",
      "1.0\n",
      "[[2.84924246]]\n",
      "1.0\n",
      "[[0.50477525]]\n",
      "1.0\n",
      "[[2.95064753]]\n",
      "1.0\n",
      "[[2.96984849]]\n",
      "1.0\n",
      "[[0.20101006]]\n",
      "1.0\n",
      "[[1.36926847]]\n",
      "1.0\n",
      "[[1.63133157]]\n",
      "1.0\n",
      "[[0.58337918]]\n",
      "1.0\n",
      "[[1.7678384]]\n",
      "1.0\n",
      "[[2.91824591]]\n",
      "1.0\n",
      "[[0.08955449]]\n",
      "1.0\n",
      "[[0.38836943]]\n",
      "1.0\n",
      "[[1.959848]]\n",
      "1.0\n",
      "[[0.0697535]]\n",
      "1.0\n",
      "[[2.379869]]\n",
      "1.0\n",
      "[[1.97874894]]\n",
      "1.0\n",
      "[[1.76048803]]\n",
      "1.0\n",
      "[[0.99304966]]\n",
      "1.0\n",
      "[[1.80144008]]\n",
      "1.0\n",
      "[[0.41972099]]\n",
      "1.0\n",
      "[[0.02760139]]\n",
      "1.0\n",
      "[[1.93869694]]\n",
      "1.0\n",
      "[[0.72543628]]\n",
      "1.0\n",
      "[[1.56367819]]\n",
      "1.0\n",
      "[[1.90479524]]\n",
      "1.0\n",
      "[[0.03420172]]\n",
      "1.0\n",
      "[[2.90489525]]\n",
      "1.0\n",
      "[[1.34301716]]\n",
      "1.0\n",
      "[[1.27371369]]\n",
      "1.0\n",
      "[[2.8420421]]\n",
      "1.0\n",
      "[[1.65323267]]\n",
      "1.0\n",
      "[[0.82744138]]\n",
      "1.0\n",
      "[[1.93674684]]\n",
      "1.0\n",
      "[[2.44332217]]\n",
      "1.0\n",
      "[[0.53462674]]\n",
      "1.0\n",
      "[[1.88739437]]\n",
      "1.0\n",
      "[[1.22511126]]\n",
      "1.0\n",
      "[[1.4919746]]\n",
      "1.0\n",
      "[[2.00080004]]\n",
      "1.0\n",
      "[[1.51687585]]\n",
      "1.0\n",
      "[[0.06705336]]\n",
      "1.0\n",
      "[[2.72683634]]\n",
      "1.0\n",
      "[[0.56012801]]\n",
      "1.0\n",
      "[[2.87984399]]\n",
      "1.0\n",
      "[[2.17120856]]\n",
      "1.0\n",
      "[[1.54537727]]\n",
      "1.0\n",
      "[[2.68348418]]\n",
      "1.0\n",
      "[[1.05920297]]\n",
      "1.0\n",
      "[[0.2538127]]\n",
      "1.0\n",
      "[[1.03940198]]\n",
      "1.0\n",
      "[[2.02840142]]\n",
      "1.0\n",
      "[[1.42567129]]\n",
      "1.0\n",
      "[[2.38151908]]\n",
      "1.0\n",
      "[[1.37346868]]\n",
      "1.0\n",
      "[[0.83419172]]\n",
      "1.0\n",
      "[[0.59372969]]\n",
      "1.0\n",
      "[[2.66983349]]\n",
      "1.0\n",
      "[[0.41567079]]\n",
      "1.0\n",
      "[[0.84934247]]\n",
      "1.0\n",
      "[[2.40837042]]\n",
      "1.0\n",
      "[[0.28696436]]\n",
      "1.0\n",
      "[[0.85924297]]\n",
      "1.0\n",
      "[[0.89554478]]\n",
      "1.0\n",
      "[[0.86884345]]\n",
      "1.0\n",
      "[[1.78178909]]\n",
      "1.0\n",
      "[[1.48612431]]\n",
      "1.0\n",
      "[[1.85244263]]\n",
      "1.0\n",
      "[[2.44497225]]\n",
      "1.0\n",
      "[[0.30841543]]\n",
      "1.0\n",
      "[[1.00220012]]\n",
      "1.0\n",
      "[[1.45012251]]\n",
      "1.0\n",
      "[[1.09520477]]\n",
      "1.0\n",
      "[[2.27036352]]\n",
      "1.0\n",
      "[[2.89304465]]\n",
      "1.0\n",
      "[[1.31031552]]\n",
      "1.0\n",
      "[[1.26141308]]\n",
      "1.0\n",
      "[[2.11810591]]\n",
      "1.0\n",
      "[[2.52507626]]\n",
      "1.0\n",
      "[[0.62313116]]\n",
      "1.0\n",
      "[[2.10505526]]\n",
      "1.0\n",
      "[[2.42412121]]\n",
      "1.0\n",
      "[[0.39256964]]\n",
      "1.0\n",
      "[[0.64338218]]\n",
      "1.0\n",
      "[[1.53157658]]\n",
      "1.0\n",
      "[[1.2039602]]\n",
      "1.0\n",
      "[[0.96604831]]\n",
      "1.0\n",
      "[[0.77598881]]\n",
      "1.0\n",
      "[[2.60623031]]\n",
      "1.0\n",
      "[[1.88514426]]\n",
      "1.0\n",
      "[[0.63513176]]\n",
      "1.0\n",
      "[[1.56097805]]\n",
      "1.0\n",
      "[[2.83574179]]\n",
      "1.0\n",
      "[[1.37256863]]\n",
      "1.0\n",
      "[[2.18275914]]\n",
      "1.0\n",
      "[[0.78933947]]\n"
     ]
    }
   ],
   "source": [
    "for x, temp in get_train_batch(test_data_by_class, test_temp_by_class, 1):\n",
    "    print(x[0][0][0][0])\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40949"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_data_by_class[0][0] == train_data_by_class[0][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data, label, mask = get_balance_train_batch_3(train_data_by_class, train_label_by_class, train_mask_by_class, 60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, label, mask = get_balance_train_batch_2(train_data_by_class, train_label_by_class, train_mask_by_class, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average magnetization matrices\n",
    "# Input x has shape: [batch_size, 32, 32, 1]\n",
    "def get_avg_mag(x):\n",
    "    batch_size = x.get_shape()[0]\n",
    "    # Create average magnetization matrix\n",
    "    avg_mag = tf.math.reduce_mean(x, axis=[1, 2, 3]) # Take mean of each graph\n",
    "    assert avg_mag.get_shape().as_list() == [batch_size]\n",
    "    concat_array = []\n",
    "    for i in range(batch_size):\n",
    "        fill_tensor = tf.fill(dims=[32, 32, 1], value=avg_mag[i]) # Fill 32*32*1 tensorf with avg mag value\n",
    "        fil_tensor_expand = tf.expand_dims(fill_tensor, axis=0) # Expand tensor dimension for batch size\n",
    "        concat_array.append(fil_tensor_expand)\n",
    "    return_tensor = tf.concat(concat_array, axis=0) \n",
    "    assert return_tensor.get_shape() == [bath_size, 32, 32, 1]\n",
    "    return return_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_norm(w, iteration=1):\n",
    "    w_shape = w.shape.as_list()\n",
    "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "\n",
    "    #u = tf.Variable(\"u\", [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n",
    "    u = tf.Variable(tf.random.normal(shape=[1, w_shape[-1]]), trainable=False)\n",
    "    u_hat = u\n",
    "    v_hat = None\n",
    "    for i in range(iteration):\n",
    "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
    "        v_hat = tf.nn.l2_normalize(v_)\n",
    "\n",
    "        u_ = tf.matmul(v_hat, w)\n",
    "        u_hat = tf.nn.l2_normalize(u_)\n",
    "\n",
    "    u_hat = tf.stop_gradient(u_hat)\n",
    "    v_hat = tf.stop_gradient(v_hat)\n",
    "\n",
    "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
    "\n",
    "    with tf.control_dependencies([u.assign(u_hat)]):\n",
    "        w_norm = w / sigma\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "\n",
    "    return w_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The discriminator/critic\n",
    "# Note: temperatures tensor has shape [batch_size]: [temp0, temp1, temp2,...]\n",
    "def D(x, temperatures, dropout_rate, is_training, reuse = True, print_summary = True):\n",
    "#def D(x, temperature, dropout_rate, is_training, reuse = True, print_summary = True):\n",
    "    # discriminator (x -> n + 1 class)\n",
    "\n",
    "    with tf.compat.v1.variable_scope('Discriminator', reuse = reuse) as scope:\n",
    "        batch_size = x.get_shape().as_list()[0] # Get the batch size\n",
    "        \n",
    "        # Weight matrix for fully connected layer--alternative version\n",
    "        #W_fc_input = tf.Variable(tf.truncated_normal([1, 32*32*1], stddev=0.1), name='W_fc_input')\n",
    "        #b_fc_input = tf.Variable(tf.constant(0.1, shape=[32*32*1]), name='b_fc_input')\n",
    "        #temp_fc = tf.nn.xw_plus_b(temperatures, W_fc_input, b_fc_input, name='temp_fc')\n",
    "        #temp_reshaped = tf.reshape(temp_fc, [-1, 32, 32, 1])\n",
    "        \n",
    "        # Concatenate the avg_mag tensor with the input\n",
    "        #concat = tf.concat([x, temp_reshaped], axis=3)\n",
    "        #assert concat.shape.as_list()[1:] == [32, 32, 2]\n",
    "        \n",
    "        # layer1 - do not use Batch Normalization on the first layer of Discriminator\n",
    "        # The W filter has shape: [height, width, input_channels, output_channels]\n",
    "        w_conv1 = tf.Variable(tf.random.truncated_normal([3, 3, num_channels, 32], stddev=0.1), name='w_conv1')\n",
    "        #conv1 = tf.layers.conv2d(concat, 32, [3, 3],\n",
    "                                 #strides = [2, 2],\n",
    "                                 #padding = 'same')\n",
    "        conv1 = tf.nn.conv2d(x, spectral_norm(w_conv1), strides=[2,2], padding='SAME', name='conv1')\n",
    "        b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]), name='b_conv1') # Bias\n",
    "        #lrelu1 = tf.maximum(0.2 * conv1, conv1) #leaky relu\n",
    "        lrelu1 = tf.nn.leaky_relu(tf.nn.bias_add(conv1, b_conv1), alpha=0.2, name='lrelu1')\n",
    "        \n",
    "        dropout1 = tf.nn.dropout(lrelu1, dropout_rate)\n",
    "\n",
    "        # layer2\n",
    "#         conv2 = tf.layers.conv2d(dropout1, 64, [3, 3],\n",
    "#                                  strides = [2, 2],\n",
    "#                                  padding = 'same')\n",
    "#         batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training, momentum=0.8)\n",
    "#         lrelu2 = tf.maximum(0.2 * batch_norm2, batch_norm2)\n",
    "#         dropout2 = tf.layers.dropout(lrelu2, dropout_rate)\n",
    "        w_conv2 = tf.Variable(tf.random.truncated_normal([3, 3, 32, 64], stddev=0.1), name='w_conv2')\n",
    "        conv2 = tf.nn.conv2d(dropout1, spectral_norm(w_conv2), strides=[2,2], padding='SAME', name='conv2')\n",
    "        b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]), name='b_conv2') # Bias\n",
    "        lrelu2 = tf.nn.leaky_relu(tf.nn.bias_add(conv2, b_conv2), alpha=0.2, name='lrelu2')\n",
    "        dropout2 = tf.nn.dropout(lrelu2, dropout_rate)\n",
    "\n",
    "        # layer3\n",
    "#         conv3 = tf.layers.conv2d(lrelu2, 64, [3, 3],\n",
    "#                                  strides = [1, 1],\n",
    "#                                  padding = 'same')\n",
    "#         batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
    "#         lrelu3 = tf.maximum(0.2 * batch_norm3, batch_norm3)\n",
    "#         dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
    "        w_conv3 = tf.Variable(tf.random.truncated_normal([3, 3, 64, 64], stddev=0.1), name='w_conv3')\n",
    "        conv3 = tf.nn.conv2d(dropout2, spectral_norm(w_conv3), strides=[1,1], padding='SAME', name='conv3')\n",
    "        b_conv3 = tf.Variable(tf.constant(0.1, shape=[64]), name='b_conv3') # Bias\n",
    "        lrelu3 = tf.nn.leaky_relu(tf.nn.bias_add(conv3, b_conv3), alpha=0.2, name='lrelu3')\n",
    "        dropout3 = tf.nn.dropout(lrelu3, dropout_rate)\n",
    "        #dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
    "        #dropout3 = tf.layers.dropout(lrelu2, dropout_rate)\n",
    "        # layer 4\n",
    "        #conv4 = tf.layers.conv2d(dropout3, 256, [3, 3],\n",
    "                                #strides = [1, 1],\n",
    "                                #padding = 'same')\n",
    "        # do not use batch_normalization on this layer - next layer, \"flatten5\",\n",
    "        # will be used for \"Feature Matching\"\n",
    "        #lrelu4 = tf.maximum(0.2 * conv4, conv4)\n",
    "\n",
    "        # layer 5\n",
    "        flatten_length = dropout3.get_shape().as_list()[1] * \\\n",
    "                         dropout3.get_shape().as_list()[2] * dropout3.get_shape().as_list()[3]\n",
    "        flatten5 = tf.reshape(dropout3, (-1, flatten_length)) # used for \"Feature Matching\" \n",
    "        \n",
    "        w_flatten5 = tf.Variable(tf.random.truncated_normal([flatten_length, 2], stddev=0.1), name='w_flatten5')\n",
    "        b_flatten5 = tf.Variable(tf.constant(0.1, shape=[2]), name='b_flatten5')\n",
    "        fc5 = tf.math.add(tf.linalg.matmul(flatten5, w_flatten5), b_flatten5, name='fc5')\n",
    "        #fc5 = tf.nn.xw_plus_b(flatten, w_flatten5, b_flatten5, name='fc5')\n",
    "        #fc5 = tf.layers.dense(flatten5, (1)) # WGAN only needs one output node for the critic score\n",
    "        #output = tf.nn.softmax(fc5, name=\"D_output\")\n",
    "        assert fc5.get_shape()[1:] == [2]\n",
    "        output_classification_prob = fc5[:,0:1] # A probability score (0 to 1) enforced lip-1 by spect-norm\n",
    "        output_regression_label = tf.nn.leaky_relu(fc5[:,1:2], alpha=0.2) # Continuous label regression, leaky relu activation\n",
    "        print('output_classification_prob.get_shape() is')\n",
    "        print(output_classification_prob.get_shape())\n",
    "        assert output_classification_prob.get_shape()[1:] == [1]\n",
    "        assert output_regression_label.get_shape()[1:] == [1]\n",
    "        \n",
    "        if print_summary:\n",
    "            print('Discriminator summary:\\n x: %s\\n' \\\n",
    "                  ' D1: %s\\n D2: %s\\n D3: %s\\n D4: %s\\n' %(x.get_shape(), \n",
    "                                                           dropout1.get_shape(),\n",
    "                                                           lrelu2.get_shape(), \n",
    "                                                           dropout3.get_shape(),\n",
    "                                                           lrelu4.get_shape()))\n",
    "        #return flatten5, fc5, output\n",
    "        #debug: return each layer's output --BZ Nov 28, 2019\n",
    "        return flatten5, fc5, output_classification_prob, output_regression_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testA = np.zeros([5,2])\n",
    "testA[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def G(z, is_training, reuse = False, print_summary = False):\n",
    "# Input: temperatures is a list of temperatures with size batch_size, where the temperature values match that\n",
    "# from the real data in the batch (in value and in order)\n",
    "# Note: temperatures tensor has shape [batch_size]: [temp0, temp1, temp2]\n",
    "def G(z, temperatures, is_training, reuse = False, print_summary = False): # Added: temperature\n",
    "    # generator (z -> x)\n",
    "\n",
    "    with tf.compat.v1.variable_scope('Generator', reuse = reuse) as scope:\n",
    "        batch_size = z.get_shape().as_list()[0] # Get the batch size\n",
    "        # Create temperature \"embedding\":\n",
    "        #temp_50 = tf.repeat(temperatures, repeats=[50], axis=1)\n",
    "        temp_fc = tf.compat.v1.layers.dense(temperatures, 8*8*1)\n",
    "        temp_reshaped = tf.compat.v1.reshape(temp_fc, [-1, 8, 8, 1])\n",
    "        \n",
    "        \n",
    "        #z has shape [batch_size, 100]\n",
    "        fc1 = tf.compat.v1.layers.dense(z, 8*8*128)\n",
    "        # layer 0\n",
    "        z_reshaped = tf.compat.v1.reshape(fc1, [-1, 8, 8, 128])\n",
    "\n",
    "        \n",
    "        #Concatenate the noise matrix and the temperature matrix as per CGAN specs\n",
    "        concat = tf.compat.v1.concat([z_reshaped, temp_reshaped], axis=3)\n",
    "        assert concat.shape.as_list()[1:] == [8, 8, 129]\n",
    "        \n",
    "        # layer 1\n",
    "        deconv1 = tf.compat.v1.layers.conv2d_transpose(concat,\n",
    "                                             filters = 129,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [2, 2],\n",
    "                                             padding = 'same')\n",
    "        batch_norm1 = tf.compat.v1.layers.batch_normalization(deconv1, training = is_training, momentum=0.8)\n",
    "        relu1 = tf.compat.v1.nn.relu(batch_norm1)\n",
    "        #64*64*64\n",
    "        # layer 2\n",
    "        deconv2 = tf.compat.v1.layers.conv2d_transpose(relu1,\n",
    "                                             filters = 64,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [2, 2],\n",
    "                                             padding = 'same')\n",
    "        batch_norm2 = tf.compat.v1.layers.batch_normalization(deconv2, training = is_training, momentum=0.8)\n",
    "        relu2 = tf.compat.v1.nn.relu(batch_norm2)\n",
    "        #128*128*3\n",
    "        # layer 3\n",
    "        #deconv3 = tf.layers.conv2d_transpose(relu2,\n",
    "                                             #filters = 64,\n",
    "                                             #kernel_size = [3, 3],\n",
    "                                             #strides = [1, 1],\n",
    "                                             #padding = 'same')\n",
    "        #batch_norm3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
    "        #relu3 = tf.nn.relu(batch_norm3)\n",
    "\n",
    "        # layer 4 - do not use Batch Normalization on the last layer of Generator\n",
    "        deconv4 = tf.compat.v1.layers.conv2d_transpose(relu2,\n",
    "                                             filters = num_channels,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [1, 1],\n",
    "                                             padding = 'same')\n",
    "        tanh4 = tf.compat.v1.tanh(deconv4, name=\"G_output\")\n",
    "\n",
    "        assert tanh4.get_shape()[1:] == [x_height, x_width, num_channels]\n",
    "        if print_summary:\n",
    "            print('Generator summary:\\n z: %s\\n' \\\n",
    "                  ' G0: %s\\n G1: %s\\n G2: %s\\n G3: %s\\n G4: %s\\n' %(z.get_shape(),\n",
    "                                                                    z_reshaped.get_shape(),\n",
    "                                                                    relu1.get_shape(),\n",
    "                                                                    relu2.get_shape(),\n",
    "                                                                    relu3.get_shape(),\n",
    "                                                                    tanh4.get_shape()))\n",
    "        return tanh4 # The fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build model for each batch using D() and G() functions\n",
    "#def build_model(x_real, z, label, dropout_rate, is_training, print_summary = False):\n",
    "def build_model(x_real, z, temperatures, dropout_rate, is_training, print_summary = False):\n",
    "    # build model\n",
    "    #Discriminator on real data (labeled and unlabeled)  flatten5, fc5, output\n",
    "    # Due to one output node, the logit is kinda useless\n",
    "    D_real_features, D_real_logit, D_real_prob, D_real_regression = D(x_real, temperatures, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    #generate fake images\n",
    "    x_fake = G(z, temperatures, is_training, reuse = False, print_summary = print_summary)\n",
    "    #Discriminator for fake images\n",
    "    D_fake_features, D_fake_logit, D_fake_prob, D_fake_regression= D(x_fake, temperatures, dropout_rate, is_training,\n",
    "                                                   reuse = True, print_summary = print_summary)\n",
    "\n",
    "    return D_real_features, D_real_logit, D_real_prob, D_real_regression, D_fake_features, D_fake_logit, D_fake_prob, D_fake_regression, x_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
    "                  #D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
    "# Note: real_imgs and fake_imgs have the same temperature/goal temperature\n",
    "# Note: input \"label\" is only for real classes: 0 for real, 1 for fake\n",
    "def loss_accuracy(real_imgs, fake_imgs, D_real_features, D_real_logit, D_real_prob, D_real_regression, D_fake_features,\n",
    "                  D_fake_logit, D_fake_prob, D_fake_regression, real_labels):\n",
    "    epsilon = 1e-8 # used to avoid NAN loss\n",
    "    prob_real_be_real = D_real_prob + epsilon\n",
    "    prob_fake_be_real = D_fake_prob + epsilon \n",
    "    # *** Discriminator loss ***\n",
    "    # For SQA-GAN, the discriminiator has L_regression and L_hinge, where L_regression is for \n",
    "    # laebeld data (continuous label)--which is supervised\n",
    "    # and L_hinge is unsupervised loss\n",
    "    print('real_imgs')\n",
    "    batch_size = tf.shape(real_imgs)[0]\n",
    "    print('batch_size is')\n",
    "    print(batch_size)\n",
    "    # L_regression_error: Use mean here because batch sizes are not guaranteed to agree.\n",
    "    D_L_regression_error = tf.reduce_mean(tf.square(D_real_regression - real_labels), axis=0)\n",
    "    # First part of L_hinge\n",
    "    D_L_hinge_real_be_real = -1.0*tf.reduce_mean(tf.math.minimum(tf.constant(0.0, shape=[1]), \\\n",
    "                    tf.constant(-1.0, shape=[1])+prob_real_be_real), axis=0)\n",
    "    # Second part of L_hinge\n",
    "    D_L_hinge_fake_be_fake = -1.0*tf.reduce_mean(tf.math.minimum(tf.constant(0.0, shape=[1]), \\\n",
    "                    tf.constant(-1.0, shape=[1])-1*prob_fake_be_real), axis=0)\n",
    "    D_L_hinge = D_L_hinge_real_be_real + D_L_hinge_fake_be_fake\n",
    "    \n",
    "    D_L = D_L_hinge + 100*D_L_regression_error\n",
    "    #prob_real_be_real = D_real_prob + epsilon\n",
    "    #tmp_log = tf.log(prob_real_be_real)\n",
    "    #D_L_unsupervised1 = -1 * tf.reduce_mean(tmp_log)\n",
    "    #D_L_unsupervised1 = -1 * tf.reduce_mean(prob_real_be_real, axis=0)\n",
    "\n",
    "    # data is fake\n",
    "    #prob_fake_be_real = D_fake_prob + epsilon # The \"real class\" probability score on fake data\n",
    "    #tmp_log = tf.log(prob_fake_be_fake)\n",
    "    #D_L_unsupervised2 = -1 * tf.reduce_mean(tmp_log)\n",
    "    #D_L_unsupervised2 = tf.reduce_mean(prob_fake_be_real, axis=0)\n",
    "    \n",
    "    #D_L = D_L_unsupervised2 + D_L_unsupervised1\n",
    "\n",
    "    # *** Generator loss ***\n",
    "    # Four parts to generator loss: 1. Fool the discriminator, 2. Fool the temperature regressor, 3. Feature mapping\n",
    "    # 4. Avg_mag.\n",
    "    # fake data is mistaken to be real (Fool the discriminator)\n",
    "    #tmp_log =  tf.log(prob_fake_be_real)\n",
    "    G_L1 = -1 * tf.reduce_mean(prob_fake_be_real, axis=0)\n",
    "\n",
    "    # Fool the discriminator regrossor\n",
    "    G_L2 = tf.reduce_mean(tf.square(real_labels - D_fake_regression), axis=0)\n",
    "    \n",
    "    # Feature Maching\n",
    "    tmp1 = tf.reduce_mean(D_real_features, axis = 0)\n",
    "    tmp2 = tf.reduce_mean(D_fake_features, axis = 0)\n",
    "    G_L3 = tf.reduce_mean(tf.square(tmp1 - tmp2)) # Mean or sum, both are ok I think\n",
    "    \n",
    "    # Avg_mag\n",
    "    avg_mag_real = tf.math.reduce_mean(real_imgs, axis=[1, 2, 3]) # Take mean of each graph\n",
    "    avg_mag_fake = tf.math.reduce_mean(fake_imgs, axis=[1, 2, 3])\n",
    "    G_L4 = tf.reduce_mean(tf.square(avg_mag_real-avg_mag_fake), axis=0)\n",
    "    \n",
    "\n",
    "    G_L = G_L1 + G_L2 + G_L3 + G_L4\n",
    "\n",
    "    # Since the goals isn't classification, the discriminator accuracy is just to see how good the cirtic is\n",
    "    # Only get accuracy among real data.\n",
    "    # accuracy--This is cross validation accuracy within the training set--Nov, 2019\n",
    "    #correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  #tf.argmax(extended_label[:, :-1], 1), name='correct_prediction')\n",
    "    \n",
    "    # D_real_prob has shape [batch_size, 1] need to reduce the second dimension\n",
    "    D_real_prob_reduced_dimension = tf.squeeze(D_real_prob, axis=1)\n",
    "    correct_prediction = tf.equal(D_real_prob_reduced_dimension >= 0.5, True) # Whether the data is predicted as \"real\"\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return D_L, D_L_hinge_real_be_real, D_L_hinge_fake_be_fake, D_L_regression_error, G_L, G_L1, G_L2, G_L3, G_L4, correct_prediction, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimizer(D_Loss, G_Loss, D_learning_rate, G_learning_rate):\n",
    "    # D and G optimizer\n",
    "    extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS) # Batch_norm ops\n",
    "    with tf.compat.v1.control_dependencies(extra_update_ops): # Make sure batch_norms' running mean and var are updated\n",
    "        all_vars = tf.compat.v1.trainable_variables()\n",
    "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
    "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
    "        #print('D_vars:')\n",
    "        #print(D_vars)\n",
    "        D_optimizer = tf.compat.v1.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
    "        G_optimizer = tf.compat.v1.train.AdamOptimizer(G_learning_rate).minimize(G_Loss, var_list = G_vars)\n",
    "        return D_optimizer, G_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assume len(data) > 5 --BZ Nov 30, 2019\n",
    "\n",
    "# def plot_fake_data(data, epoch):\n",
    "#     # visualize some data generated by G\n",
    "#     data = (1/2.5) * data + 0.5\n",
    "#     fig, axs = plt.subplots(len(data), figsize=(30,30))\n",
    "#     cnt = 0\n",
    "#     for j in range(len(data)):\n",
    "#         #print(j)\n",
    "#         #print(data[cnt, :, :, :])\n",
    "#         axs[j].imshow(data[cnt, :, :, :])\n",
    "#         axs[j].axis(\"off\")\n",
    "#         cnt = cnt + 1\n",
    "#     print('graphed!')        \n",
    "#     if not os.path.exists(\"./training_fake_figure\"):\n",
    "#         os.mkdir(\"./training_fake_figure\");\n",
    "#     plt.savefig(\"training_fake_figure/%d.jpg\" % epoch)\n",
    "#     plt.close()\n",
    "\n",
    "def save_fake_image(data, epoch):\n",
    "    #if not os.path.exists(\"./training_fake_imageMatrix\"):\n",
    "        #os.mkdir(\"./training_fake_imageMatrix\");\n",
    "    #np.save(\"./training_fake_imageMatrix/epoch_\"+str(epoch), data)\n",
    "    #only plot the last image of the batch\n",
    "    if not os.path.exists(\"./training_fake_figure\"):\n",
    "        os.mkdir(\"./training_fake_figure\");\n",
    "    # Select the last fake image in the batch and reduce its shape\n",
    "    # to 32, 32\n",
    "    data_reduce = np.squeeze(data[-1], axis=2)\n",
    "    plt.imshow(data_reduce)\n",
    "    plt.set_cmap('hot')\n",
    "    plt.axis('off')\n",
    "    #print(data[-1])\n",
    "    plt.savefig(\"training_fake_figure/%d.jpg\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def save_model_on_improvement(file_path, sess, cv_acc, cv_accs):\n",
    "#   #  # save model when there is improvemnet in cv_acc value\n",
    "#     if cv_accs == [] or cv_acc >= np.max(cv_accs):\n",
    "#         saver = tf.train.Saver(max_to_keep = 1)\n",
    "#         saver.save(sess, file_path)\n",
    "#         print('Model saved')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the top 5 best model based on validation accuracy\n",
    "def save_model_top_five(folder_path, sess, cv_acc, cv_accs):\n",
    "    #cv_acc is inside cv_accs\n",
    "    if not os.path.exists(folder_path+'/'+task_name):\n",
    "        os.mkdir(folder_path+'/'+task_name)\n",
    "    sortedAccs = np.sort(cv_accs)\n",
    "    for i in range(len(cv_accs)):\n",
    "        if i >= 5:\n",
    "            return\n",
    "        if cv_acc >= sortedAccs[i]:\n",
    "            saver = tf.compat.v1.train.Saver(max_to_keep = 1)\n",
    "            saver.save(sess, folder_path+'/'+task_name+'/'+'_top_'+str(i+1)+'_ISING_GAN.ckpt')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model based on TPR and TNR criteria\n",
    "# def save_model_TPR_TNR(folder_path, sess, epoch, cv_acc, cv_accs, TPR, TNR, TPRs):\n",
    "#     if not os.path.exists(folder_path+'/'+task_name):\n",
    "#         os.mkdir(folder_path+'/'+task_name)\n",
    "#     sortedTPRs = np.sort(TPRs)[::-1] # Sort in Descending Order!\n",
    "#     for i in range(len(sortedTPRs)):\n",
    "#         if i >= 5:\n",
    "#             return\n",
    "#         #if TPR >= sortedTPRs[i] and TPR >= 0.88 and TNR >= 0.91:\n",
    "#         if TPR >= sortedTPRs[i]:\n",
    "#             print('save model')\n",
    "#             saver = tf.train.Saver(max_to_keep = 1)\n",
    "#             saver.save(sess, folder_path+'/'+task_name+'/'+'_TPR_top_'+str(i+1)+'_epoch_'+str(epoch)+'_SSL_GAN.ckpt')\n",
    "#             return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(file_path, sess):\n",
    "#     saver = tf.train.Saver(max_to_keep = 1)\n",
    "#     saver.save(sess, file_path)\n",
    "#     print('Every 500 model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_loss_acc_binary(file_path, epoch, L_D_hinge1, L_D_hinge2, L_D_reg, L_G_reg, L_G_heu, L_G_fea, L_G_avgmag, log_mode = 'a'):\n",
    "    # log train and cv losses as well as accuracy\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, L_D_hinge1, L_D_hinge2, L_D_reg, L_G_reg, L_G_heu, L_G_fea, L_G_avgmag,' \\\n",
    "                     'train_Acc, val_Acc \\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f, %f, %f, %f, %f, %f\\n' %(epoch, L_D_hinge1, L_D_hinge2, L_D_reg, \\\n",
    "                                                            L_G_reg, L_G_heu, L_G_fea, L_G_avgmag)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Modified by BZ on Nov 3, 2019\n",
    "#Note: predictions vector has binary entries: 1 corresponds to correct prediction, 0 is wrong prediction\n",
    "def compute_val_accuracy(correct_predictions):\n",
    "    return np.sum(correct_predictions)/len(correct_predictions) # or tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training function--Bill Zhai Aug 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: batch_size is the number of REAL data supplied to each iteration\n",
    "def train_SSL_GAN(batch_size, num_epochs, train_data_by_class, train_temp_by_class, test_data_by_class, test_temp_by_class):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # train Semi-Supervised Learning GAN\n",
    "    train_D_losses, train_G_losses, train_Accs = [], [], []\n",
    "    val_D_losses, val_G_losses, val_Accs, TPRs = [], [], [], []\n",
    "    \n",
    "    cv_size = batch_size\n",
    "    num_train_exs = numTrain\n",
    "    num_val_exs = numTest\n",
    "    print(batch_size)\n",
    "    print(\"num_train_exs: \", num_train_exs)\n",
    "    print(\"num_val_exs: \", num_val_exs)\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    x = tf.compat.v1.placeholder(tf.float32, name = 'x', shape = [None, x_height, x_width, num_channels])\n",
    "    temperature = tf.compat.v1.placeholder(tf.float32, name='temperature', shape=[None, 1])\n",
    "    #label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes]) # one hot label--BZ, August, 2019\n",
    "    #labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
    "    z = tf.compat.v1.placeholder(tf.float32, name = 'z', shape = [None, latent_size])#one 1-d noise vector per training example\n",
    "    dropout_rate = tf.compat.v1.placeholder(tf.float32, name = 'dropout_rate')\n",
    "    is_training = tf.compat.v1.placeholder(tf.bool, name = 'is_training')\n",
    "    G_learning_rate = tf.compat.v1.placeholder(tf.float32, name = 'G_learning_rate')\n",
    "    D_learning_rate = tf.compat.v1.placeholder(tf.float32, name = 'D_learning_rate')\n",
    "\n",
    "    model = build_model(x, z, temperature, dropout_rate, is_training, print_summary = False)\n",
    "    D_real_features, D_real_logit, D_real_prob, D_real_regression, \\\n",
    "    D_fake_features, D_fake_logit, D_fake_prob, D_fake_regression, fake_data = model\n",
    "    #extended_label = prepare_labels(label) #is only for real image data\n",
    "        \n",
    "    loss_acc  = loss_accuracy(x, fake_data, D_real_features, D_real_logit, D_real_prob, D_real_regression,\n",
    "                              D_fake_features, D_fake_logit, D_fake_prob, D_fake_regression, temperature)\n",
    "\n",
    "    D_L, D_L_hinge_1, D_L_hinge_2, D_L_regression_error, G_L, G_L1, G_L2, G_L3, G_L4, correct_prediction, accuracy = loss_acc\n",
    "    D_optimizer, G_optimizer = optimizer(D_L, G_L, G_learning_rate, D_learning_rate)\n",
    "\n",
    "    \n",
    "#     validation_generator = get_batch(data_path, label_path, num_val_exs, num_train_exs, True)\n",
    "\n",
    "    print('training....')\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:       \n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        #t_total = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            #for iteration in range(int(numTrain/(batch_size*(num_classes+c_ul)/(num_classes+1+c_ul)))):\n",
    "            #batch_num = 0 #added--BZ Nov 2, 2019\n",
    "            #training_generator = get_training_batch_and_labeled_mask(X_train, y_train, LABELED_MASK, batch_size);\n",
    "            train_batch_generator = get_train_batch(train_data_by_class,\n",
    "                                                                  train_temp_by_class,\n",
    "                                                                  batch_size)\n",
    "            for train_batch_x, train_batch_temp in train_batch_generator:\n",
    "                #train_batch_x, train_batch_y, train_batch_mask = get_balance_train_batch_3(train_data_by_class, train_label_by_class, train_mask_by_class, batch_size, c_ul)            #for train_batch_x, train_batch_y, train_batch_mask in training_generator:\n",
    "                #t_start = time.time()\n",
    "                #batch_z = np.random.uniform(-1.0, 1.0, size = (batch_size, latent_size)) #\n",
    "                #batch_z = np.random.normal(0, 1, size = (int(batch_size/num_classes), latent_size))\n",
    "                batch_z = np.random.normal(0, 1, size = [len(train_batch_temp), latent_size])\n",
    "                #function is to be modified as labeled mask should stay with \"labeled\" images which are shuffled BZ--August, 2019\n",
    "                #mask = get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize);--marked as solved BZ, Nov, 2019\n",
    "                train_feed_dictionary = {x: train_batch_x,\n",
    "                                         z: batch_z,\n",
    "                                         temperature: train_batch_temp,\n",
    "                                         #labeled_mask: train_batch_mask,\n",
    "                                         dropout_rate: 0.25,\n",
    "                                         G_learning_rate: 2e-4,\n",
    "                                         D_learning_rate: 2e-4,\n",
    "                                         is_training: True} # BatchNorm training mode\n",
    "                \n",
    "                fetch_ops =sess.run(fetches=[D_optimizer, G_optimizer, D_L, G_L, accuracy, \n",
    "                                             D_L_hinge_1, D_L_hinge_2, D_L_regression_error, G_L1, G_L2, G_L3, G_L4], \n",
    "                                    feed_dict=train_feed_dictionary)\n",
    "                _, _, train_D_loss, train_G_loss, train_accuracy, \\\n",
    "                train_D_hinge_1, train_D_hinge_2, train_D_regression_error, train_G_L1, train_G_L2, train_G_L3, train_G_L4 = fetch_ops\n",
    "                print('train_D_hinge_1, train_D_hinge_2, train_D_regression_error')\n",
    "                print(train_D_hinge_1)\n",
    "                print(train_D_hinge_2)\n",
    "                print(train_D_regression_error)\n",
    "                print('\\n')\n",
    "                print(\"train_G_L1, train_G_L2, train_G_L3, train_G_l4\")\n",
    "                print(train_G_L1)\n",
    "                print(train_G_L2)\n",
    "                print(train_G_L3)\n",
    "                print(train_G_L4)\n",
    "                print('==================')\n",
    "                #t_total += (time.time() - t_start)\n",
    "\n",
    "                #debug:\n",
    "                #trainDLogits = D_real_logit.eval(feed_dict = train_feed_dictionary)\n",
    "                #trainRealFeatures = D_real_features.eval(feed_dict = train_feed_dictionary)\n",
    "            \n",
    "                \n",
    "                \n",
    "                #print('Epoch: '+str(epoch)+\" Iter: \"+str(iteration)+\" Time: \"+str(t_total)+\" train_G_loss: \"\n",
    "                    #  +str(train_G_loss)+\" train_D_loss: \"+str(train_D_loss)+\" train_accuracy: \"+str(train_accuracy))\n",
    "\n",
    "                #print('trainDLogits:')\n",
    "                #print(trainDLogits)\n",
    "                #print('trainFeatures:')\n",
    "                #print(trainRealFeatures)\n",
    "                #print('xinput')\n",
    "                #print(xinput.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout1')\n",
    "                #print(dropout1.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('batch_norm2')\n",
    "                #print(batch_norm2.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout3')\n",
    "                #print(dropout3.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout4')\n",
    "                #print(dropout4.eval(feed_dict = train_feed_dictionary))\n",
    "        \n",
    "                train_D_losses.append(train_D_loss)\n",
    "                train_G_losses.append(train_G_loss)\n",
    "                train_Accs.append(train_accuracy)\n",
    "                #batch_num = batch_num + 1;\n",
    "            # Validation at the end of each epoch--BZ, Nov, 2019\n",
    "\n",
    "            \n",
    "            test_generator= get_test_batch(test_data_by_class, test_temp_by_class, batch_size)\n",
    "            val_correct_preds = []\n",
    "            val_real_probs = np.zeros((0,1))\n",
    "            #labels = np.zeros((0, num_classes))\n",
    "            for test_batch_x, test_batch_temp in test_generator:\n",
    "            #test_batch_generator = get_test_batch(X_test, y_test, batch_size);#added--BZ, Nov 2, 2019\n",
    "            #for test_batch_x, test_batch_y in test_batch_generator:\n",
    "                val_batch_z = np.random.normal(0, 1, size = [len(test_batch_temp), latent_size])\n",
    "                #mask = np.ones(len(test_batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "                val_feed_dictionary = {x: test_batch_x,\n",
    "                                       z: val_batch_z,\n",
    "                                       #label: test_batch_y,\n",
    "                                       #labeled_mask: mask,\n",
    "                                       temperature: test_batch_temp,\n",
    "                                       dropout_rate: 0.0,\n",
    "                                       is_training: False} # BatchNorm training mode\n",
    "\n",
    "\n",
    "                #val_D_loss = D_L.eval(feed_dict = val_feed_dictionary)\n",
    "                #val_G_loss = G_L.eval(feed_dict = val_feed_dictionary)\n",
    "\n",
    "                #val_correct_pred = correct_prediction.eval(feed_dict = val_feed_dictionary)\n",
    "                \n",
    "                    \n",
    "                #val_D_real_prob = D_real_prob.eval(feed_dict = val_feed_dictionary)\n",
    "                \n",
    "                fetch_ops = sess.run(fetches=[D_L_hinge_1, D_L_hinge_2, D_L_regression_error, G_L1, G_L2, G_L3, G_L4],\n",
    "                                    feed_dict=val_feed_dictionary)\n",
    "                #val_D_loss, val_G_loss, val_correct_pred, val_D_real_prob = fetch_ops\n",
    "                L_D_hinge1, L_D_hinge2, L_D_reg,\\\n",
    "                L_G_heu, L_G_reg, L_G_fea, L_G_avgmag = fetch_ops\n",
    "                \n",
    "                #val_correct_preds = np.concatenate((val_correct_preds, val_correct_pred))\n",
    "                #val_real_probs = np.concatenate((val_real_probs, val_D_real_prob))\n",
    "                #val_fake_probs = np.concatenate((val_fake_probs, val_fake_prob))\n",
    "                #predictions = np.concatenate((predictions, np.argmax(val_D_real_prob[:, :-1], axis = 1)))\n",
    "                #labels = np.concatenate((labels, test_batch_y))\n",
    "            \n",
    "            #val_accuracy = compute_val_accuracy(val_correct_preds)\n",
    "            #val_Accs.append(val_accuracy)\n",
    "    \n",
    "            #CM = confusion_matrix(np.argmax(labels, axis = 1), predictions)\n",
    "            #TPR = CM[1][1]/(CM[1][0]+CM[1][1])\n",
    "            #TNR = CM[0][0]/(CM[0][0]+CM[0][1])\n",
    "            #TPRs.append(TPR)\n",
    "            #print(val_correct_preds);\n",
    "            #print('validation_acc: %f' %(val_accuracy))\n",
    "            log_loss_acc_binary(log_path, epoch, L_D_hinge1, L_D_hinge2, L_D_reg, L_G_reg, L_G_heu, L_G_fea, L_G_avgmag, 'w')\n",
    "    \n",
    "            #save_model_top_five(model_path, sess, val_accuracy, val_Accs)\n",
    "            \n",
    "            #fakes = fake_data.eval(feed_dict = val_feed_dictionary)\n",
    "            #save_fake_image(fakes, epoch)\n",
    "            #print(fakes)\n",
    "            #confusion matrix\n",
    "            #print('epoch'+str(epoch)+' CM:')\n",
    "            #print(confusion_matrix(np.argmax(labels, axis = 1), predictions, normalize = 'true'))\n",
    "            #print('val_real_probs: ')\n",
    "            #print(val_real_probs)\n",
    "            #print('val_fake_probs: ')\n",
    "            #print(val_fake_probs)\n",
    "            #print('epoch '+str(epoch)+' done')  \n",
    "            \n",
    "        \n",
    "    return train_D_losses, train_G_losses, train_Accs, val_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "num_train_exs:  18000\n",
      "num_val_exs:  2000\n",
      "WARNING:tensorflow:From /Users/pengyuanzhai/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "output_classification_prob.get_shape() is\n",
      "(None, 1)\n",
      "WARNING:tensorflow:From <ipython-input-19-d2ec7c241fb7>:12: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/pengyuanzhai/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-19-d2ec7c241fb7>:31: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "WARNING:tensorflow:From <ipython-input-19-d2ec7c241fb7>:32: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "output_classification_prob.get_shape() is\n",
      "(None, 1)\n",
      "real_imgs\n",
      "batch_size is\n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "training....\n",
      "Device mapping:\n",
      "\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[1.4019425]\n",
      "[0.6579832]\n",
      "[4.5441976]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.1451783]\n",
      "[2.3440633]\n",
      "0.107082605\n",
      "0.954871\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.8546835]\n",
      "[0.5119017]\n",
      "[3.9176223]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[0.98200244]\n",
      "[2.9277163]\n",
      "0.066428214\n",
      "0.95771134\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.5626264]\n",
      "[0.286288]\n",
      "[3.045987]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.5749352]\n",
      "[2.3405483]\n",
      "0.049288686\n",
      "0.94877344\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.71948814]\n",
      "[0.33153054]\n",
      "[2.3600678]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.201798]\n",
      "[1.7279559]\n",
      "0.041808497\n",
      "0.9596981\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.603095]\n",
      "[0.26250395]\n",
      "[2.8832214]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.4150702]\n",
      "[2.0196106]\n",
      "0.03714446\n",
      "0.9543778\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.63599735]\n",
      "[0.2153155]\n",
      "[2.1177807]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.5319982]\n",
      "[1.4872266]\n",
      "0.03338132\n",
      "0.95492655\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.4453373]\n",
      "[0.20355596]\n",
      "[1.9429613]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.5972]\n",
      "[1.421682]\n",
      "0.030011853\n",
      "0.95238763\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.34185505]\n",
      "[0.14917873]\n",
      "[1.6713196]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.7850258]\n",
      "[1.8248967]\n",
      "0.027443163\n",
      "0.9465649\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.48133084]\n",
      "[0.13103162]\n",
      "[2.4363875]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.8375432]\n",
      "[1.5107754]\n",
      "0.024551408\n",
      "0.9370455\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.4211197]\n",
      "[0.03900563]\n",
      "[2.0758688]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.0682185]\n",
      "[1.221698]\n",
      "0.022959404\n",
      "0.93168193\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.33403584]\n",
      "[0.13942337]\n",
      "[1.848808]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[1.9019412]\n",
      "[1.8233095]\n",
      "0.021336913\n",
      "0.9156144\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.21760008]\n",
      "[0.01824038]\n",
      "[1.7283798]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4774394]\n",
      "[1.3778208]\n",
      "0.020209549\n",
      "0.9155323\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.1950462]\n",
      "[0.0095558]\n",
      "[1.6787044]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.542296]\n",
      "[1.7760086]\n",
      "0.019656751\n",
      "0.91120404\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.17734142]\n",
      "[0.01795683]\n",
      "[1.7465831]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6475785]\n",
      "[1.6353791]\n",
      "0.018484006\n",
      "0.90549976\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.18175176]\n",
      "[0.04678205]\n",
      "[2.1282768]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3701239]\n",
      "[0.96246356]\n",
      "0.01770752\n",
      "0.89751226\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.14030491]\n",
      "[0.01700349]\n",
      "[1.5054047]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6321578]\n",
      "[1.0748245]\n",
      "0.017154172\n",
      "0.89224535\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.18439168]\n",
      "[0.02965488]\n",
      "[1.714022]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4915512]\n",
      "[1.2753873]\n",
      "0.016446289\n",
      "0.88569605\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.22574359]\n",
      "[0.02547854]\n",
      "[1.4937973]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6821494]\n",
      "[1.0693187]\n",
      "0.016277572\n",
      "0.8767855\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.08494145]\n",
      "[-0.]\n",
      "[1.5381292]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.764487]\n",
      "[1.7043742]\n",
      "0.015607088\n",
      "0.86460865\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.1073039]\n",
      "[0.00722424]\n",
      "[1.4025599]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7118049]\n",
      "[1.0561755]\n",
      "0.015312816\n",
      "0.8676746\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.09573552]\n",
      "[0.00505858]\n",
      "[1.1385745]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.845168]\n",
      "[1.3104846]\n",
      "0.014978688\n",
      "0.86474293\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.08180037]\n",
      "[0.00253309]\n",
      "[1.5574183]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.744097]\n",
      "[1.4998261]\n",
      "0.014414268\n",
      "0.8597265\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.12681347]\n",
      "[0.00693983]\n",
      "[1.4645913]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8854105]\n",
      "[1.5628102]\n",
      "0.014017608\n",
      "0.84804684\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.07151341]\n",
      "[0.0134592]\n",
      "[1.3851646]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8075726]\n",
      "[0.96219724]\n",
      "0.013651777\n",
      "0.8449329\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.16972964]\n",
      "[0.01644849]\n",
      "[1.3994342]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8492324]\n",
      "[1.0286007]\n",
      "0.013247892\n",
      "0.8413136\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.06270532]\n",
      "[0.00206542]\n",
      "[1.6217147]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6229322]\n",
      "[0.95441604]\n",
      "0.013019305\n",
      "0.84102136\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.08144181]\n",
      "[0.00512221]\n",
      "[1.3924525]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.734875]\n",
      "[1.111728]\n",
      "0.01282601\n",
      "0.83838046\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.03444199]\n",
      "[-0.]\n",
      "[1.296832]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7266173]\n",
      "[1.3336552]\n",
      "0.01261222\n",
      "0.8303982\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.07338277]\n",
      "[0.0106987]\n",
      "[1.4386581]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6879506]\n",
      "[0.565566]\n",
      "0.012339712\n",
      "0.82742715\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.03847871]\n",
      "[0.01527813]\n",
      "[1.6569997]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6979702]\n",
      "[1.2159424]\n",
      "0.01207548\n",
      "0.8183343\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02911239]\n",
      "[-0.]\n",
      "[0.95234996]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.595406]\n",
      "[0.82895446]\n",
      "0.011771075\n",
      "0.81423897\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02235393]\n",
      "[0.00302302]\n",
      "[0.9966619]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.723236]\n",
      "[1.2192589]\n",
      "0.011834894\n",
      "0.8116141\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02074152]\n",
      "[0.01005482]\n",
      "[1.402995]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5553107]\n",
      "[1.1751969]\n",
      "0.011684487\n",
      "0.7967346\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02935871]\n",
      "[0.01039425]\n",
      "[1.321344]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.683285]\n",
      "[1.2471212]\n",
      "0.011521509\n",
      "0.7881809\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.04940693]\n",
      "[-0.]\n",
      "[0.8967588]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3492203]\n",
      "[0.81378216]\n",
      "0.011272431\n",
      "0.79205984\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01764287]\n",
      "[0.00137359]\n",
      "[1.4567142]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3432233]\n",
      "[0.8500598]\n",
      "0.011350011\n",
      "0.78608984\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0211634]\n",
      "[0.01179503]\n",
      "[1.3398196]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3666525]\n",
      "[1.2518834]\n",
      "0.011280138\n",
      "0.78209317\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01541485]\n",
      "[0.02517929]\n",
      "[1.4836334]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.397088]\n",
      "[1.113326]\n",
      "0.011161739\n",
      "0.7788634\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02228876]\n",
      "[0.00712739]\n",
      "[1.1337467]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5121908]\n",
      "[1.0150024]\n",
      "0.010918675\n",
      "0.7750152\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00818023]\n",
      "[0.02423746]\n",
      "[1.1794257]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.243419]\n",
      "[0.90733445]\n",
      "0.010737299\n",
      "0.7708056\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00870037]\n",
      "[0.00581388]\n",
      "[1.3383489]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3362463]\n",
      "[0.89687705]\n",
      "0.01063921\n",
      "0.76683563\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02501884]\n",
      "[0.00643634]\n",
      "[1.2949805]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3359869]\n",
      "[1.0144043]\n",
      "0.010564839\n",
      "0.7598615\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01775016]\n",
      "[0.00617562]\n",
      "[1.1225189]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.2831635]\n",
      "[0.9851954]\n",
      "0.010283031\n",
      "0.75323474\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00958651]\n",
      "[0.01087057]\n",
      "[1.4386756]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4852087]\n",
      "[1.2311249]\n",
      "0.010170022\n",
      "0.7496179\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.03318895]\n",
      "[0.00315741]\n",
      "[0.97491366]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.2724044]\n",
      "[1.0305272]\n",
      "0.010195679\n",
      "0.74459493\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01938362]\n",
      "[0.00380461]\n",
      "[1.1024245]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.276798]\n",
      "[0.9438724]\n",
      "0.009988166\n",
      "0.7336306\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01904692]\n",
      "[0.0085641]\n",
      "[1.0213045]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.452307]\n",
      "[1.1540109]\n",
      "0.009937338\n",
      "0.73091257\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.04845458]\n",
      "[0.00037877]\n",
      "[1.3182358]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4419322]\n",
      "[0.9124277]\n",
      "0.009894315\n",
      "0.72568804\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.014372]\n",
      "[0.0115391]\n",
      "[1.298804]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.1867743]\n",
      "[1.0042981]\n",
      "0.009830493\n",
      "0.7169375\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01805341]\n",
      "[-0.]\n",
      "[1.0948306]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5646312]\n",
      "[1.0907158]\n",
      "0.009642873\n",
      "0.70933795\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0265135]\n",
      "[0.02264668]\n",
      "[1.4772547]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4919713]\n",
      "[0.87132275]\n",
      "0.009599388\n",
      "0.7086315\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.03029475]\n",
      "[-0.]\n",
      "[1.01103]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.48789]\n",
      "[0.87298954]\n",
      "0.009626154\n",
      "0.70263183\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.03159968]\n",
      "[-0.]\n",
      "[0.9937596]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6076417]\n",
      "[0.74148613]\n",
      "0.009396419\n",
      "0.6952006\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0404743]\n",
      "[0.01547407]\n",
      "[1.0858762]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.342157]\n",
      "[0.66328216]\n",
      "0.009371606\n",
      "0.68037146\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01192491]\n",
      "[0.00277362]\n",
      "[0.9182063]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4966197]\n",
      "[0.85902405]\n",
      "0.009188774\n",
      "0.6844789\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01200302]\n",
      "[0.00415866]\n",
      "[0.86905956]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3717194]\n",
      "[0.83585817]\n",
      "0.00940353\n",
      "0.6819368\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02039162]\n",
      "[0.0193951]\n",
      "[1.0836467]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.368144]\n",
      "[0.9654961]\n",
      "0.009260714\n",
      "0.67581373\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0137306]\n",
      "[-0.]\n",
      "[1.0923249]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.506221]\n",
      "[0.6690084]\n",
      "0.009110472\n",
      "0.6726293\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00374268]\n",
      "[0.0010664]\n",
      "[0.93822545]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4886572]\n",
      "[0.8863306]\n",
      "0.009206541\n",
      "0.6660889\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01894045]\n",
      "[0.01121624]\n",
      "[1.1231188]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.504096]\n",
      "[1.2854403]\n",
      "0.009199475\n",
      "0.664229\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0007019]\n",
      "[0.0090315]\n",
      "[0.88189274]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.593651]\n",
      "[0.81059307]\n",
      "0.009073332\n",
      "0.6580667\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01136499]\n",
      "[0.00309914]\n",
      "[0.9507497]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.453012]\n",
      "[0.84801]\n",
      "0.008995706\n",
      "0.65537524\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0122758]\n",
      "[-0.]\n",
      "[1.0291477]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.519805]\n",
      "[0.71439755]\n",
      "0.009052952\n",
      "0.647789\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01824378]\n",
      "[-0.]\n",
      "[0.9295469]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3674061]\n",
      "[0.9794337]\n",
      "0.009029489\n",
      "0.64617324\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.05204969]\n",
      "[0.0059253]\n",
      "[1.358398]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.2898753]\n",
      "[0.6628362]\n",
      "0.009179592\n",
      "0.64179194\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01026206]\n",
      "[0.01197028]\n",
      "[1.296748]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.2244296]\n",
      "[0.60284877]\n",
      "0.009071711\n",
      "0.6327806\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00747945]\n",
      "[0.0136568]\n",
      "[0.8252449]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5567296]\n",
      "[0.778301]\n",
      "0.00903762\n",
      "0.6326085\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01014341]\n",
      "[0.02278738]\n",
      "[1.0542814]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3419294]\n",
      "[0.77040845]\n",
      "0.009213848\n",
      "0.6293457\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01502877]\n",
      "[0.00473543]\n",
      "[0.95702934]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.521787]\n",
      "[0.85795236]\n",
      "0.009273041\n",
      "0.624158\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00567529]\n",
      "[0.767061]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.636856]\n",
      "[0.660381]\n",
      "0.009246893\n",
      "0.6149819\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01726513]\n",
      "[-0.]\n",
      "[1.0765481]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.534989]\n",
      "[0.8085107]\n",
      "0.00945589\n",
      "0.6170259\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01557799]\n",
      "[0.01119963]\n",
      "[1.4854928]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3350089]\n",
      "[0.79597956]\n",
      "0.009513334\n",
      "0.60762584\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0051229]\n",
      "[0.0008236]\n",
      "[1.0920354]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5731654]\n",
      "[0.88318187]\n",
      "0.0095277345\n",
      "0.6035407\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00589832]\n",
      "[0.00387863]\n",
      "[0.89467126]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.654772]\n",
      "[0.7956295]\n",
      "0.0096385125\n",
      "0.60179263\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00703672]\n",
      "[0.00621196]\n",
      "[1.2791271]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.474452]\n",
      "[0.8077654]\n",
      "0.009648621\n",
      "0.5978123\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00511343]\n",
      "[0.00926579]\n",
      "[1.0215491]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4406047]\n",
      "[0.6262759]\n",
      "0.009510417\n",
      "0.5826878\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02130516]\n",
      "[-0.]\n",
      "[1.007692]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6472335]\n",
      "[0.64351267]\n",
      "0.009754463\n",
      "0.5879613\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.03110677]\n",
      "[0.00102898]\n",
      "[0.91456246]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8299463]\n",
      "[1.0030786]\n",
      "0.009826345\n",
      "0.584283\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00137214]\n",
      "[0.00249864]\n",
      "[0.93921506]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7055984]\n",
      "[1.1411898]\n",
      "0.009865817\n",
      "0.5716327\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00758504]\n",
      "[-0.]\n",
      "[1.0597899]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7824357]\n",
      "[0.8028382]\n",
      "0.009679588\n",
      "0.57676417\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00936028]\n",
      "[-0.]\n",
      "[1.0274414]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6930654]\n",
      "[1.1210525]\n",
      "0.009726195\n",
      "0.57403356\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01864928]\n",
      "[0.00019749]\n",
      "[1.0671788]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6312287]\n",
      "[0.76492226]\n",
      "0.009821001\n",
      "0.5719258\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01522739]\n",
      "[0.00778963]\n",
      "[0.9604765]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5140595]\n",
      "[0.803673]\n",
      "0.010017133\n",
      "0.5684739\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02018056]\n",
      "[0.01029902]\n",
      "[0.8089821]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6520722]\n",
      "[0.73499644]\n",
      "0.010080916\n",
      "0.5685503\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00939702]\n",
      "[-0.]\n",
      "[0.97994494]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.727092]\n",
      "[0.82187307]\n",
      "0.010271665\n",
      "0.5656169\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02052013]\n",
      "[0.00110154]\n",
      "[1.0507923]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6806638]\n",
      "[0.98834264]\n",
      "0.010173714\n",
      "0.56510913\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00354476]\n",
      "[0.01911931]\n",
      "[0.9621588]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.510887]\n",
      "[0.7195091]\n",
      "0.010272414\n",
      "0.5611068\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01972378]\n",
      "[-0.]\n",
      "[1.1277529]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5917573]\n",
      "[0.6299831]\n",
      "0.010520539\n",
      "0.55889857\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00979317]\n",
      "[0.01637924]\n",
      "[0.75166476]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4553835]\n",
      "[0.83252984]\n",
      "0.01048331\n",
      "0.56127244\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02285476]\n",
      "[0.00793492]\n",
      "[0.7215767]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5183852]\n",
      "[0.80123466]\n",
      "0.01026356\n",
      "0.5579403\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.02987178]\n",
      "[0.0156274]\n",
      "[0.952846]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5171685]\n",
      "[0.61244553]\n",
      "0.01067969\n",
      "0.5575486\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01971608]\n",
      "[-0.]\n",
      "[1.0683932]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6487162]\n",
      "[0.9344744]\n",
      "0.010815263\n",
      "0.5563811\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00402162]\n",
      "[0.00833763]\n",
      "[1.1389441]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.588177]\n",
      "[0.790711]\n",
      "0.010845902\n",
      "0.5555576\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00862864]\n",
      "[-0.]\n",
      "[0.9097364]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4696126]\n",
      "[0.6715239]\n",
      "0.010858628\n",
      "0.55201375\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00638239]\n",
      "[-0.]\n",
      "[0.9588179]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6369517]\n",
      "[0.48196536]\n",
      "0.010950561\n",
      "0.55432016\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01070146]\n",
      "[0.01088357]\n",
      "[0.765904]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5112736]\n",
      "[0.69180226]\n",
      "0.011183013\n",
      "0.55371743\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01164474]\n",
      "[0.00490061]\n",
      "[0.9518656]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5510173]\n",
      "[0.5770853]\n",
      "0.011083382\n",
      "0.5488842\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00241624]\n",
      "[0.00338988]\n",
      "[1.0513762]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4990256]\n",
      "[0.84540635]\n",
      "0.011311062\n",
      "0.5509815\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00635394]\n",
      "[0.00866042]\n",
      "[1.0836158]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6059504]\n",
      "[0.78560776]\n",
      "0.011425056\n",
      "0.55018896\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01607377]\n",
      "[-0.]\n",
      "[0.79725343]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.4661844]\n",
      "[0.777471]\n",
      "0.011482384\n",
      "0.5534602\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01478214]\n",
      "[0.00808578]\n",
      "[0.69365555]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.550438]\n",
      "[0.7703644]\n",
      "0.011518026\n",
      "0.5539893\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00193299]\n",
      "[-0.]\n",
      "[0.78496516]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.3306365]\n",
      "[0.7102583]\n",
      "0.011571524\n",
      "0.5497204\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00453109]\n",
      "[0.01086084]\n",
      "[0.99141186]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7396739]\n",
      "[0.81051856]\n",
      "0.011474233\n",
      "0.552621\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00364691]\n",
      "[-0.]\n",
      "[0.94721043]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6087258]\n",
      "[0.74956346]\n",
      "0.011615949\n",
      "0.55425364\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.01282567]\n",
      "[0.0015192]\n",
      "[1.0539992]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.54034]\n",
      "[0.75520587]\n",
      "0.01167362\n",
      "0.5455162\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00888263]\n",
      "[0.8879371]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6380427]\n",
      "[0.7631731]\n",
      "0.012109725\n",
      "0.55466425\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00222097]\n",
      "[0.00800569]\n",
      "[0.9459322]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7659926]\n",
      "[0.5819847]\n",
      "0.011752837\n",
      "0.5486418\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00587881]\n",
      "[9.5164774e-05]\n",
      "[0.88049626]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5644565]\n",
      "[0.74919796]\n",
      "0.012080712\n",
      "0.5531633\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00671212]\n",
      "[0.0117261]\n",
      "[0.7384296]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6848218]\n",
      "[0.6033877]\n",
      "0.011942225\n",
      "0.5537021\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00721334]\n",
      "[0.01498018]\n",
      "[0.94571674]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5199983]\n",
      "[0.7307335]\n",
      "0.012004342\n",
      "0.5523951\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.03226556]\n",
      "[0.9464632]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6760676]\n",
      "[0.71200585]\n",
      "0.012115913\n",
      "0.552468\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00391145]\n",
      "[0.0061409]\n",
      "[0.78457206]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7836318]\n",
      "[0.9401627]\n",
      "0.012226889\n",
      "0.55432236\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00324234]\n",
      "[0.01174314]\n",
      "[0.82394636]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.5479937]\n",
      "[0.57625663]\n",
      "0.012331149\n",
      "0.55482036\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00590831]\n",
      "[0.00287967]\n",
      "[0.8662271]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6006556]\n",
      "[0.69697267]\n",
      "0.012285909\n",
      "0.5495603\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00458758]\n",
      "[-0.]\n",
      "[0.990285]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8011267]\n",
      "[0.9203675]\n",
      "0.012749681\n",
      "0.5531708\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0086092]\n",
      "[-0.]\n",
      "[0.91097695]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8635492]\n",
      "[0.66760576]\n",
      "0.012574156\n",
      "0.55516213\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00744213]\n",
      "[0.00022556]\n",
      "[1.1363704]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.888335]\n",
      "[0.5673655]\n",
      "0.012721029\n",
      "0.554633\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.01433141]\n",
      "[0.7510448]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7049944]\n",
      "[0.7838659]\n",
      "0.012915047\n",
      "0.55549186\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0050182]\n",
      "[0.00985343]\n",
      "[0.8789484]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7186098]\n",
      "[0.75940377]\n",
      "0.012973271\n",
      "0.5545469\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.80828744]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.757904]\n",
      "[0.5665437]\n",
      "0.013245029\n",
      "0.54865634\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00226131]\n",
      "[0.00847144]\n",
      "[0.79450685]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.6669743]\n",
      "[0.60360986]\n",
      "0.012917812\n",
      "0.5576507\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00749989]\n",
      "[0.95171124]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2067235]\n",
      "[0.71863914]\n",
      "0.012902341\n",
      "0.55848634\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00324574]\n",
      "[0.00151256]\n",
      "[0.94077736]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7394855]\n",
      "[0.7353289]\n",
      "0.013256196\n",
      "0.5585291\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00218258]\n",
      "[-0.]\n",
      "[0.99520284]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7660582]\n",
      "[0.6677844]\n",
      "0.01346372\n",
      "0.55554503\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8975009]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9697316]\n",
      "[0.7356533]\n",
      "0.013277051\n",
      "0.5615045\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00131661]\n",
      "[0.00427414]\n",
      "[1.0623844]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8989806]\n",
      "[0.8396263]\n",
      "0.01325599\n",
      "0.56290466\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00178632]\n",
      "[0.00784129]\n",
      "[0.756539]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8374732]\n",
      "[0.80419856]\n",
      "0.013719161\n",
      "0.5644585\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00158532]\n",
      "[0.00120046]\n",
      "[0.71694386]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.124298]\n",
      "[0.5650781]\n",
      "0.013439549\n",
      "0.56685823\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00813274]\n",
      "[0.01127361]\n",
      "[0.94830406]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.099223]\n",
      "[0.7259904]\n",
      "0.013531128\n",
      "0.5687129\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0019717]\n",
      "[-0.]\n",
      "[0.71041715]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0523713]\n",
      "[0.8962205]\n",
      "0.0135530075\n",
      "0.57197034\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00706677]\n",
      "[0.00947966]\n",
      "[0.9039742]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0950587]\n",
      "[0.98413926]\n",
      "0.013685578\n",
      "0.5742985\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.02257944]\n",
      "[0.9106205]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7854025]\n",
      "[0.67585695]\n",
      "0.01410028\n",
      "0.5762255\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00404707]\n",
      "[1.0886579]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8779018]\n",
      "[0.6947128]\n",
      "0.01385192\n",
      "0.57811064\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00419754]\n",
      "[-0.]\n",
      "[0.9114933]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.199436]\n",
      "[0.748526]\n",
      "0.014031084\n",
      "0.5769559\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[1.0328944]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8799872]\n",
      "[0.5201094]\n",
      "0.013901875\n",
      "0.577906\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.75505775]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8681467]\n",
      "[0.58188426]\n",
      "0.014427727\n",
      "0.5788825\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.0109897]\n",
      "[0.93997675]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8311408]\n",
      "[0.83548325]\n",
      "0.014238145\n",
      "0.58282536\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.79274905]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0483968]\n",
      "[0.89365095]\n",
      "0.014179228\n",
      "0.5836616\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00508148]\n",
      "[0.90588975]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7560308]\n",
      "[0.8129967]\n",
      "0.014385082\n",
      "0.58037007\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00485558]\n",
      "[0.00681024]\n",
      "[0.8887345]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9500992]\n",
      "[0.74697036]\n",
      "0.014324046\n",
      "0.5825952\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.5959767]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9793477]\n",
      "[0.7434892]\n",
      "0.014405914\n",
      "0.58508545\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00010094]\n",
      "[0.00232327]\n",
      "[0.6860961]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8373682]\n",
      "[0.8711369]\n",
      "0.014473926\n",
      "0.5845348\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.81395364]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8870625]\n",
      "[0.7489492]\n",
      "0.014584413\n",
      "0.5859795\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.87889194]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0642843]\n",
      "[1.0091434]\n",
      "0.01436421\n",
      "0.5856229\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0013035]\n",
      "[0.00072299]\n",
      "[0.6767825]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.121115]\n",
      "[0.7010977]\n",
      "0.014271649\n",
      "0.58533496\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.01143311]\n",
      "[0.87852925]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9826345]\n",
      "[0.6512491]\n",
      "0.014482483\n",
      "0.58523065\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9033754]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0346365]\n",
      "[0.63773704]\n",
      "0.014397982\n",
      "0.58524626\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0031662]\n",
      "[0.00593632]\n",
      "[0.8372699]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.03907]\n",
      "[0.7020372]\n",
      "0.014511343\n",
      "0.5730757\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00072011]\n",
      "[0.00140191]\n",
      "[0.9472187]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9751174]\n",
      "[0.6100681]\n",
      "0.0147274155\n",
      "0.58710575\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7420477]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9958127]\n",
      "[0.9127809]\n",
      "0.0146272015\n",
      "0.5853363\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8439085]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2049406]\n",
      "[0.58443505]\n",
      "0.015012492\n",
      "0.5835945\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00295959]\n",
      "[-0.]\n",
      "[0.9645701]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.968072]\n",
      "[1.0571542]\n",
      "0.014780772\n",
      "0.5788701\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00180296]\n",
      "[-0.]\n",
      "[0.8630895]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.959858]\n",
      "[0.6531837]\n",
      "0.015055724\n",
      "0.5960092\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00388567]\n",
      "[-0.]\n",
      "[0.8520301]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9979596]\n",
      "[0.7780092]\n",
      "0.015196101\n",
      "0.5918737\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[1.1172833]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0251508]\n",
      "[0.6126184]\n",
      "0.015435646\n",
      "0.60004383\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00100384]\n",
      "[0.8933975]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0630367]\n",
      "[0.8815021]\n",
      "0.015274143\n",
      "0.5979752\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00744242]\n",
      "[-0.]\n",
      "[0.6058492]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8825676]\n",
      "[0.7895099]\n",
      "0.015335051\n",
      "0.6064593\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00073281]\n",
      "[0.8501837]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9131827]\n",
      "[0.782114]\n",
      "0.015105925\n",
      "0.6089125\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00196378]\n",
      "[-0.]\n",
      "[0.7775033]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7500174]\n",
      "[0.66435456]\n",
      "0.015405864\n",
      "0.61043334\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00207971]\n",
      "[-0.]\n",
      "[0.7717401]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9246383]\n",
      "[0.70104027]\n",
      "0.015617673\n",
      "0.6080128\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00068065]\n",
      "[0.00304242]\n",
      "[0.88777184]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9970007]\n",
      "[0.63595146]\n",
      "0.015139063\n",
      "0.60873604\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8381589]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.7990336]\n",
      "[0.8981257]\n",
      "0.015148633\n",
      "0.59790736\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9033548]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9512765]\n",
      "[0.98452723]\n",
      "0.015269334\n",
      "0.5944625\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.02938136]\n",
      "[0.8814205]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9027133]\n",
      "[0.9849476]\n",
      "0.01579809\n",
      "0.60225207\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00169118]\n",
      "[-0.]\n",
      "[0.94188994]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0815432]\n",
      "[0.5519319]\n",
      "0.015503369\n",
      "0.6022774\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00651001]\n",
      "[0.6332925]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9214969]\n",
      "[0.8104267]\n",
      "0.015428389\n",
      "0.5928568\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.01100616]\n",
      "[0.83466303]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.9105458]\n",
      "[0.57089806]\n",
      "0.015675817\n",
      "0.5993628\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.74553794]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.1818287]\n",
      "[0.6836225]\n",
      "0.016039763\n",
      "0.59876114\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.95128196]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8994238]\n",
      "[0.8426146]\n",
      "0.016280046\n",
      "0.59714025\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00596039]\n",
      "[-0.]\n",
      "[0.80422413]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0467725]\n",
      "[0.79537964]\n",
      "0.016099017\n",
      "0.6019031\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.01650034]\n",
      "[0.9140795]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.8418655]\n",
      "[1.0162209]\n",
      "0.016859233\n",
      "0.6035884\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00224757]\n",
      "[0.8848042]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0950017]\n",
      "[0.51765275]\n",
      "0.016541474\n",
      "0.59992385\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.77814585]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0955408]\n",
      "[0.7049358]\n",
      "0.016878694\n",
      "0.6087295\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00021104]\n",
      "[-0.]\n",
      "[0.7634512]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.1621706]\n",
      "[0.95741975]\n",
      "0.017108418\n",
      "0.6080586\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00608728]\n",
      "[0.80611384]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[2.946456]\n",
      "[0.45426145]\n",
      "0.017581757\n",
      "0.6050814\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8984254]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3325522]\n",
      "[0.7292298]\n",
      "0.017406004\n",
      "0.6093601\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.0009284]\n",
      "[0.8078961]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2065892]\n",
      "[0.88827485]\n",
      "0.017807782\n",
      "0.60487556\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[1.0717059]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2444637]\n",
      "[0.92693466]\n",
      "0.017539822\n",
      "0.61152637\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00037171]\n",
      "[-0.]\n",
      "[0.94549525]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5692694]\n",
      "[0.6004301]\n",
      "0.017812826\n",
      "0.6104916\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.0027969]\n",
      "[0.8833773]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2458944]\n",
      "[0.6587992]\n",
      "0.018002752\n",
      "0.60777205\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.84874636]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3375738]\n",
      "[0.85352385]\n",
      "0.017941996\n",
      "0.6082408\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.776664]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6977425]\n",
      "[0.8034551]\n",
      "0.01783052\n",
      "0.60828435\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6724843]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3102844]\n",
      "[0.8649301]\n",
      "0.018054415\n",
      "0.60511947\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7212569]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3241918]\n",
      "[0.8008878]\n",
      "0.018297346\n",
      "0.6083897\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7504602]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3798766]\n",
      "[0.68741745]\n",
      "0.018403107\n",
      "0.5991614\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00192005]\n",
      "[0.00193275]\n",
      "[0.726135]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.1875148]\n",
      "[0.7075302]\n",
      "0.018391717\n",
      "0.6002176\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8668182]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3620503]\n",
      "[0.70596707]\n",
      "0.018225275\n",
      "0.60997146\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[1.1181328]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5370183]\n",
      "[0.65640265]\n",
      "0.018468384\n",
      "0.6099731\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00055558]\n",
      "[-0.]\n",
      "[0.8131166]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2704434]\n",
      "[0.82797736]\n",
      "0.01897816\n",
      "0.6123379\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00094847]\n",
      "[0.00191772]\n",
      "[0.83805853]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2302861]\n",
      "[0.8911115]\n",
      "0.019259341\n",
      "0.61378086\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7530767]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3833146]\n",
      "[0.6064238]\n",
      "0.01873843\n",
      "0.6174275\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8349374]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3153408]\n",
      "[0.76971954]\n",
      "0.018868506\n",
      "0.6207091\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00202863]\n",
      "[-0.]\n",
      "[0.64020145]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.338004]\n",
      "[0.78993237]\n",
      "0.01884361\n",
      "0.62261987\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00068915]\n",
      "[0.00073511]\n",
      "[0.7717432]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6947808]\n",
      "[0.8100204]\n",
      "0.018974986\n",
      "0.62054163\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00201248]\n",
      "[-0.]\n",
      "[0.74466896]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2207682]\n",
      "[0.84151524]\n",
      "0.019234393\n",
      "0.6167719\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8294148]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3348536]\n",
      "[0.6128019]\n",
      "0.019261345\n",
      "0.6183261\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00221104]\n",
      "[1.0419856]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4707255]\n",
      "[0.6533638]\n",
      "0.019045545\n",
      "0.598795\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.5449855]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2370956]\n",
      "[0.52321917]\n",
      "0.019225895\n",
      "0.6112541\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.62251335]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4257364]\n",
      "[0.6190699]\n",
      "0.019658081\n",
      "0.6018612\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9589329]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3818395]\n",
      "[0.82692564]\n",
      "0.019918611\n",
      "0.6078265\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00668322]\n",
      "[0.00246815]\n",
      "[0.95607823]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3191972]\n",
      "[0.8188898]\n",
      "0.019688752\n",
      "0.6117619\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00644717]\n",
      "[-0.]\n",
      "[0.9466388]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3343775]\n",
      "[0.7398392]\n",
      "0.0199016\n",
      "0.608941\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00735449]\n",
      "[0.8016119]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.30841]\n",
      "[0.6436965]\n",
      "0.020318063\n",
      "0.61244\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00207555]\n",
      "[-0.]\n",
      "[0.77038044]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3866947]\n",
      "[0.67175955]\n",
      "0.02042025\n",
      "0.6161221\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00206086]\n",
      "[0.7942171]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3086903]\n",
      "[0.71084684]\n",
      "0.020287074\n",
      "0.61724347\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00079615]\n",
      "[0.01358918]\n",
      "[0.760634]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.283998]\n",
      "[0.8121469]\n",
      "0.020372493\n",
      "0.6097715\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00047551]\n",
      "[0.69325745]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5295603]\n",
      "[0.6190638]\n",
      "0.020514615\n",
      "0.62174124\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00037464]\n",
      "[-0.]\n",
      "[0.8581456]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.111464]\n",
      "[0.5923233]\n",
      "0.021268973\n",
      "0.6228457\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.77464014]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0861309]\n",
      "[0.7513162]\n",
      "0.021184376\n",
      "0.610041\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[6.283323e-06]\n",
      "[-0.]\n",
      "[0.97731936]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.2258344]\n",
      "[0.5020227]\n",
      "0.021696325\n",
      "0.6216091\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[1.0846759]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.515973]\n",
      "[0.69708127]\n",
      "0.0213709\n",
      "0.6211521\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.88883597]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5121384]\n",
      "[0.65111107]\n",
      "0.021542778\n",
      "0.6261947\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9640841]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3193405]\n",
      "[0.7422375]\n",
      "0.022120109\n",
      "0.6291951\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00064545]\n",
      "[-0.]\n",
      "[0.9063109]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3692803]\n",
      "[0.5731459]\n",
      "0.022168456\n",
      "0.62860715\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00458802]\n",
      "[-0.]\n",
      "[0.7983686]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.278705]\n",
      "[0.55445313]\n",
      "0.022536784\n",
      "0.6306368\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00259113]\n",
      "[0.01120078]\n",
      "[0.90688]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.24713]\n",
      "[0.9315342]\n",
      "0.022079593\n",
      "0.6182854\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8855479]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3013966]\n",
      "[0.67504084]\n",
      "0.022606404\n",
      "0.6299919\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00107976]\n",
      "[-0.]\n",
      "[0.59116036]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.0647252]\n",
      "[0.95774686]\n",
      "0.023225505\n",
      "0.6296314\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00843505]\n",
      "[0.71184707]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.209221]\n",
      "[0.9400702]\n",
      "0.022842303\n",
      "0.6276208\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00378126]\n",
      "[0.00504332]\n",
      "[0.7330579]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.3753343]\n",
      "[0.8881845]\n",
      "0.022906128\n",
      "0.6240102\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00810736]\n",
      "[0.9024549]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4071648]\n",
      "[0.6929427]\n",
      "0.023331974\n",
      "0.6277354\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.79139644]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4587352]\n",
      "[0.581754]\n",
      "0.023373239\n",
      "0.6275256\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8061627]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4176245]\n",
      "[0.77186906]\n",
      "0.023518998\n",
      "0.6291411\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.99434775]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5655048]\n",
      "[0.9304127]\n",
      "0.02386332\n",
      "0.62835056\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8634072]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5576282]\n",
      "[0.5580873]\n",
      "0.024261618\n",
      "0.6268838\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.629958]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5808094]\n",
      "[0.66662216]\n",
      "0.023921438\n",
      "0.617475\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7856725]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5885057]\n",
      "[0.5896635]\n",
      "0.024428662\n",
      "0.62685865\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00216653]\n",
      "[0.91622096]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.531447]\n",
      "[0.7796571]\n",
      "0.024535988\n",
      "0.6233191\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9163378]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8113458]\n",
      "[0.79002845]\n",
      "0.024589418\n",
      "0.6267662\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7827721]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7787535]\n",
      "[0.7399359]\n",
      "0.024917666\n",
      "0.62632906\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0051359]\n",
      "[0.00045084]\n",
      "[0.75274754]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5639787]\n",
      "[0.9942549]\n",
      "0.02511015\n",
      "0.62306917\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7909252]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6725543]\n",
      "[0.8794375]\n",
      "0.025112655\n",
      "0.62567466\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8871168]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6608543]\n",
      "[0.5253948]\n",
      "0.024778213\n",
      "0.62706506\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.64519244]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.439189]\n",
      "[0.8006694]\n",
      "0.025554912\n",
      "0.6297022\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7471945]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7417717]\n",
      "[0.7475676]\n",
      "0.02510841\n",
      "0.6286104\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9260817]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6158285]\n",
      "[0.6968082]\n",
      "0.025227822\n",
      "0.6322796\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.84752464]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6076808]\n",
      "[0.6325418]\n",
      "0.025232337\n",
      "0.62516224\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8093141]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5177932]\n",
      "[0.78231776]\n",
      "0.025540499\n",
      "0.6343815\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00350126]\n",
      "[1.0606248]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5858526]\n",
      "[0.6126742]\n",
      "0.025694693\n",
      "0.63410574\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00567366]\n",
      "[0.63056743]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6602743]\n",
      "[0.80461407]\n",
      "0.025994362\n",
      "0.63329667\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9091437]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8079195]\n",
      "[0.8985411]\n",
      "0.025761588\n",
      "0.6253956\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6446032]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5130703]\n",
      "[0.8170635]\n",
      "0.025930407\n",
      "0.6277584\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00685224]\n",
      "[-0.]\n",
      "[0.91309714]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6340966]\n",
      "[0.79457074]\n",
      "0.02592719\n",
      "0.6221053\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7894231]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.39854]\n",
      "[0.74827325]\n",
      "0.026655518\n",
      "0.6352547\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00171059]\n",
      "[0.9191396]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.66242]\n",
      "[0.6009287]\n",
      "0.026363123\n",
      "0.6346807\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.04310478]\n",
      "[0.7120131]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4653337]\n",
      "[1.1250167]\n",
      "0.026903156\n",
      "0.635315\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8211623]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6578805]\n",
      "[0.84067166]\n",
      "0.026587497\n",
      "0.6325896\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6056866]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.578298]\n",
      "[0.7008305]\n",
      "0.026689637\n",
      "0.6342003\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00203133]\n",
      "[0.8760502]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7680664]\n",
      "[1.0394744]\n",
      "0.026854444\n",
      "0.634082\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9069777]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.655816]\n",
      "[0.6111341]\n",
      "0.027649488\n",
      "0.6353836\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.628565]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.654595]\n",
      "[0.75413316]\n",
      "0.027453315\n",
      "0.63444936\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8324567]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.972735]\n",
      "[1.0151079]\n",
      "0.027442465\n",
      "0.6334497\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00032072]\n",
      "[0.7540079]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.84995]\n",
      "[0.7708736]\n",
      "0.027405668\n",
      "0.63280743\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7154349]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9042659]\n",
      "[0.8565197]\n",
      "0.02767035\n",
      "0.6331567\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6810127]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8684459]\n",
      "[0.6812063]\n",
      "0.027863724\n",
      "0.6352594\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.81007445]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9836023]\n",
      "[0.6285222]\n",
      "0.0282056\n",
      "0.63672143\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.795504]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[4.135753]\n",
      "[0.7331188]\n",
      "0.028068196\n",
      "0.63545185\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0012539]\n",
      "[-0.]\n",
      "[0.7688159]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.709407]\n",
      "[0.68902904]\n",
      "0.02872349\n",
      "0.6363246\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.78642464]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8315904]\n",
      "[0.69984907]\n",
      "0.028763562\n",
      "0.63601494\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00198868]\n",
      "[-0.]\n",
      "[0.7427266]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.776435]\n",
      "[0.5485939]\n",
      "0.028701266\n",
      "0.6375445\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7054339]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.818662]\n",
      "[0.6964683]\n",
      "0.029187704\n",
      "0.62963235\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7055138]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9888997]\n",
      "[0.8151983]\n",
      "0.029177558\n",
      "0.63664633\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00049816]\n",
      "[-0.]\n",
      "[0.8529305]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[4.0635705]\n",
      "[1.0527459]\n",
      "0.029252388\n",
      "0.63998777\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.75782114]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.651756]\n",
      "[0.77097195]\n",
      "0.030125827\n",
      "0.6406695\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00011648]\n",
      "[-0.]\n",
      "[1.0176095]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[4.0149593]\n",
      "[0.8958004]\n",
      "0.029442362\n",
      "0.63721263\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00493244]\n",
      "[0.6940422]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9459615]\n",
      "[0.90156543]\n",
      "0.029503522\n",
      "0.6334693\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.79742444]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8832204]\n",
      "[0.8713342]\n",
      "0.029731436\n",
      "0.63371915\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00249208]\n",
      "[0.7171972]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7288375]\n",
      "[0.75860685]\n",
      "0.029551025\n",
      "0.6310854\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00669929]\n",
      "[0.788232]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.836249]\n",
      "[0.84751755]\n",
      "0.029921962\n",
      "0.62449956\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.87512845]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8376749]\n",
      "[0.95957315]\n",
      "0.030080345\n",
      "0.627343\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00150883]\n",
      "[-0.]\n",
      "[0.7741465]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9669387]\n",
      "[0.7807977]\n",
      "0.030481832\n",
      "0.6273116\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8963445]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9063847]\n",
      "[0.79812014]\n",
      "0.030418683\n",
      "0.62358207\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6896096]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8113794]\n",
      "[0.57683015]\n",
      "0.030115511\n",
      "0.6264093\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7177274]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9564908]\n",
      "[0.86099416]\n",
      "0.030604368\n",
      "0.62833637\n",
      "==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.837558]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8225977]\n",
      "[0.72075284]\n",
      "0.030338727\n",
      "0.6288843\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00030649]\n",
      "[-0.]\n",
      "[0.80749136]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7907195]\n",
      "[0.65674424]\n",
      "0.030176483\n",
      "0.6296865\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0015788]\n",
      "[-0.]\n",
      "[0.7719813]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.748906]\n",
      "[0.7443603]\n",
      "0.03094076\n",
      "0.6335649\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6768618]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8976717]\n",
      "[0.7148191]\n",
      "0.030321818\n",
      "0.63357085\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00334187]\n",
      "[-0.]\n",
      "[0.75396955]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8803067]\n",
      "[0.667801]\n",
      "0.030715134\n",
      "0.6352282\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.78411424]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6905558]\n",
      "[0.706538]\n",
      "0.03123198\n",
      "0.6368977\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00325238]\n",
      "[-0.]\n",
      "[0.86557573]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6700184]\n",
      "[0.5132806]\n",
      "0.031868454\n",
      "0.63825774\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00051043]\n",
      "[-0.]\n",
      "[0.83132106]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6954854]\n",
      "[0.9331735]\n",
      "0.031244298\n",
      "0.6382199\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.677457]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5101929]\n",
      "[0.84008884]\n",
      "0.031683017\n",
      "0.64084053\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00107438]\n",
      "[0.7498546]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.359477]\n",
      "[0.8513781]\n",
      "0.031894937\n",
      "0.6368406\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00274242]\n",
      "[0.78340554]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7323258]\n",
      "[1.0223931]\n",
      "0.031467292\n",
      "0.6371269\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00094939]\n",
      "[-0.]\n",
      "[0.8551798]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.77132]\n",
      "[0.8566668]\n",
      "0.031644724\n",
      "0.6408268\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.827884]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5300226]\n",
      "[0.70322025]\n",
      "0.033000585\n",
      "0.6417323\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.62776095]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6987202]\n",
      "[0.8807055]\n",
      "0.032407228\n",
      "0.6403828\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7782429]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.651528]\n",
      "[0.7310041]\n",
      "0.03215249\n",
      "0.6376518\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00361985]\n",
      "[-0.]\n",
      "[0.52384275]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4108021]\n",
      "[0.67356676]\n",
      "0.033163518\n",
      "0.63405323\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.86555374]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8218987]\n",
      "[0.8923425]\n",
      "0.032673962\n",
      "0.63762474\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7751806]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.833867]\n",
      "[0.51755977]\n",
      "0.032963835\n",
      "0.6387853\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9721054]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.4385848]\n",
      "[0.6689093]\n",
      "0.03371451\n",
      "0.63774276\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00280358]\n",
      "[0.8704195]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6234915]\n",
      "[0.70555687]\n",
      "0.033317864\n",
      "0.639146\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.63543266]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.6662498]\n",
      "[0.74509084]\n",
      "0.033641398\n",
      "0.6370156\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00243891]\n",
      "[0.72578925]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5116293]\n",
      "[0.8560609]\n",
      "0.034346603\n",
      "0.6412164\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00116338]\n",
      "[-0.]\n",
      "[0.663066]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7422853]\n",
      "[0.98697054]\n",
      "0.034943253\n",
      "0.6357395\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00229511]\n",
      "[-0.]\n",
      "[0.5517344]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7615714]\n",
      "[0.7350099]\n",
      "0.03422468\n",
      "0.64005154\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6439405]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.9476256]\n",
      "[0.6298215]\n",
      "0.035399742\n",
      "0.64003146\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.77060086]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7740488]\n",
      "[0.9301322]\n",
      "0.03578773\n",
      "0.6433746\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.74305856]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.890908]\n",
      "[0.53957593]\n",
      "0.035471313\n",
      "0.642836\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[0.00055269]\n",
      "[0.6755903]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7883413]\n",
      "[0.91285235]\n",
      "0.036047302\n",
      "0.6417603\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.9516945]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7747505]\n",
      "[0.5043831]\n",
      "0.03618\n",
      "0.6413068\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.83274096]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5429535]\n",
      "[0.75615895]\n",
      "0.036271516\n",
      "0.6406802\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.0007891]\n",
      "[-0.]\n",
      "[0.9289589]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[4.103174]\n",
      "[0.67451984]\n",
      "0.0361951\n",
      "0.63535815\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.7116585]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.5807557]\n",
      "[0.8736836]\n",
      "0.036077097\n",
      "0.6388095\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.6770276]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7341037]\n",
      "[0.93691]\n",
      "0.0356673\n",
      "0.6413267\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.87400943]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8622167]\n",
      "[1.067641]\n",
      "0.036063984\n",
      "0.6407096\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[0.00541126]\n",
      "[-0.]\n",
      "[0.86013794]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7411883]\n",
      "[0.7199323]\n",
      "0.03590276\n",
      "0.6434717\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.82385343]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.8514028]\n",
      "[0.75370115]\n",
      "0.03543724\n",
      "0.6428964\n",
      "==================\n",
      "train_D_hinge_1, train_D_hinge_2, train_D_regression_error\n",
      "[-0.]\n",
      "[-0.]\n",
      "[0.8195565]\n",
      "\n",
      "\n",
      "train_G_L1, train_G_L2, train_G_L3, train_G_l4\n",
      "[3.7514157]\n",
      "[0.75019306]\n",
      "0.036457792\n",
      "0.645642\n",
      "==================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f8761b06965f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_SSL_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_temp_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_temp_by_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-3d14ca4d91ae>\u001b[0m in \u001b[0;36mtrain_SSL_GAN\u001b[0;34m(batch_size, num_epochs, train_data_by_class, train_temp_by_class, test_data_by_class, test_temp_by_class)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 fetch_ops =sess.run(fetches=[D_optimizer, G_optimizer, D_L, G_L, accuracy, \n\u001b[1;32m     72\u001b[0m                                              D_L_hinge_1, D_L_hinge_2, D_L_regression_error, G_L1, G_L2, G_L3, G_L4], \n\u001b[0;32m---> 73\u001b[0;31m                                     feed_dict=train_feed_dictionary)\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_D_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_G_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtrain_D_hinge_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_D_hinge_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_D_regression_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_G_L1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_G_L2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_G_L3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_G_L4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_SSL_GAN(60, 300, train_data_by_class, train_temp_by_class, test_data_by_class, test_temp_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp_by_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, labels, numClasses):\n",
    "            #pred 0   #pred 1\n",
    "    #real 0\n",
    "    #real 1\n",
    "    matrix = np.zeros((numClasses, numClasses))\n",
    "    for n in range(len(predictions)):\n",
    "        matrix[int(labels[n]), int(predictions[n])] += 1\n",
    "    return matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Retrieve best DCGAN model and run the whole training set or test set to get the feature map vectors, \n",
    "#which are later used to either\n",
    "#i. train a Random Forest Model\n",
    "#or\n",
    "#ii. run on test set\n",
    "#start new session:\n",
    "def testModel(test_data_by_class, test_label_by_class, batch_size):\n",
    "    labels = np.zeros((0,num_classes))\n",
    "    predictions = np.zeros((0))\n",
    "    correct_preds = np.zeros((0))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess: \n",
    "        #bestModel = tf.train.import_meta_graph('./savedModels/GAN/'+str(datetime.date.today())+'_SSL_GAN.ckpt.meta')#load graph\n",
    "        bestModel = tf.train.import_meta_graph('./savedModels/GAN/2020-01-27_SSL_GAN.ckpt.meta')#load graph\n",
    "\n",
    "        bestModel.restore(sess,tf.train.latest_checkpoint('./savedModels/GAN'))#load parameters\n",
    "        #feed placeholders:\n",
    "        x = sess.graph.get_tensor_by_name(\"x:0\")\n",
    "        label = sess.graph.get_tensor_by_name(\"label:0\")\n",
    "        labeled_mask = sess.graph.get_tensor_by_name(\"labeled_mask:0\")\n",
    "        dropout_rate = sess.graph.get_tensor_by_name(\"dropout_rate:0\")\n",
    "        is_training = sess.graph.get_tensor_by_name(\"is_training:0\")\n",
    "        #output metrics of interest:\n",
    "        D_real_prob = sess.graph.get_tensor_by_name(\"Discriminator/D_output:0\")\n",
    "        G_output = sess.graph.get_tensor_by_name(\"Generator/G_output:0\")\n",
    "        correct_prediction = sess.graph.get_tensor_by_name(\"correct_prediction:0\")\n",
    "#         prediction = tf.argmax(D_real_prob, axis = 1)\n",
    "        prediction = tf.argmax( D_real_prob[:, :-1], axis = 1)\n",
    "\n",
    "        test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "        \n",
    "        #for debugging: output weight names\n",
    "        weights = [v.eval(session=sess) for v in tf.trainable_variables()]\n",
    "        weightNames = [v for v in tf.trainable_variables()]\n",
    "        i = 0\n",
    "        for batch_x, batch_y in test_generator:\n",
    "            #print('batch_y:')\n",
    "            #print(batch_y)\n",
    "            mask = np.ones(len(batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "            feed_dictionary = {x: normalize(batch_x),\n",
    "                               label: batch_y,\n",
    "                               labeled_mask: mask,\n",
    "                               dropout_rate: 0.0,\n",
    "                               is_training: False}\n",
    "            \n",
    "            #accuracy_eval = accuracy.eval(feed_dict = feed_dictionary)\n",
    "            correct_pred_eval = correct_prediction.eval(feed_dict = feed_dictionary)\n",
    "            D_real_prob_eval = D_real_prob.eval(feed_dict = feed_dictionary)\n",
    "            \n",
    "            correct_preds = np.concatenate((correct_preds, correct_pred_eval))\n",
    "            prediction_eval = prediction.eval(feed_dict=feed_dictionary)\n",
    "            labels = np.concatenate((labels, batch_y))#array of k+1 one hot\n",
    "            predictions = np.concatenate((predictions, prediction_eval))#array of ints\n",
    "            #print('real features: batch'+str(i))\n",
    "            #print(D_real_features_eval)\n",
    "            #print('accuracy: '+str(accuracy_eval))\n",
    "            #print(str(D_real_features_eval.shape))\n",
    "            #print('labels'+str(labels.shape))\n",
    "            #print('featuremaps'+str(feature_maps.shape))\n",
    "            #print('real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            #print('dropout1:')\n",
    "            #print(dropout1.eval(feed_dict = feed_dictionary))\n",
    "            #print('conv1:')\n",
    "            #print(conv1.eval(feed_dict = feed_dictionary))\n",
    "            #print('x:')\n",
    "            #print(x.eval(feed_dict = feed_dictionary))\n",
    "            #print('D_real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            i = i + 1\n",
    "        sess.close()\n",
    "    total_accuracy = np.sum(correct_preds)/len(correct_preds)\n",
    "    print('total accuracy: '+str(total_accuracy))\n",
    "#     print('labels:')\n",
    "#     print(labels)\n",
    "    print('predictions:')\n",
    "    print(predictions)\n",
    "    print('Confusion Matirx:')\n",
    "#     print(confusion_matrix(predictions, np.argmax(labels, axis = 1), num_classes+1))\n",
    "    print(confusion_matrix(np.argmax(labels, axis = 1), predictions))\n",
    "#     return labels, weights, weightNames\n",
    "    return predictions, labels, weights, weightNames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test model:\n",
    "predictions, labels_test, params, paramNames = testModel(test_data_by_class, test_label_by_class, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "def loss_accuracy_baseline(D_real_features, D_real_logit, D_real_prob, extended_label, labeled_mask):\n",
    "    epsilon = 1e-8 # used to avoid NAN loss\n",
    "    # *** Discriminator loss ***\n",
    "    # supervised loss\n",
    "    # which class the real data belongs to\n",
    "    tmp = tf.nn.softmax_cross_entropy_with_logits(logits = D_real_logit,#cross_entrypy_with_logits is the only function available\n",
    "                                                  labels = extended_label)\n",
    "    #question: is this an over simplification?--no, becuase tmp is a vector: num_samples_in_batch * 1\n",
    "    D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / (tf.reduce_sum(labeled_mask)+1e-6) # to ignore unlabeled data\n",
    "                                                                                     \n",
    "    # accuracy--This is cross validation accuracy within the training set--Nov, 2019\n",
    "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  tf.argmax(extended_label[:, :-1], 1), name='correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast boolean to float32--Bill Zhai Aug, 2019\n",
    "    \n",
    "    \n",
    "    return D_L_supervised, accuracy, correct_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_baseline(D_Loss, D_learning_rate):\n",
    "    # D and G optimizer\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        all_vars = tf.trainable_variables()\n",
    "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
    "        D_optimizer = tf.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
    "        return D_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model for each batch using D() and G() functions\n",
    "def build_model_baseline(x_real, label, dropout_rate, is_training, print_summary = False):\n",
    "    # build model\n",
    "    #Discriminator on real data (labeled and unlabeled)  flatten5, fc5, output\n",
    "    D_real_features, D_real_logit, D_real_prob = D(x_real, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    return D_real_features, D_real_logit, D_real_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(batch_size, num_epochs, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class):\n",
    "    # train Semi-Supervised Learning GAN\n",
    "    train_D_losses, train_Accs = [], []\n",
    "    val_D_losses, val_Accs = [], []\n",
    "\n",
    "    cv_size = batch_size\n",
    "    num_train_exs = numTrain\n",
    "    num_val_exs = numTest\n",
    "    print(batch_size)\n",
    "    print(\"num_train_exs: \", num_train_exs)\n",
    "    print(\"num_val_exs: \", num_val_exs)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, name = 'x', shape = [None, x_height, x_width, num_channels])\n",
    "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes]) # one hot label--BZ, August, 2019\n",
    "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    D_learning_rate = tf.placeholder(tf.float32, name = 'D_learning_rate')\n",
    "\n",
    "    model = build_model_baseline(x, label, dropout_rate, is_training, print_summary = False)\n",
    "    D_real_features, D_real_logit, D_real_prob = model\n",
    "    extended_label = prepare_labels(label) #is only for real image data\n",
    "    loss_acc  = loss_accuracy_baseline(D_real_features, D_real_logit, D_real_prob,\n",
    "                              extended_label, labeled_mask)\n",
    "    D_L, accuracy, correct_prediction = loss_acc\n",
    "    D_optimizer = optimizer_baseline(D_L, D_learning_rate)\n",
    "\n",
    "    \n",
    "#     validation_generator = get_batch(data_path, label_path, num_val_exs, num_train_exs, True)\n",
    "\n",
    "    print('training....')\n",
    "\n",
    "    with tf.Session() as sess:       \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #mnist_set = get_data()\n",
    "\n",
    "        t_total = 0\n",
    "        #changed to iterating on number of epochs!\n",
    "        iter_count = 0\n",
    "        iter_since_last_val = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            for iteration in range(int(numTrain/batch_size)):\n",
    "            #batch_num = 0 #added--BZ Nov 2, 2019\n",
    "            #training_generator = get_training_batch_and_labeled_mask(X_train, y_train, LABELED_MASK, batch_size);\n",
    "                train_batch_x, train_batch_y, train_batch_mask = get_balance_train_batch(train_data_by_class, train_label_by_class, train_mask_by_class, batch_size)\n",
    "            #for train_batch_x, train_batch_y, train_batch_mask in training_generator:\n",
    "                t_start = time.time()\n",
    "                #function is to be modified as labeled mask should stay with \"labeled\" images which are shuffled BZ--August, 2019\n",
    "                #mask = get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize);--marked as solved BZ, Nov, 2019\n",
    "                train_feed_dictionary = {x:  normalize(train_batch_x),\n",
    "                                         label: train_batch_y,\n",
    "                                         labeled_mask: train_batch_mask,\n",
    "                                         dropout_rate: 0.0,\n",
    "                                         D_learning_rate: 1e-5,\n",
    "                                         is_training: True}\n",
    "\n",
    "                D_optimizer.run(feed_dict = train_feed_dictionary)\n",
    "\n",
    "                train_D_loss = D_L.eval(feed_dict = train_feed_dictionary)\n",
    "                train_accuracy = accuracy.eval(feed_dict = train_feed_dictionary)\n",
    "                t_total += (time.time() - t_start)\n",
    "\n",
    "                print('Epoch: '+str(epoch)+\" Iter: \"+str(iteration)+\" Time: \"+str(t_total)+\" train_D_loss: \"+str(train_D_loss)+\" train_accuracy: \"+str(train_accuracy))\n",
    "\n",
    "                #print('trainDLogits:')\n",
    "                #print(trainDLogits)\n",
    "                #print('trainFeatures:')\n",
    "                #print(trainRealFeatures)\n",
    "                #print('xinput')\n",
    "                #print(xinput.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout1')\n",
    "                #print(dropout1.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('batch_norm2')\n",
    "                #print(batch_norm2.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout3')\n",
    "                #print(dropout3.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout4')\n",
    "                #print(dropout4.eval(feed_dict = train_feed_dictionary))\n",
    "\n",
    "                train_D_losses.append(train_D_loss)\n",
    "                train_Accs.append(train_accuracy)\n",
    "                #batch_num = batch_num + 1;\n",
    "            # Validation at the end of each epoch--BZ, Nov, 2019\n",
    "\n",
    "            \n",
    "            test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "            val_correct_preds = []\n",
    "            for test_batch_x, test_batch_y in test_generator:\n",
    "            #test_batch_generator = get_test_batch(X_test, y_test, batch_size);#added--BZ, Nov 2, 2019\n",
    "            #for test_batch_x, test_batch_y in test_batch_generator:\n",
    "                mask = np.ones(len(test_batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "                val_feed_dictionary = {x: normalize(test_batch_x),\n",
    "                                       label: test_batch_y,\n",
    "                                       labeled_mask: mask,\n",
    "                                       dropout_rate: 0.0,\n",
    "                                       is_training: False}\n",
    "\n",
    "\n",
    "                val_D_loss = D_L.eval(feed_dict = val_feed_dictionary)\n",
    "\n",
    "                val_correct_pred = correct_prediction.eval(feed_dict = val_feed_dictionary)\n",
    "                val_correct_preds = np.concatenate((val_correct_preds, val_correct_pred))\n",
    "                    \n",
    "            \n",
    "            val_accuracy = compute_val_accuracy(val_correct_preds)\n",
    "            val_Accs.append(val_accuracy)\n",
    "    \n",
    "            #print(val_correct_preds);\n",
    "            #print('validation_acc: %f' %(val_accuracy))\n",
    "            log_loss_acc('./baseline_log_'+str(datetime.date.today())+'.csv', epoch, train_D_loss, 0, train_accuracy,\n",
    "                 val_accuracy, 'w')\n",
    "    \n",
    "            save_model_on_improvement(baseline_path+'/'+str(datetime.date.today())+'_baseline.ckpt', sess, val_accuracy, val_Accs)\n",
    "            \n",
    "                \n",
    "        \n",
    "    return train_D_losses, train_Accs, val_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline(64, 500000, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve best DCGAN model and run the whole training set or test set to get the feature map vectors, \n",
    "#which are later used to either\n",
    "#i. train a Random Forest Model\n",
    "#or\n",
    "#ii. run on test set\n",
    "#start new session:\n",
    "def testModel_baseline(test_data_by_class, test_label_by_class, batch_size):\n",
    "    labels = np.zeros((0,num_classes))\n",
    "    predictions = np.zeros((0))\n",
    "    correct_preds = np.zeros((0))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess: \n",
    "        bestModel = tf.train.import_meta_graph('./savedModels/baseline/'+str(datetime.date.today())+'_baseline.ckpt.meta')#load graph\n",
    "        bestModel.restore(sess,tf.train.latest_checkpoint('./savedModels/baseline'))#load parameters\n",
    "        #feed placeholders:\n",
    "        x = sess.graph.get_tensor_by_name(\"x:0\")\n",
    "        label = sess.graph.get_tensor_by_name(\"label:0\")\n",
    "        labeled_mask = sess.graph.get_tensor_by_name(\"labeled_mask:0\")\n",
    "        dropout_rate = sess.graph.get_tensor_by_name(\"dropout_rate:0\")\n",
    "        is_training = sess.graph.get_tensor_by_name(\"is_training:0\")\n",
    "        #output metrics of interest:\n",
    "        D_real_prob = sess.graph.get_tensor_by_name(\"Discriminator/D_output:0\")\n",
    "        correct_prediction = sess.graph.get_tensor_by_name(\"correct_prediction:0\")\n",
    "#         prediction = tf.argmax(D_real_prob, axis = 1)\n",
    "        prediction = tf.argmax( D_real_prob[:, :-1], axis = 1)\n",
    "\n",
    "        test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "        \n",
    "        #for debugging: output weight names\n",
    "        weights = [v.eval(session=sess) for v in tf.trainable_variables()]\n",
    "        weightNames = [v for v in tf.trainable_variables()]\n",
    "        i = 0\n",
    "        for batch_x, batch_y in test_generator:\n",
    "            #print('batch_y:')\n",
    "            #print(batch_y)\n",
    "            mask = np.ones(len(batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "            feed_dictionary = {x: normalize(batch_x),\n",
    "                               label: batch_y,\n",
    "                               labeled_mask: mask,\n",
    "                               dropout_rate: 0.0,\n",
    "                               is_training: False}\n",
    "            \n",
    "            #accuracy_eval = accuracy.eval(feed_dict = feed_dictionary)\n",
    "            correct_pred_eval = correct_prediction.eval(feed_dict = feed_dictionary)\n",
    "            D_real_prob_eval = D_real_prob.eval(feed_dict = feed_dictionary)\n",
    "            \n",
    "            correct_preds = np.concatenate((correct_preds, correct_pred_eval))\n",
    "            prediction_eval = prediction.eval(feed_dict=feed_dictionary)\n",
    "            labels = np.concatenate((labels, batch_y))#array of k+1 one hot\n",
    "            predictions = np.concatenate((predictions, prediction_eval))#array of ints\n",
    "            #print('real features: batch'+str(i))\n",
    "            #print(D_real_features_eval)\n",
    "            #print('accuracy: '+str(accuracy_eval))\n",
    "            #print(str(D_real_features_eval.shape))\n",
    "            #print('labels'+str(labels.shape))\n",
    "            #print('featuremaps'+str(feature_maps.shape))\n",
    "            #print('real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            #print('dropout1:')\n",
    "            #print(dropout1.eval(feed_dict = feed_dictionary))\n",
    "            #print('conv1:')\n",
    "            #print(conv1.eval(feed_dict = feed_dictionary))\n",
    "            #print('x:')\n",
    "            #print(x.eval(feed_dict = feed_dictionary))\n",
    "            #print('D_real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            i = i + 1\n",
    "        sess.close()\n",
    "    total_accuracy = np.sum(correct_preds)/len(correct_preds)\n",
    "    print('total accuracy: '+str(total_accuracy))\n",
    "#     print('labels:')\n",
    "#     print(labels)\n",
    "    print('predictions:')\n",
    "    print(predictions)\n",
    "    print('Confusion Matirx:')\n",
    "#     print(confusion_matrix(predictions, np.argmax(labels, axis = 1), num_classes+1))\n",
    "    print(confusion_matrix(np.argmax(labels, axis = 1), predictions))\n",
    "#     return labels, weights, weightNames\n",
    "    return predictions, labels, weights, weightNames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel_baseline(test_data_by_class, test_label_by_class, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
